---
title: إعادة بناء Alpaca باستخدام صنف Hugging Face Trainer
description: ضبط Llama-2-7B بدقة باستخدام مجموعة بيانات Alpaca وأداة Hugging Face Trainer
date: "2023-11-07"
tags: [ml/ai, open-source]
---

_تحديث 2024: قد يكون الكود في هذه التدوينة قديمًا. أنصح بالاطلاع على [دليل Hugging Face Trainer](https://huggingface.co/docs/transformers/v4.41.3/en/trainer) لمعرفة أحدث المعلومات._

## المقدمة

في مارس من هذا العام (2023)، أصدر مختبر في Stanford مشروعًا صغيرًا سرعان ما أصبح شديد التأثير — [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html). استخدم المؤلفون `text-davinci-003` (أحد نماذج InstructGPT من OpenAI) لتوليد مجموعة بيانات تضم 52 ألف مثال من المطالبات والاستجابات، ثم أجروا ضبطًا دقيقًا لنموذج Llama-7B باستخدام تلك الأزواج من المطالبات والاستجابات.

كانت النتيجة مفاجئة بجودتها — إذ تمكن Alpaca من التفاعل مع المستخدمين بطريقة مشابهة لنماذج InstructGPT من OpenAI، رغم انخفاض تكلفة تدريبه وعدم اعتماده على مجموعة بيانات تدريب مُنشأة بشريًا. في هذه التدوينة، سنكتب شيفرة لتدريب نموذجنا الخاص من الصفر باستخدام مجموعة بيانات Alpaca.

_تعتمد الشيفرة في هذه التدوينة على ما ورد في [مستودع Alpaca](https://github.com/tatsu-lab/stanford_alpaca)، على أمل أن تكون أبسط وأكثر بداهة. يعود الفضل الكامل إلى المؤلفين الأصليين للورقة._

## الإعداد

ستحتاج إلى تثبيت `torch` و`transformers` و`datasets` و`accelerate`. يعد `wandb` خيارًا ممتازًا إذا أردت تتبّع خسارة التدريب مع مرور الوقت. وبالطبع ستحتاج إلى بعض وحدات GPU قوية إذا رغبت في تدريب نموذجك بسرعة.

ابدأ بإنشاء مجلد رئيسي باسم `alpaca-repro`، مع مجلدين فرعيين: أحدهما باسم `trainer` لوضع كود التدريب، والآخر باسم `finetunes` لحفظ نموذجك المكيَّف (المضبوط) بدقة.

## الخطوة 1: تحميل البيانات ومعالجتها

ضع كل الشفرة في هذا القسم داخل `trainer/get_data.py`.

سنبدأ بتحميل [بيانات Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) من منصة HuggingFace. يحتاج كل زوج سؤال/مُحفِّز في مجموعة البيانات إلى تحويل إلى سلسلة نصية واحدة يمكننا تدريب النموذج عليها، لكننا في الواقع نولّد سلسلة إضافية واحدة: `source`، والتي نستخدمها لاحقًا لتجاهل الوسوم (labels) حتى لا يتدرّب نموذجنا على التعليمات.

```python
from datasets import load_dataset

original_dataset = load_dataset("tatsu-lab/alpaca")["train"]

template_no_context = """فيما يلي تعليمات تصف مهمة. \
اكتب ردًا يكمل الطلب بشكل مناسب.

### التعليمات:
{instruction}

### الرد:
"""

template_context = """فيما يلي تعليمات تصف مهمة. \
اكتب ردًا يكمل الطلب بشكل مناسب.

### التعليمات:
{instruction}

### المدخلات:
{input}

### الرد:
"""

def data_to_string(data):

    instruction = data["instruction"]
    context = data["input"]
    response = data["output"]

    template = template_context if len(context) > 0 else template_no_context
    source = template.format(instruction=instruction, input=context)

    return {
        "source": source,
        "text": source + response,
    }


dataset = original_dataset.map(
    data_to_string
).remove_columns(['instruction', 'input', 'output'])
```

هنا نقسّم البيانات لنستخدم 10% للتقييم والاختبارات لاحقًا.

```python
processed_dataset = dataset.train_test_split(test_size=0.1)

train_dataset = processed_dataset["train"]
eval_dataset = processed_dataset["test"]
```

أخيرًا، نعرّف مُجمِّع بيانات لاستخدامه في حلقة التدريب لدينا. تذكّر أن كل سلسلة `text` تتكوّن ببساطة من `source` متبوعًا بالاستجابة. لذا نقوم بترميز سلسلة `source` لمعرفة عدد الوسوم/التصنيفات في سلسلة `text` التي ينبغي تجاهلها.

```python
IGNORE_TOKEN = -100

def data_collator(features, tokenizer):
    sources = [feature["source"] for feature in features]
    targets = [feature["text"] for feature in features]

    source_tokens = tokenizer(
        sources,
        return_tensors="pt",
        padding='longest',
        max_length=None,
    )

    target_tokens = tokenizer(
        targets,
        return_tensors="pt",
        padding='longest',
        max_length=None,
    )

    labels = target_tokens["input_ids"].clone()

    for i in range(len(labels)):
        source_len = source_tokens["attention_mask"][i].sum()

        labels[i, :source_len] = IGNORE_TOKEN

    res = {
        "input_ids": target_tokens["input_ids"],
        "attention_mask": target_tokens["attention_mask"],
        "labels": labels,
    }

    return res
```


## الخطوة 2: كتابة حلقة التدريب الخاصة بنا

ضع كل الشفرة في هذا القسم داخل `trainer/loop.py`.

هذه الشفرة واضحة إلى حد كبير، لذا اكتفيت بإضافة تعليقات توضيحية.

```python
from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments
from accelerate import Accelerator
from get_data import train_dataset, eval_dataset, data_collator

accelerator = Accelerator()

MODEL_PATH = "meta-llama/Llama-2-7b-hf" # مسار نموذج Llama على منصة Hugging Face Hub
OUTPUT_DIR = "../finetunes/alpaca-7b" # مجلد حفظ النموذج المُحسَّن

tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH, legacy=False)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" # غير مُعرَّف افتراضياً، وهذا أمر غريب

model = LlamaForCausalLM.from_pretrained(
    MODEL_PATH, device_map="auto"
)

training_args = TrainingArguments(
    output_dir='checkpoints', # المجلد الذي سيحفظ فيه المدرب نقاط التفتيش
    num_train_epochs=1, # ابدأ بعدد قليل من الحقب للاختبار
    learning_rate=2e-5,
    logging_steps=10,
    per_device_train_batch_size=8,
    remove_unused_columns=False,
    save_steps=1000,
    save_total_limit=1,
    report_to="wandb",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=lambda x: data_collator(x, tokenizer),
)

trainer.train()
trainer.evaluate()

model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
```


## الخطوة 3: تشغيل حلقة التدريب

أنشئ الملف `trainer/accelerate_config.yaml`، ثم الصق الإعدادات التالية:

```yaml
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: "NO"
downcast_bf16: "no"
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
mixed_precision: "no"
num_machines: 1
num_processes: 1
use_cpu: false
```

ثم انتقل بالأمر `cd` إلى `./trainer` وشغّل:

```bash
accelerate launch --config_file accelerate_config.yaml loop.py
```

قد يستغرق حفظ النموذج والأوزان بعض الوقت، لذا كن صبورًا!


## الخطوة 4: اختبار نموذجنا المُعَدّ بدقّة!

كتبتُ سكربتًا بسيطًا لتحميل نموذجنا المُعَدّ بدقّة والتفاعل معه! لا يدعم المحادثات السياقية، لكنه طريقة رائعة للتعرّف إلى كيفية عمل النموذج.

أنشئ ملفًا جديدًا باسم `alpaca-repro/model_test.py`، ثم شغّل `python3 model_test.py`.

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

template = """فيما يلي تعليمات تصف مهمة. \
اكتب ردًا يكمل الطلب بشكل مناسب.

### التعليمات:
{instruction}

### الرد:
"""

model_path = "./finetunes/alpaca-7b"

tokenizer = AutoTokenizer.from_pretrained(model_path, legacy=False)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

model = AutoModelForCausalLM.from_pretrained(
    model_path, device_map="auto", local_files_only=True
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    do_sample=True,
    temperature=0.9,
    max_new_tokens=200,
)

def prompt_model():
    prompt = input("أدخل سؤالك: ")
    prompt = template.format(instruction=prompt)
    answer = pipe(prompt)
    print(answer[0]["generated_text"])

while True:
    prompt_model()
```


## الخلاصة

آمل أن يكون هذا المقال مفيدًا وغنيًا بالمعلومات! أخطّط لنشر متابعة خلال بضعة أيام أشرح فيها كيفية استخدام FSDP مع Hugging Face Trainer.

إذا التبست عليك الأمور أثناء القراءة، فإليك Gist يضم الشيفرة النهائية للمشروع: https://gist.github.com/bgub/1da2c0064d53decf197a304267799708