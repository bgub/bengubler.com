---
title: "تقديم GOM: مراقبة وحدات معالجة الرسوميات عبر الحاويات"
description: نشرت GOM، وهي أداة سطر أوامر لمراقبة استخدام وحدات معالجة الرسوميات عبر حاويات Docker.
date: "2023-10-16"
tags: [ml/ai, open-source]
---

## الخلاصة

`gom` اختصار لـ GPU Output Monitor. إنها حزمة pip توفّر واجهة سطر أوامر لمراقبة استخدام وحدة معالجة الرسومات (GPU). فكّر فيها كأداة `nvidia-smi`، لكنها أسرع وأكثر بساطة. وتتمتع بميزة إضافية: **في البيئات التي تستخدم فيها حاويات Docker وحدات GPU، ستعرض تفصيلاً للاستخدام حسب كل حاوية**! (لا تقلق، فهي تعمل أيضًا في البيئات من دون Docker وحتى داخل حاويات Docker نفسها.)

_أدين لزميلي [Vin](https://howe.vin/) بالفضل في إلهام هذا المشروع. لقد استخدم GPT-4 لإنشاء نموذج أولي بلغة Bash، لكنني اضطررت إلى إعادة كتابته من الصفر بسبب الأخطاء ومشكلات الأداء._

## التعليمات

1. نفّذ `pip3 install gom`
2. بحسب إصدار CUDA لديك، ثبّت الإصدار المناسب من `pynvml`
3. نفّذ `gom show` (لعرض الاستخدام مرة واحدة) أو `gom watch` (لمراقبة الاستخدام مع تحديثٍ يقارب مرة كل ثانية)

## مقارنة بين `gom` و`nvidia-smi`

أعتقد أن النتائج تتحدث عن نفسها :). لقطة الشاشة الأولى هي ناتج تشغيل `gom watch`. يمكنك أن ترى أن أربع حاويات Docker مختلفة — `r0` و`r1` و`r2` و`r3` — تستخدم كلٌ منها وحدة معالجة رسومات (GPU) بشكل مكثف. هناك أيضًا استخدام طفيف لكل وحدات الـGPU لا يأتي من أي حاوية.

![output of running `gom watch` command](/blog-images/gom-watch.png)

لقطة الشاشة الثانية هي ناتج تشغيل `nvidia-smi`. إنها معقدة ومطوّلة بلا داعٍ. ومع أنها تشغل مساحة أكبر من `gom`، فإنها لا تعرض سوى معلومات عن 8 وحدات GPU!

![output of running `nvidia-smi` command](/blog-images/nvidia-smi.png)

## الخلاصة

أنشأت `gom` لأنني أردت مراقبة استخدام GPU عبر حاويات Docker المختلفة. أستخدمه كثيرًا عند تنفيذ مهام تعلم الآلة لأنه سريع ويُظهر المخرجات بشكل يناسب طرفية صغيرة. آمل أن يكون مفيدًا لك. إذا كانت لديك اقتراحات، فلا تتردد في فتح قضية على [مستودع GitHub](https://github.com/bgub/gom).