---
title: الاستدلال متعدد وحدات GPU باستخدام Accelerate
description: نفّذ الاستدلال بسرعة أكبر عبر تمرير المطالبات إلى عدة وحدات GPU بالتوازي.
date: "2023-06-12"
lastUpdated: "2024-06-24"
tags: [ml/ai]
archived: true
---

*تحديث 2024: قد تكون المعلومات في هذه التدوينة قديمة أو غير دقيقة. كما أشار Alex Salinas في التعليقات أدناه، ينبغي على هذا الكود على الأرجح استخدام torchpippy بدلًا من split&#95;between&#95;processes.*

تاريخيًا، حَظِيَ التدريب الموزع باهتمام أكبر من الاستدلال الموزع، إذ إن التدريب أعلى كلفةً حسابيًا. غير أن نماذج اللغة الكبيرة (Large Language Models) الأكبر حجمًا والأكثر تعقيدًا قد تستغرق وقتًا طويلًا في تنفيذ مهام إكمال النص. سواءً للبحث أو للإنتاج، فمن القيّم موازاة الاستدلال لتعظيم الأداء.

من المهم إدراك الفرق بين توزيع أوزان نموذج واحد على عدة وحدات GPU وبين توزيع مطالبات النموذج أو المدخلات على عدة نماذج. الأولى أبسط نسبيًا، بينما الثانية (والتي سأركّز عليها) أكثر تعقيدًا قليلًا.

قبل أسبوع، في الإصدار 0.20.0، طرح [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/index) ميزة تُبسّط الاستدلال متعدد وحدات GPU بشكل ملحوظ: `Accelerator.split_between_processes()`. وهي مبنية على `torch.distributed`، لكنها أبسط بكثير في الاستخدام.

لنرَ كيف يمكن استخدام هذه الميزة الجديدة مع LLaMA. سيُكتب الكود بافتراض أنك حفظت أوزان LLaMA بتنسيق [Hugging Face Transformers](https://huggingface.co/docs/transformers/main/model_doc/llama).

أولًا، ابدأ باستيراد الوحدات المطلوبة وتهيئة أداة التقطيع (tokenizer) والنموذج.

```python
from transformers import LlamaForCausalLM, LlamaTokenizer
from accelerate import Accelerator

accelerator = Accelerator()

MODEL_PATH = "path-to-llama-model"

tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)

model = LlamaForCausalLM.from_pretrained(MODEL_PATH, device_map="auto")
```

لاحظ كيف نمرّر `device_map="auto"`. يتيح ذلك لـ Accelerate توزيع أوزان النموذج بالتساوي عبر وحدات GPU المتاحة.

إذا أردنا، يمكننا استدعاء `model.to(accelerator.device)`. سيؤدي ذلك إلى نقل النموذج إلى وحدة GPU معيّنة. ستكون قيمة `accelerator.device` مختلفة لكل عملية تعمل بالتوازي، لذا قد يكون لديك نموذج مُحمَّل على GPU 0 وآخر على GPU 1، وهكذا. في هذه الحالة سنلتزم بـ `device_map="auto"`. يتيح لنا هذا استخدام نماذج أكبر مما يمكن أن تتسع له وحدة GPU واحدة.

بعد ذلك، سنكتب الشيفرة لتنفيذ الاستدلال!

```python

data = [
    "كلب",
    "قطة",
    "فأر الحقل",
    "خفاش",
    "طائر",
    "سمكة",
    "حصان",
    "بقرة",
    "خروف",
    "ماعز",
    "خنزير",
    "دجاجة",
]

# سيقوم Accelerator تلقائياً بتوزيع هذه البيانات بين كل عملية قيد التشغيل.
# المصفوفة أعلاه تحتوي على 12 عنصراً. لذا إذا كان لدينا 4 عمليات، فستحصل كل عملية
# على 3 نصوص كمطالبات.

with accelerator.split_between_processes(data,) as prompts:
    for prompt in prompts:

        # نقل المتجه إلى GPU، حيث يجب أن يكون على CUDA
        inputs = tokenizer(prompt, return_tensors="pt").to(accelerator.device)

        # الاستنتاج
        generate_ids = model.generate(**inputs, max_length=30)

        # فك التشفير
        result = tokenizer.batch_decode(
            generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

        # طباعة رقم العملية ونتيجة الاستنتاج
        print(
            f"Process {accelerator.process_index} - "
            + result.replace("\n", "\\n")
        )

```

أخيرًا، لم يتبقَّ سوى تشغيل Accelerate باستخدام واجهة الأوامر Accelerate CLI:

```bash
accelerate launch --num_processes=4 script.py
```

عند تشغيل الكود أعلاه، تُحمَّل 4 نُسَخ من النموذج عبر وحدات معالجة الرسومات (GPU) المتاحة. تُوزَّع مطالباتنا بالتساوي على النماذج الأربعة، مما يحسّن الأداء بشكل ملحوظ.

يجب أن يبدو ناتج الكود أعلاه (بعد سجلات تحميل النماذج) كما يلي:


```text
العملية 1 - حوض استحمام مع رأس دش حوض استحمام مع رأس دش حوض استحمام مع رأس دش ومحمول باليد
العملية 0 - حياة الكلاب، حياة الكلاب، حياة الكلاب، حياة الكلاب، حياة الكلاب
العملية 1 - عصفور في اليد خير من عشرة على الشجرة.\nعصفور في اليد خير من عشرة على الشجرة.\nعصفور في
العملية 2 - حصان، حصان، مملكتي مقابل حصان!\nحصان، حصان، مملكتي مقابل حصان!\nحصان،
العملية 3 - ماعز، خروف، بقرة، خنزير، كلب، قطة، حصان، دجاجة، بطة
العملية 0 - حدث كارثي سيغير العالم إلى الأبد.\nالعالم في قبضة جائحة عالمية.\nال
العملية 1 - رحلة صيد إلى جزر البهاما.\nلست متأكداً من قدرتي على الذهاب.\n
العملية 2 - زميل لي في العمل معجب كبير بالمسلسل وكان يحاول إقناعي بمشاهدته. لقد
العملية 3 - حصالة نقود، حصالة نقود، حصالة نقود، حصالة نقود، حصالة نقود
العملية 0 - فأر الحقل، فأر، زبابة، هامستر، جربيل، خنزير غينيا، أرنب،
العملية 2 - خروف، ماعز، كبش، ثور، حمل ذكر، يمامة،
العملية 3 - دجاجة في كل إناء، سيارة في كل مرآب، منزل في كل فناء خلفي، وظيفة لكل رجل، كلية
```

آمل أن يكون هذا قد كان مفيدًا! يمكنك التعرّف أكثر على Accelerate والاستدلال الموزّع في وثائق Accelerate [من هنا](https://huggingface.co/docs/accelerate/usage_guides/distributed_inference) إذا كنت مهتمًا.
