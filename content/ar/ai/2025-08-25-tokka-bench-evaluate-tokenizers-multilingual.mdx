---
title: "تقديم tokka-bench"
description: "إطار تقييم شامل لمقارنة أدوات الترميز عبر اللغات الطبيعية ولغات البرمجة."
date: "2025-08-25"
tags: [ml/ai, linguistics, open-source]
---

(مستعجل؟ تفضّل بزيارة [tokka-bench.streamlit.app](https://tokka-bench.streamlit.app/))

![لقطة شاشة لمخطط tokka-bench](/blog-images/tokka-bench-hero.png)

قبل بضعة أشهر، بدأت أعمل في وقت فراغي على مشروع جديد — ما قبل تدريب نموذج لغة كبير صغير ومتعدد اللغات. وكما يحدث في المغامرات، تشعّبت رحلتي، وأصبحت شديد الاهتمام بجانب محدد من تدريب النماذج: الترميز (tokenization).

اليوم أودّ أن أشارك إطار عمل لتقييم أدوات الترميز، وأشرح أيضًا كيف يمكن لهذه الأدوات أن تساعدنا على فهم:

- ما مصادر البيانات التي قد يكون تم تدريب نموذج معيّن عليها
- لماذا تؤدي نماذج نماذج اللغة الكبيرة (Large Language Models) — وخاصة النماذج المملوكة مثل ChatGPT وClaude وGemini — أداءً أفضل بكثير من غيرها في المهام متعددة اللغات
- لماذا أصبحت أدوات الترميز الخاصة بـ Claude وGemini وGPT 4o وما بعده مغلقة المصدر
- لماذا تكون بعض نماذج المصدر المفتوح أفضل من غيرها لعمليات الضبط الدقيق

## الخلفية التقنية

### ترميز النص والنحو

يبدأ فهم التقسيم إلى وحدات (tokenization) بفهم كيفية ترميز النص على مستوى البايت. تُرمَّز كل اللغات باستخدام UTF-8، لكن تختلف الأنظمة الكتابية كثيرًا في عدد البايتات اللازمة لترميز المحتوى الدلالي نفسه. يبلغ متوسط الإنجليزية قليلاً فوق بايت واحد لكل حرف، ما يجعلها مدمجة للغاية. تحتاج العربية إلى أكثر من بايتين لكل حرف، بينما قد تتطلب الصينية أكثر من 3 بايتات لكل حرف للترميز السليم.

إلى جانب كفاءة الترميز، تمتلك اللغات فروقًا نحوية أساسية تؤثر في كيفية حزم المعلومات داخل الكلمات. فاللغات التصريفية تحشد قدرًا كبيرًا من المعلومات النحوية في كلمة واحدة. على سبيل المثال، أنا أتحدث التشيكية، حيث تُترجم عبارة "vzali se" إلى "they married each other" بالإنجليزية. تجعل هذه الكثافة النحوية من الصعب مقارنة كفاءة الترميز.

### التقسيم إلى وحدات (Tokenization)

نماذج اللغة الكبيرة لا تتعامل مباشرةً مع البايتات، بل مع "الرموز" (tokens)، وهي أشبه بعلامات تقابل مجموعات من البايتات. تستخدم معظم القواطع (tokenizers) الحديثة ترميز أزواج البايت (BPE)، حيث تبدأ من البايتات المفردة وتندمج تكراريًا أكثر الأزواج شيوعًا لبناء معجم من وحدات تحت‑كلمية.

توجد مقاربات بديلة مثل [Byte Latent Transformer](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/)، لكنها حتى الآن لم تنتشر فعليًا في الأنظمة الإنتاجية.

قرارات التصميم التقني في القواطع كثيرة ومؤثرة:

- هل تُضاف مسافات بادئة؟ (بحيث تُقسَّم "hello" في "hello world" و" hello world" بالطريقة نفسها؟)
- هل تُمنَع عمليات دمج البايت عبر حدود المسافات البيضاء؟ ماذا عن عبر حدود الأنظمة الكتابية المختلفة؟
- هل تستخدم رمزًا غير معروفًا (UNK) أم تتراجع إلى البايتات عند مواجهة تسلسلات خارج المعجم؟

نأمل أن يساعدك هذا على فهم منشور Karpathy الكلاسيكي حول [التقسيم إلى وحدات](https://x.com/karpathy/status/1759996551378940395):

![اقتباس من Karpathy: "Tokenization is at the heart of much weirdness of LLMs..."](https://pbs.twimg.com/media/GGzDbMRasAAZf_D?format=png&name=medium)

## كيف يؤثر تقطيع الرموز على مرحلة ما قبل التدريب

العلاقة بين أدوات التقطيع وبيانات ما قبل التدريب تخلق شبكة معقدة من التأثيرات التي تشكّل قدرات النماذج بصورة جوهرية. غالباً ما تُدرَّب أدوات التقطيع على بيانات ما قبل التدريب لنموذج LLM الذي ستُستخدم معه، لكن اللغات المختلفة تحظى بمستويات متفاوتة من "التغطية" ضمن مفردات أداة التقطيع.

لنتخذ مثالاً: الخمير. نظراً لقلة الموارد المتاحة للخميرية على الإنترنت، فإن جزءاً أصغر من مفردات أداة التقطيع سيُخصص لفك ترميزات إلى الخميرية مقارنة بالإنجليزية. هذا التفاوت في التغطية يعني أن ترميز العدد نفسه من الكلمات بالخميرية سيتطلب عدداً أكبر بكثير من الرموز مقارنة بالإنجليزية. وهنا تظهر المشكلة: غالباً ما تعتمد مرحلة ما قبل التدريب تقسيمات نسبية للغات المختلفة بناءً على عدد الرموز. أي قد تدرب على 10 ملايين رمز من نصوص إنجليزية ومليون رمز من الخميرية، على أمل تحقيق نسبة محتوى 10:1. لكن نص الخميرية يمثّل فعلياً أقل بكثير من 10% من عدد الكلمات مقارنة بالنص الإنجليزي!

والتبعات الدلالية أشد وطأة. رموز الخميرية، لقِلّتها، يُرجَّح أن تمثل حروفاً أو أزواجاً من الحروف الساكنة أكثر من تمثيلها لوحدات دلالية كاملة. وهذا يعني أن النماذج لا تستطيع "تخزين" المفاهيم والسمات والتعريفات والمعرفة الدلالية الأخرى في متجهات التضمين بنفس السهولة في اللغات الممثلة تمثيلاً ناقصاً.

هناك مجتمع مفتوح المصدر نابض بالحياة يجري ضبطاً دقيقاً لنماذج الأساس مفتوحة المصدر للغات الأصغر. إذا كانت أداة التقطيع لديك لا تتعامل جيداً مع اللغات الأجنبية، فسيكون الضبط الدقيق أكثر صعوبة وربما يتطلب توسيع مفردات الأداة برموز مخصصة. وعلى الجانب الآخر، فإن إدخال رموز "شبه مدرَّبة" (رموز لن تظهر في بيانات ما قبل التدريب) قد يربك نموذج الـLLM وحتى يتيح "[هجمات الرموز](https://x.com/karpathy/status/1789590397749957117)."

## كيف تؤثر عملية ترميز الرموز على الاستدلال

تستمر الفروقات في الترميز التي تظهر أثناء مرحلة ما قبل التدريب في التسبب بمشكلات خلال الاستدلال. يحتاج تمثيل النص في اللغات منخفضة الموارد (اللغات ذات الموارد القليلة على الإنترنت) إلى عدد أكبر بكثير من الرموز، ما يسبب عدة مشكلات متتالية:

**تدهور الأداء**: يصبح انخفاض معدل المعالجة مشكلة كبيرة عندما تتطلب كل جملة عدداً من الرموز يزيد بمقدار 2–3 مرات. يحصل المستخدمون على استجابات أبطأ، وتزداد تكلفة تقديم المحادثات على المزودين.

**قيود السياق**: تملأ التسلسلات الأطول نافذة السياق بشكل أسرع، ويتدهور أداء الاسترجاع مع مواجهة النموذج صعوبة في الحفاظ على فهم متماسك عبر تسلسلات الرموز المتضخمة.

**جودة التوليد**: قد يؤدي اختيار الرموز أثناء التوليد إلى إدخال أخطاء. المزيد من الرموز لكل كلمة يعني المزيد من “الفرص للخطأ” في كل كلمة، ما قد يفضي إلى انجراف تراكمي حيث تتفاقم الأخطاء الصغيرة في اختيار الرموز إلى إخفاقات دلالية أكبر.

## تقييم المُقسِّمات النصية باستخدام tokka-bench

طوّرت أداة تتيح استكشاف أداء المُقسِّمات النصية بسهولة عبر 100 لغة طبيعية و20 لغة برمجة. بدأتُ بتقييم 7 مُقسِّمات: Gemma 3 وGPT-2 وGPT-4 وgpt-oss وKimi K2 وLlama 3 وQwen 3.

يضم المشروع عدة مكونات مصممة لحالات استخدام مختلفة:

**مستودع مفتوح المصدر**: يمكنك استنساعه وتشغيل اختبارات القياس محلياً. [https://github.com/bgub/tokka-bench](https://github.com/bgub/tokka-bench)

**لوحة تفاعلية مباشرة**: بالإضافة إلى الشيفرة الخاصة بتشغيل اختبارات القياس، أنشأت أيضاً لوحة تفاعلية مباشرة! [https://tokka-bench.streamlit.app/](https://tokka-bench.streamlit.app/)

يتيح لك هذا بسهولة اختيار توليفات من اللغات والمُقسِّمات للمقارنة، والتبديل بين مقاييس مختلفة لفهم الطبيعة متعددة الأبعاد لأداء المُقسِّمات النصية.

### مجموعات البيانات والمنهجية

**مجموعات البيانات**: للتقييم، أستخدم ثلاث مجموعات بيانات عالية الجودة تمثل مجالات نصية مختلفة:

- [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) للمحتوى باللغة الإنجليزية
- [FineWeb 2](https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) للغات البشرية الأخرى
- [StarCoder](https://huggingface.co/datasets/bigcode/starcoderdata) للغات البرمجة

**مقاييس لكل لغة**: آخذ عيّنة بحجم 2MB من النص من كل مجموعة بيانات وأجري عملية التقسيم إلى رموز (tokenization) لحساب مقاييس الأداء الخاصة بكل لغة. لهذا النهج قيد مهم: بسبب اختلافات ترميز UTF-8، تمثل 2MB كميات متباينة جدًا من المحتوى الدلالي عبر اللغات. قد يكون نهجٌ أفضل هو احتساب "ثابت تحجيم" عالمي بناءً على محتوى دلالي مكافئ—على سبيل المثال، استخدام ترجمات متوازية للتطبيع وفقًا لحجم البايت لكتاب Harry Potter بالإنجليزية مقسومًا على الوحدات الدلالية. في الوضع الحالي، ينبغي تفسير المقارنات بين اللغات بحذر، ومن الأكثر موثوقية مقارنة المحوِّلات إلى رموز المختلفة ضمن اللغة نفسها.

**مقاييس المفردات**: لتحليل مفردات المحوِّلات إلى رموز نفسها، أختار عشوائيًا 10,000 رمز من مفردات كل محوِّل وأحلّل خصائصها بعد فك الترميز.

**تعريفات وحدات اللغة**: بما أنّ اللغات تنظّم المعلومات بطرق مختلفة، فإنني أعرّف "الوحدات" لمقاييس الخصوبة والتجزئة على النحو التالي:

- **اللغات المعتمدة على المسافات**: عدد الرموز لكل كلمة (وحدات مفصولة بمسافات)
- **اللغات المعتمدة على الحروف** (مثل الصينية واليابانية والتايلاندية): عدد الرموز لكل حرف (باستثناء المسافات)
- **اللغات المعتمدة على المقاطع الصوتية** (مثل التبتية): عدد الرموز لكل مقطع صوتي (وحدات مفصولة بعلامة tsheg، مع أساليب احتياطية)

## المقاييس والنتائج حسب اللغة

لنقارن بين GPT-2 وLlama 3 وKimi K2 على مجموعة فرعية من اللغات الشائعة لتوضيح نوع الرؤى التي يمكن أن يكشفها tokka-bench. اخترت هذه النماذج الثلاثة لإبراز تطوّر نهج التقطيع إلى وحدات على مرّ الوقت.

السياق لكل منها:

- GPT-2 يضم حوالي 50 ألف مفردة، وصدر في فبراير 2019
- Llama 3 يضم حوالي 128 ألف مفردة، وصدر في أبريل 2024
- Kimi K2 يضم حوالي 164 ألف مفردة، وصدر في يوليو 2025

### الكفاءة (بايت لكل رمز)

**`bytes_per_token`**: متوسط عدد بايتات UTF-8 لكل رمز (total_bytes / total_tokens). تشير القيم الأعلى إلى ضغط النص في رموز بكفاءة أكبر.

![Graph of bytes-per-token in multiple languages](/blog-images/tokka-bench-efficiency.png)

تكشف فروقات الكفاءة عن أولويات التدريب وتركيبة البيانات. اللغات ذات نسب بايت-لكل-رمز الأعلى تُضغط بشكل أكثر فاعلية، ما قد يشير إلى تخصيص أفضل للمفردات أو إلى توفر بيانات تدريب أكثر لتعلّم المفردات.

**قيد مهم**: لا يأخذ هذا المقياس في الحسبان اختلافات ترميز UTF-8 عبر الأنظمة الكتابية. على سبيل المثال، تبدو الهندية أكثر كفاءة بشكل مصطنع لأن كل حرف يحتاج إلى 3 بايتات للترميز—فحتى تخصيص 50 رمزًا فقط لتمثيل كل حرف في الأبجدية الهندية سينتج كفاءة قدرها 3 بايت/رمز. ومع ذلك، تتكون كثير من حروف الهندية من دمج الحروف الساكنة مع علامات الحركات أو من عناقيد ساكنة، لذا فإن إضافة رموز لهذه التركيبات (التي تمثل 6–9 بايتات لكل منها) قد يرفع مقاييس الكفاءة بينما يظل يوفر تغطية دلالية ضعيفة. هذا لا يعكس كفاءة دلالية حقيقية. يعمل هذا المقياس على نحو أفضل عند مقارنة أدوات تقطيع مختلفة ضمن اللغة نفسها بدلًا من مقارنة الكفاءة عبر أنظمة كتابية متنوعة.

### التغطية (الرموز الفريدة)

**`unique_tokens`**: عدد معرّفات الرموز المميّزة المختلفة المستخدمة عند ترميز نص عينة في كل لغة. تشير القيم الأعلى إلى تغطية أفضل لكتابة تلك اللغة مع تقليل حالات الرجوع إلى بايتات الأحرف الفردية.

![Graph of unique tokens in multiple languages](/blog-images/tokka-bench-coverage.png)

أجد عمومًا أن التغطية هي الأكثر دلالة على البنية اللغوية لبيانات ما قبل التدريب. انظر إلى مدى ارتفاع تغطية كتابة الماندرين في Kimi K2 مقارنة بالمقسِّمات الأخرى! هذا بالضبط ما نتوقّعه، لأنه نموذج LLM صيني بمفردات مُحسّنة خصيصًا للنصوص الصينية.

تكشف هرمية التغطية عن أولويات تدريب واضحة:

- تغطية استثنائية للصينية في Kimi K2
- الإنجليزية تمتلك أفضل تغطية للكتابة بفارق كبير عبر جميع النماذج، لكنها في المركز الثاني ضمن Kimi K2
- اللغات اللاتينية (وخاصة الرومانسية) تؤدي أداءً جيدًا
- تليها اللغات الأخرى ذات الأبجدية اللاتينية
- الكورية واليابانية والروسية تُظهر تغطية متوسطة
- الهندية والفارسية والخميرية تتأخر بشكل ملحوظ

**ملاحظة حول المقارنة عبر اللغات**: بما أن التغطية تُحتسب على عينات نصية ثابتة بحجم 2MB، فإن اختلاف عدد بايتات UTF-8 اللازمة لتمثيل محتوى دلالي متماثل بين اللغات يجعل المقارنة المباشرة إشكالية. منهجية أكثر انضباطًا ستحتسب التغطية كنسبة مئوية مقارنةً بخط أساس مُطبّع—لكن في الوقت الحالي، يبقى هذا المقياس أكثر موثوقية عند مقارنة مقسِّمات مختلفة ضمن اللغة نفسها بدلًا من مقارنة التغطية عبر كتابات متنوّعة.

### معدل تفكيك الكلمات

**`word_split_pct`**: نسبة الوحدات التي تتفكك إلى أكثر من رمز واحد. تُعرَّف الوحدات بحسب اللغة (كلمات في اللغات المعتمدة على المسافات، أحرف في اللغات المعتمدة على الحروف، مقاطع في اللغات المعتمدة على المقاطع). تشير القيم الأقل عمومًا إلى مواءمة أفضل مع حدود الوحدات الطبيعية.

![Graph of word splitting percentage in multiple languages](/blog-images/tokka-bench-word-splitting.png)

في الماندرين، يحقق Kimi K2 أدنى معدل لاستمرار الكلمة! فقط 4% من الرموز تواصل كلمةً قائمة.

*تنبيه: تذكّر أنه في اللغات المعتمدة على الحروف مثل الماندرين، يُقاس هذا المؤشر على مستوى الحرف، لا الكلمة. قد تتكوّن الكلمات في الماندرين من حرف واحد أو أكثر — ومعظمها في الواقع من حرفين — لكن تحديد ذلك بسرعة ضمن اختبار قياسي أمر معقد حسابيًا.*

### خصوبة المقاطع الفرعية

**`subword_fertility`**: عدد الرموز لكل وحدة، حيث تُعرَّف الوحدات استنادًا إلى بنية اللغة (انظر المنهجية أعلاه). القيم الأقل أفضل — الاقتراب من 1 يعني قطعًا أقل لكل وحدة دلالية.

![رسم بياني لخصوبة المقاطع الفرعية في عدة لغات](/blog-images/tokka-bench-subword-fertility.png)

في المندرينية، يمتلك Kimi K2 أدنى خصوبة للمقاطع الفرعية! الخصوبة أقل من 1، ما يعني أن كل رمز يمثّل في المتوسط أكثر من حرف واحد.

## مقاييس المفردات (مجمّعة عبر جميع اللغات)

تُحتسب بأخذ عينات من الرموز من مفردات أداة التقسيم (tokenizer) ثم فكّ ترميزها:

**`tokens_starting_with_space_pct`**: نسبة الرموز التي يُفَكّ ترميزها مع مسافة في البداية. يوضح ذلك كلاً من تصميم أداة التقسيم (مدى تخصيص المفردات لبدايات الكلمات مقابل الاستمرارات) وخصائص بيانات التدريب (فاللغات التي لا تتضمن مسافات بين الكلمات ستنتج بطبيعتها نسباً أقل).

**`tokens_with_whitespace_in_middle_pct`**: نسبة الرموز التي يحتوي نصها المفكوك على مسافة بيضاء ليست في البداية. يشير ذلك إلى رموز متعددة الكلمات أو غنية بالمسافات تتجاوز حدوداً طبيعية.

**`tokens_with_script_overlap_pct`**: نسبة الرموز التي تحتوي على محارف من عائلات مخطوطات Unicode متعددة. قد تشير القيم الأعلى إلى رموز مختلطة المخطوطات أو على مستوى البايت لا تراعي حدود المخطوطات.

**`tokens_with_{script}_unicode_pct`**: التوزيع عبر المخطوطات (مثل Latin، Cyrillic، Chinese، Japanese، Korean، Arabic، Devanagari، Thai، Hebrew، Greek، الأرقام، علامات الترقيم، الرموز). يبيّن أنظمة الكتابة التي تغطيها عملياً رموز أداة التقسيم.

## القسم الإضافي: لغات البرمجة

أخيرًا، دعونا نستعرض أمرًا لافتًا لاحظته بخصوص لغات البرمجة (سننتقل هنا من GPT-2 إلى gpt-oss):

![رسم بياني يوضح عدد البايتات لكل رمز في لغات برمجة مختلفة](/blog-images/tokka-bench-coding-efficiency.png)

هناك تباين أقل بكثير في الكفاءة عبر لغات البرمجة — حيث تُظهر Kimi K2 وLlama 3 وGPT-OSS أداءً شبه متطابق في عدد البايتات لكل رمز في كل لغة برمجة!

لست متأكدًا تمامًا من سبب هذا التقارب، لكنني أجد ذلك مثيرًا للاهتمام. قد يشير إلى مجموعات بيانات مشتركة تستخدمها النماذج الثلاثة جميعًا، أو ربما إلى نسب مشابهة للغات البرمجة المختلفة في GitHub ومصادر التدريب الشائعة الأخرى.

## الخلاصة

آمل أن تجد tokka-bench مفيدًا وكاشفًا بقدر ما أفعل أنا! قد تكون هناك بعض الأخطاء الكامنة — لقد اختبرت قدرًا لا بأس به، لكن الأداة يمكن أن تستفيد كثيرًا من اختبارات مجتمعية أكثر شمولًا عبر لغات وحالات استخدام متنوعة.

الرجاء المساهمة! سواء عبر الإبلاغ عن الأخطاء، أو إضافة مقاييس جديدة، أو تضمين أدوات تقطيع إضافية، أو توسيع نطاق اللغات المدعومة، فإن مشاركة المجتمع ستجعل هذه الأداة أكثر قيمة بكثير.

إذا كنت تعمل في مختبر ذكاء اصطناعي يمتلك نموذجًا مملوك الحقوق وتشعر بالارتياح لمشاركة مقاييس أداة التقطيع الخاصة بكم لأغراض معلوماتية، فيُرجى التواصل معي! سيستفيد المجتمع كثيرًا من فهم كيفية تعاطي الأنظمة المتقدمة مع التقطيع متعدد اللغات.

## الشكر والمراجع

- [Vin Howe](https://howe.vin/)، و[Sachin Raja](https://x.com/s4chinraja)، و[Jacob Holloway](https://www.linkedin.com/in/jhollowayj/) راجعوا مقالي وقدّموا ملاحظات قيّمة
- [Harsha Vardhan Khurdula](https://www.linkedin.com/in/harsha-vardhan-khurdula-99b400183/) ساعدني في جمع أبحاث ذات صلة والتفكير في المقاييس بشكلٍ منهجي
- كانت Judit Ács، حسب علمي، أوّل من قدّم مفهوم “خصوبة الوحدات تحت الكلمية” و“نسبة أجزاء الكلمات الاستمرارية” بوصفهما مقاييس معيارية للتجزئة في [هذه التدوينة](https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html)
- طوّر Rust وآخرون هذه الأفكار في [ورقة مقدّمة إلى ACL](https://aclanthology.org/2021.acl-long.243.pdf) كانت مفيدة للغاية

## أفكار لأبحاث مستقبلية

**ارتباط الأداء**: أيهما يؤثر أكثر في الأداء متعدد اللغات في المهام اللاحقة: كفاءة التقسيم أم تغطية المفردات؟ العلاقة ليست بديهية على الفور ويرجّح أنها تختلف باختلاف نوع المهمة.

**مفاضلات التحسين**: إلى أي مدى يمكن تحسين التغطية مع الحفاظ على الكفاءة؟ هل هناك حد باريتو يمكننا توصيفه رياضياً؟

**القوة التنبؤية**: هل يمكننا التنبؤ بقدرات النماذج متعددة اللغات من مقاييس المُقسِّم وحدها؟ إن أمكن، فقد يوفّر ذلك طريقة سريعة لتقييم إمكانات النموذج قبل عمليات تقييم مكلفة.