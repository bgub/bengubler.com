---
title: "Predstavujeme tokka-bench"
description: "Komplexný rámec na vyhodnocovanie a porovnávanie tokenizérov naprieč prirodzenými aj programovacími jazykmi."
date: "2025-08-25"
tags: [ml/ai, linguistics, open-source]
---

Ponáhľaš sa? Navštív [tokka-bench.streamlit.app](https://tokka-bench.streamlit.app/)

![Snímka obrazovky grafu tokka-bench](/blog-images/tokka-bench-hero.png)

Pred niekoľkými mesiacmi som začal vo voľnom čase pracovať na novom projekte — predtrénovaní malého, viacjazyčného veľkého jazykového modelu (LLMs). Ako to pri výpravách býva, aj tá moja sa po ceste stočila inam a veľmi ma zaujal jeden konkrétny aspekt trénovania modelov: tokenizácia.

Dnes chcem predstaviť rámec na vyhodnocovanie tokenizérov a zároveň vysvetliť, ako nám tokenizéry môžu pomôcť pochopiť:

- Na akých dátových zdrojoch mohol byť daný model natrénovaný
- Prečo niektoré veľké jazykové modely (LLMs) — najmä proprietárne modely ako ChatGPT, Claude a Gemini — dosahujú pri viacjazyčných úlohách výrazne lepšie výsledky než iné
- Prečo majú Claude, Gemini a GPT-4o a novšie uzavreté tokenizéry
- Prečo sú niektoré OSS modely na doladovanie vhodnejšie než iné

## Technické zázemie

### Kódovanie písma a gramatika

Pochopenie tokenizácie sa začína pochopením toho, ako je text kódovaný na úrovni bajtov. Všetky jazyky sa kódujú pomocou UTF-8, no rôzne písma vyžadujú výrazne odlišný počet bajtov na zakódovanie rovnakého sémantického obsahu. Angličtina má v priemere trochu viac než 1 bajt na znak, vďaka čomu je mimoriadne kompaktná. Arabčina potrebuje 2+ bajty na znak, zatiaľ čo čínština môže na správne zakódovanie vyžadovať 3+ bajty na znak.

Okrem efektívnosti kódovania majú jazyky základné gramatické rozdiely, ktoré ovplyvňujú, ako sa informácie vkladajú do slov. Syntetické jazyky zhustia veľa syntaktických informácií do jednotlivých slov. Napríklad, hovorím po česky, kde fráza „vzali se“ by sa do angličtiny preložila ako „they married each other“. Táto gramatická hustota sťažuje porovnávanie efektívnosti kódovania.

### Tokenizácia

Veľké jazykové modely (LLMs) nepracujú priamo s bajtmi — pracujú s „tokenmi“, teda symbolmi zodpovedajúcimi skupinám bajtov. Väčšina moderných tokenizérov používa Byte Pair Encoding (BPE): začína od jednotlivých bajtov a iteratívne spája najčastejšie dvojice, čím buduje slovník podslovných jednotiek.

Existujú aj alternatívne prístupy, napríklad [Byte Latent Transformer](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/), no zatiaľ sa v produkčných systémoch veľmi neujali.

Technických rozhodnutí pri návrhu tokenizéra je mnoho a majú zásadné dôsledky:

- Pridávate úvodné medzery? (Aby sa „hello“ v „hello world“ a „ hello world“ tokenizovalo rovnako?)
- Zakazujete spájanie bajtov cez hranice bielych znakov? A čo cez hranice písiem/skriptov?
- Používate neznámy (UNK) token, alebo pri výskyte sekvencií mimo slovníka prechádzate späť na bajty?

Dúfam, že vám to pomôže pochopiť Karpathyho klasický [príspevok o tokenizácii](https://x.com/karpathy/status/1759996551378940395):

![Citát od Karpathyho: "Tokenization is at the heart of much weirdness of LLMS..."](https://pbs.twimg.com/media/GGzDbMRasAAZf_D?format=png&name=medium)

## Ako tokenizácia ovplyvňuje pretrénovanie

Vzťah medzi tokenizérmi a pretrénovacími dátami vytvára zložitú sieť vplyvov, ktoré zásadne formujú schopnosti modelu. Tokenizéry sa často trénujú na pretrénovacích dátach LLM, v ktorom budú použité, no rôzne jazyky dostávajú v slovníku tokenizéra odlišnú úroveň „pokrytia“.

Vezmime si príklad: khmérčina. Keďže má khmérčina menej online zdrojov, menšia časť slovníka tokenizéra bude reprezentovať dekódovania do khmérčiny než do angličtiny. Tento rozdiel v pokrytí znamená, že zakódovanie rovnakého počtu slov v khmérčine bude vyžadovať oveľa viac tokenov než v angličtine. A tu nastáva problém: pretrénovanie často používa pomerné rozdelenia jazykov podľa počtu tokenov. To znamená, že môžete trénovať na 10 miliónoch tokenov anglického textu a 1 milióne tokenov khmérčiny v nádeji, že dosiahnete pomer obsahu 10:1. Lenže khmérsky text v skutočnosti predstavuje oveľa menej než 10 % slov v porovnaní s anglickým textom!

Sémantické dôsledky sú ešte vážnejšie. Tokeny khmérčiny, pretože ich je menej, skôr reprezentujú písmená alebo páry spoluhlások než celé sémantické jednotky. To znamená, že modely nemôžu pre nedostatočne zastúpené jazyky tak jednoducho „ukladať“ koncepty, atribúty, definície a ďalšie sémantické poznatky do vektorov zabudovaní (embeddingov).

Existuje živá open-source komunita, ktorá robí jemné doladenia (fine-tuny) OSS základných modelov pre menšie jazyky. Ak váš tokenizér nezvláda cudzie jazyky dobre, jemné doladenie bude náročnejšie a pravdepodobne si vyžiada rozšírenie tokenizéra o vlastné tokeny. Na druhej strane, zavedenie „čiastočne natrénovaných“ tokenov (tokenov, ktoré sa v pretrénovacích dátach nevyskytujú) môže LLM miasť a dokonca umožniť „[token attacks](https://x.com/karpathy/status/1789590397749957117)“.

## Ako tokenizácia ovplyvňuje inferenciu

Rozdiely v tokenizácii, ktoré vznikajú počas predtrénovania, naďalej spôsobujú problémy počas inferencie. Text v nízko‑zdrojových jazykoch (jazykoch s málo online zdrojmi) potrebuje na reprezentáciu oveľa viac tokenov, čo vedie k viacerým reťazovým problémom:

**Zhoršenie výkonu**: Nižšia priepustnosť sa stáva významným problémom, keď každá veta vyžaduje 2–3× viac tokenov na reprezentáciu. Používatelia dostávajú pomalšie odpovede a prevádzka chatov stojí poskytovateľov viac peňazí.

**Obmedzenia kontextu**: Dlhšie sekvencie zapĺňajú kontextové okno rýchlejšie a schopnosť spätne vybavovať informácie klesá, keď sa model trápi udržať súvislé porozumenie naprieč nafúknutými sekvenciami tokenov.

**Kvalita generovania**: Výber tokenov počas generovania môže zavádzať chyby. Viac tokenov na slovo znamená viac „príležitostí niečo pokaziť“ pri každom slove, čo môže viesť ku kumulatívnemu driftu, kde drobné chyby vo výbere tokenov prerastú do vážnejších sémantických zlyhaní.

## Hodnotenie tokenizérov pomocou tokka-bench

Vytvoril som nástroj na jednoduché preskúmavanie výkonu tokenizérov v 100 prirodzených jazykoch a 20 programovacích jazykoch. Začal som hodnotením 7 tokenizérov: Gemma 3, GPT-2, GPT-4, gpt-oss, Kimi K2, Llama 3 a Qwen 3.

Projekt má viacero častí navrhnutých pre rôzne použitia:

**Open-source repozitár**: Môžete si ho naklonovať a spúšťať benchmarky lokálne. [https://github.com/bgub/tokka-bench](https://github.com/bgub/tokka-bench)

**Živý dashboard**: Okrem kódu na spúšťanie benchmarkov som vytvoril aj živý dashboard! [https://tokka-bench.streamlit.app/](https://tokka-bench.streamlit.app/)

Umožňuje vám jednoducho vyberať kombinácie jazykov a tokenizérov na porovnanie a prepínať medzi rôznymi metrikami, aby ste lepšie pochopili mnohorozmernú povahu výkonu tokenizérov.

### Datasety a metodológia

**Datasety**: Na vyhodnotenie používam tri vysokokvalitné datasety, ktoré reprezentujú rôzne domény textu:

- [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) pre anglický obsah
- [FineWeb 2](https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) pre ostatné ľudské jazyky
- [StarCoder](https://huggingface.co/datasets/bigcode/starcoderdata) pre programovacie jazyky

**Jazykové metriky**: Z každého datasetu odoberiem 2 MB textu a tokenizujem ho, aby som vypočítal jazykovo špecifické výkonnostné metriky. Tento prístup má dôležité obmedzenie: kvôli rozdielom v kódovaní UTF-8 predstavuje 2 MB výrazne odlišné množstvo sémantického obsahu naprieč jazykmi. Lepší prístup by mohol vypočítať globálnu „škálovaciu konštantu“ založenú na ekvivalentnom sémantickom obsahu — napríklad použiť paralelné preklady a normalizovať podľa veľkosti v bajtoch Harryho Pottera v angličtine vydelenej sémantickými jednotkami. V aktuálnej podobe treba medzijazykové porovnania interpretovať opatrne a spoľahlivejšie je porovnávať rôzne tokenizéry v rámci toho istého jazyka.

**Metriky slovníka**: Na analýzu samotných slovníkov tokenizérov náhodne vyberiem 10 000 tokenov zo slovníka každého tokenizéra a analyzujem ich dekódované vlastnosti.

**Definície jazykových jednotiek**: Rôzne jazyky štruktúrujú informácie odlišne, preto definujem „jednotky“ pre metriky fertility a delenia nasledovne:

- **Jazyky so separáciou medzerami**: tokeny na slovo (jednotky oddelené medzerou)
- **Znakové jazyky** (napr. čínština, japončina, thajčina): tokeny na znak (bez medzier)
- **Slabičné jazyky** (napr. tibetčina): tokeny na slabiku (jednotky oddelené tsheg, s náhradnými metódami)

## Metriky a výsledky podľa jazyka

Porovnajme GPT-2, Llamu 3 a Kimi K2 na podmnožine populárnych jazykov, aby sme ilustrovali, aké poznatky dokáže tokka-bench odhaliť. Zvolil som tieto tri, aby som ukázal vývoj prístupov k tokenizácii v čase.

Kontext pre každý:

- GPT-2 má veľkosť slovníka ~50K a vyšiel vo februári 2019
- Llama 3 má veľkosť slovníka ~128K a vyšla v apríli 2024
- Kimi K2 má veľkosť slovníka ~164K a vyšiel v júli 2025

### Efektivita (bajty na token)

**`bytes_per_token`**: Priemerný počet bajtov v UTF-8 na token (total_bytes / total_tokens). Vyššie hodnoty znamenajú efektívnejšiu kompresiu textu do tokenov.

![Graph of bytes-per-token in multiple languages](/blog-images/tokka-bench-efficiency.png)

Rozdiely v efektivite odhaľujú tréningové priority a zloženie dát. Jazyky s vyšším pomerom bajtov na token sú komprimované efektívnejšie, čo môže naznačovať lepšiu alokáciu slovníka alebo viac tréningových dát na učenie slovníka.

**Dôležité obmedzenie**: Tento ukazovateľ nezohľadňuje rozdiely v kódovaní UTF-8 naprieč písmami. Napríklad hindčina dosahuje umelo vysokú efektivitu už len preto, že každý znak vyžaduje na kódovanie 3 bajty — ak by sa na reprezentáciu každého znaku v hindskom písme pridelilo len 50 tokenov, vyšlo by to na efektivitu 3 bajty/token. Mnohé hindské znaky sa však vytvárajú kombináciou spoluhlások so samohláskovými znakmi alebo zo zhlukov spoluhlások, takže pridanie tokenov pre tieto kombinácie (reprezentujúce 6–9 bajtov každý) môže nafúknuť metriky efektivity, no stále poskytovať slabé sémantické pokrytie. To neodráža skutočnú sémantickú efektivitu. Táto metrika funguje najlepšie pri porovnávaní rôznych tokenizérov v rámci toho istého jazyka, nie pri porovnávaní efektivity naprieč odlišnými písmami.

### Pokrytie (unikátne tokeny)

**`unique_tokens`**: Počet odlišných ID tokenov použitých pri kódovaní ukážkového textu v každom jazyku. Vyššie hodnoty naznačujú lepšie pokrytie písma/písiem daného jazyka s menším počtom byte fallbackov na jednotlivé znaky.

![Graph of unique tokens in multiple languages](/blog-images/tokka-bench-coverage.png)

Vo všeobecnosti považujem pokrytie za najvýpovednejší ukazovateľ jazykového zloženia predtréningových dát. Pozrite sa, o koľko vyššie je pokrytie mandarínskeho písma pri Kimi K2 než pri ostatných tokenizéroch! Presne to by sme očakávali, keďže ide o čínsky LLM so slovníkom špecificky optimalizovaným pre čínsky text.

Hierarchia pokrytia odhaľuje jasné tréningové priority:

- Čínština má v Kimi K2 výnimočné pokrytie
- Angličtina má suverénne najlepšie pokrytie písiem naprieč všetkými modelmi, v Kimi K2 je druhá najlepšia
- Latinské jazyky (najmä románske) si vedú dobre
- Nasledujú ďalšie jazyky používajúce latinku
- Kórejčina, japončina a ruština vykazujú stredné pokrytie
- Hindčina, perzština a kmérčina výrazne zaostávajú

**Poznámka k medzijazykovému porovnaniu**: Keďže pokrytie sa počíta na pevných 2 MB textových vzorkách, skutočnosť, že rôzne jazyky vyžadujú rôzny počet bajtov v UTF-8 na vyjadrenie ekvivalentného sémantického obsahu, robí priame porovnanie problematickým. Principiálnejší prístup by počítal pokrytie ako percento voči normalizovanej základni — zatiaľ je však metrika najspoľahlivejšia na porovnávanie rôznych tokenizérov v rámci toho istého jazyka, nie na porovnávanie pokrytia naprieč rôznymi písmami.

### Miera delenia slov

**`word_split_pct`**: Percento jednotiek, ktoré sa rozdelia na viac ako jeden token. Jednotky sú definované podľa jazyka (slová pre jazyky s medzerami, znaky pre znakové jazyky, slabiky pre slabikové jazyky). Nižšie hodnoty vo všeobecnosti znamenajú lepšie zosúladenie s prirodzenými hranicami jednotiek.

![Graph of word splitting percentage in multiple languages](/blog-images/tokka-bench-word-splitting.png)

V mandarínčine má Kimi K2 najnižšiu mieru pokračovania slova! Iba 4 % tokenov pokračuje v slove.

*Upozornenie: pamätajte, že pri znakových jazykoch, ako je mandarínčina, metrika v skutočnosti meria na znak, nie na slovo. Slová v mandarínčine môžu mať 1 znak alebo viac — väčšina má v skutočnosti dva znaky — ale je výpočtovo náročné to rýchlo určiť v benchmarku.*

### Plodnosť podslov

**`subword_fertility`**: Počet tokenov na jednotku, pričom jednotky sú definované podľa štruktúry jazyka (pozri metodológiu vyššie). Nižšie hodnoty sú lepšie — čím bližšie k 1, tým menej častí na sémantickú jednotku.

![Graf plodnosti podslov v rôznych jazykoch](/blog-images/tokka-bench-subword-fertility.png)

V mandarínčine má Kimi K2 najnižšiu plodnosť podslov! Plodnosť je pod 1, čo znamená, že v priemere každý token predstavuje viac než 1 znak.

## Metiky slovnej zásoby (agregované naprieč všetkými jazykmi)

Vypočítané vzorkovaním tokenov zo slovníka tokenizéra a ich následným dekódovaním:

**`tokens_starting_with_space_pct`**: Podiel tokenov, ktoré sa dekódujú s úvodnou medzerou. Odráža dizajn tokenizéra (koľko slovníka je vyhradené pre začiatky slov vs. pokračovania) aj charakter tréningových dát (jazyky bez medzier medzi slovami prirodzene vykazujú nižšie percentá).

**`tokens_with_whitespace_in_middle_pct`**: Podiel tokenov, ktorých dekódovaný text obsahuje medzeru alebo iný biely znak nie na začiatku. Naznačuje viacslovné alebo na medzery bohaté tokeny, ktoré prekračujú prirodzené hranice.

**`tokens_with_script_overlap_pct`**: Podiel tokenov obsahujúcich znaky z viacerých Unicode skriptov (písiem). Vyššie hodnoty môžu naznačovať zmiešané skriptové alebo bajtové tokeny, ktoré nerešpektujú hranice písma.

**`tokens_with_{script}_unicode_pct`**: Rozdelenie naprieč skriptmi (napr. Latin, Cyrillic, Chinese, Japanese, Korean, Arabic, Devanagari, Thai, Hebrew, Greek, numbers, punctuation, symbols). Ukazuje, ktoré písomné systémy tokeny tokenizéra v praxi skutočne pokrývajú.

## Bonusová sekcia: Programovacie jazyky

Na záver sa pozrime na niečo zaujímavé, čo som si všimol pri programovacích jazykoch (tu prepneme z GPT-2 na gpt-oss):

![Graf počtu bajtov na token v rôznych programovacích jazykoch](/blog-images/tokka-bench-coding-efficiency.png)

Naprieč programovacími jazykmi je oveľa menšia variabilita efektivity — Kimi K2, Llama 3 a GPT-OSS majú v každom jazyku takmer identický počet bajtov na token!

Nie som si úplne istý, prečo k tejto konvergencii dochádza, ale je to fascinujúce. Môže to naznačovať, že všetky tri modely používajú spoločné datasety, alebo podobné zastúpenie rôznych programovacích jazykov na GitHube a v iných bežných tréningových zdrojoch.

## Záver

Dúfam, že tokka-bench pre vás bude rovnako užitočný a odhaľujúci ako pre mňa! Možno sa tam ešte skrývajú nejaké chyby — veľa som testoval, no nástroj by veľmi získal na omnoho dôkladnejšom komunitnom testovaní naprieč rôznymi jazykmi a prípadmi použitia.

Prosím, pomôžte prispieť! Či už ide o hlásenia chýb, nové metriky, ďalšie tokenizéry alebo širšie pokrytie jazykov, zapojenie komunity z tohto nástroja urobí omnoho hodnotnejší projekt.

Ak ste z AI laboratória s proprietárnym modelom, ale ste ochotní zdieľať metriky svojho tokenizéra na informačné účely, ozvite sa mi! Komunita by nesmierne získala z pochopenia, ako špičkové systémy zvládajú viacjazyčnú tokenizáciu.

## Poďakovania a odkazy

- [Vin Howe](https://howe.vin/), [Sachin Raja](https://x.com/s4chinraja) a [Jacob Holloway](https://www.linkedin.com/in/jhollowayj/) zrecenzovali môj príspevok a poskytli užitočnú spätnú väzbu
- [Harsha Vardhan Khurdula](https://www.linkedin.com/in/harsha-vardhan-khurdula-99b400183/) mi pomohol zhromaždiť relevantný výskum a systematicky premyslieť metriky
- Judit Ács bola (pokiaľ viem) prvá osoba, ktorá zaviedla plodnosť podsložiek a podiel pokračovacích word pieces ako štandardné metriky tokenizácie v [tomto blogovom príspevku](https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html)
- Rust a kol. rozpracovali tieto myšlienky v [článku na ACL](https://aclanthology.org/2021.acl-long.243.pdf), ktorý bol nesmierne prínosný

## Nápady na budúci výskum

**Korelácia výkonu**: Čo je dôležitejšie pre následný viacjazyčný výkon: efektivita tokenizácie alebo pokrytie slovníka? Súvis nie je na prvý pohľad zrejmý a pravdepodobne sa líši podľa typu úlohy.

**Kompromisy pri optimalizácii**: Do akej miery možno zlepšiť pokrytie pri zachovaní efektivity? Existuje Paretova hranica, ktorú vieme matematicky opísať?

**Prediktívna sila**: Dokážeme predpovedať schopnosti viacjazyčných modelov iba z metrík tokenizéra? Ak áno, mohlo by to poskytnúť rýchly spôsob, ako odhadnúť potenciál modelu pred nákladnými hodnoteniami.