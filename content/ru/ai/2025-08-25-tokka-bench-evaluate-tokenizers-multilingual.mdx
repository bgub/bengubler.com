---
title: "Знакомьтесь: tokka-bench"
description: "Комплексный фреймворк для сравнения токенизаторов на естественных и языках программирования."
date: "2025-08-25"
tags: [ml/ai, linguistics, open-source]
---

(Спешите? Загляните на [tokka-bench.streamlit.app](https://tokka-bench.streamlit.app/))

![Скриншот графика tokka-bench](/blog-images/tokka-bench-hero.png)

Несколько месяцев назад я в свободное время начал работать над новым проектом — предобучением небольшой многоязычной большой языковой модели (LLMs). Как это часто бывает, мой путь сделал крюк, и меня очень заинтересовал один конкретный аспект обучения моделей: токенизация.

Сегодня я хочу поделиться фреймворком для оценки токенизаторов, а также объяснить, как токенизаторы помогают понять:

- На каких источниках данных могла обучаться конкретная модель
- Почему некоторые большие языковые модели (особенно проприетарные, такие как ChatGPT, Claude и Gemini) значительно лучше других справляются с многоязычными задачами
- Почему у Claude, Gemini и GPT‑4o и новее токенизаторы с закрытым исходным кодом
- Почему некоторые модели с открытым исходным кодом лучше других подходят для дообучения

## Технический бэкграунд

### Кодировка письма и грамматика

Понимание токенизации начинается с понимания того, как текст кодируется на уровне байтов. Весь текст кодируется в UTF-8, но разные системы письма требуют существенно разного числа байтов для представления одного и того же смысла. В английском в среднем чуть больше 1 байта на символ, что делает его невероятно компактным. Арабскому требуется 2+ байта на символ, а китайскому — 3+ байта на символ для корректного кодирования.

Помимо эффективности кодирования, языки фундаментально различаются по грамматике, что влияет на то, как информация упаковывается в слова. В синтетических языках в отдельное слово закладывается много синтаксической информации. Например, я говорю по-чешски, где фраза «vzali se» переводится на английский как “they married each other”. Такая грамматическая плотность затрудняет сравнение эффективности кодирования.

### Токенизация

Большие языковые модели (LLMs) не работают напрямую с байтами — они оперируют «токенами», то есть символоподобными единицами, соответствующими группам байтов. Большинство современных токенайзеров используют Byte Pair Encoding (BPE): начинают с отдельных байтов и итеративно объединяют самые частотные пары, формируя словарь субсловных единиц.

Существуют и альтернативные подходы, например [Byte Latent Transformer](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/), но пока они не получили широкого распространения в продакшн-системах.

Технических решений при проектировании токенайзера много, и каждое из них существенно влияет на результат:

- Добавлять ли префиксные пробелы? (Чтобы «hello» в «hello world» и « hello world» токенизировались одинаково?)
- Запрещать ли слияние байтов через границы пробельных символов? А как насчёт границ разных письменностей?
- Использовать ли специальный токен неизвестного слова (UNK) или откатываться к байтам при встрече последовательностей вне словаря?

Надеюсь, это поможет вам понять классический пост Карпати [о токенизации](https://x.com/karpathy/status/1759996551378940395):

![Цитата Карпати: "Tokenization is at the heart of much weirdness of LLMs..."](https://pbs.twimg.com/media/GGzDbMRasAAZf_D?format=png&name=medium)

## Как токенизация влияет на предобучение

Взаимосвязь между токенизаторами и данными предобучения создает сложную сеть эффектов, которые кардинально влияют на возможности модели. Токенизаторы часто обучаются на тех же данных предобучения, что и LLM, в которых они будут использоваться, но разные языки получают разные уровни «покрытия» в словаре токенизатора.

Рассмотрим пример: кхмерский язык. Поскольку кхмерский язык имеет меньше онлайн-ресурсов, меньшая часть словаря токенизатора будет представлять декодирования в кхмерский по сравнению с английским. Это неравенство покрытия означает, что кодирование одинакового количества слов на кхмерском потребует гораздо больше токенов, чем на английском. Но вот где начинаются проблемы: предобучение часто использует пропорциональное разделение разных языков на основе количества токенов. Это означает, что вы можете обучаться на 10 миллионах токенов английского текста и 1 миллионе токенов кхмерского, рассчитывая получить соотношение контента 10:1. Но кхмерский текст на самом деле представляет гораздо меньше 10% слов по сравнению с английским текстом!

Семантические последствия еще более серьезны. Кхмерские токены, поскольку их меньше, с большей вероятностью представляют отдельные буквы или пары согласных, а не целые семантические единицы. Это означает, что модели не могут так же легко «сохранять» концепции, атрибуты, определения и другие семантические знания в векторах эмбеддингов для недопредставленных языков.

Существует активное open-source сообщество, создающее файн-тюны OSS базовых моделей для менее распространенных языков. Если ваш токенизатор плохо обрабатывает иностранные языки, файн-тюнинг будет более сложным и, вероятно, потребует расширения токенизатора пользовательскими токенами. С другой стороны, введение «частично обученных» токенов (токенов, которые не встречаются в данных предобучения) может запутать LLM и даже открыть возможность для «[атак через токены](https://x.com/karpathy/status/1789590397749957117)».

## Как токенизация влияет на инференс

Различия в токенизации, возникающие на этапе предобучения, продолжают создавать проблемы во время инференса. Текст на низкоресурсных языках (то есть языках с малым количеством онлайн-ресурсов) требует значительно больше токенов для представления, что ведет к целой серии каскадных проблем:

**Снижение производительности**: Пропускная способность заметно падает, когда для каждого предложения требуется в 2–3 раза больше токенов. Пользователи получают более медленные ответы, а обслуживание чатов обходится провайдерам дороже.

**Ограничения контекста**: Длинные последовательности быстрее заполняют контекстное окно, и качество воспоминания ухудшается, поскольку модели сложнее поддерживать целостное понимание при раздувшихся последовательностях токенов.

**Качество генерации**: Выбор токенов при генерации может приводить к ошибкам. Больше токенов на слово — больше «шансов ошибиться» на слово, что может вызвать накапливающийся дрейф, когда небольшие ошибки в выборе токенов перерастают в крупные семантические сбои.

## Оценка токенизаторов с помощью tokka-bench

Я создал инструмент для удобного исследования производительности токенизаторов на 100 естественных языках и 20 языках программирования. Для начала я оценил 7 токенизаторов: Gemma 3, GPT-2, GPT-4, gpt-oss, Kimi K2, Llama 3 и Qwen 3.

Проект состоит из нескольких компонентов для разных сценариев:

**Репозиторий с открытым исходным кодом**: можно клонировать и запускать бенчмарки локально. [https://github.com/bgub/tokka-bench](https://github.com/bgub/tokka-bench)

**Онлайн-дэшборд**: помимо кода для запуска бенчмарков, я сделал и живой дэшборд! [https://tokka-bench.streamlit.app/](https://tokka-bench.streamlit.app/)

Он позволяет легко выбирать комбинации языков и токенизаторов для сравнения и переключаться между разными метриками, чтобы понять многогранную природу производительности токенизаторов.

### Датасеты и методология

**Датасеты**: Для оценки я использую три высококачественных датасета, представляющих разные домены текста:

- [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) для англоязычного контента
- [FineWeb 2](https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) для других естественных языков
- [StarCoder](https://huggingface.co/datasets/bigcode/starcoderdata) для языков программирования

**Поме́трики по языкам**: Я отбираю 2 МБ текста из каждого датасета и токенизирую его, чтобы вычислить языковые метрики производительности. У этого подхода есть важное ограничение: из‑за особенностей кодировки UTF‑8 2 МБ соответствуют существенно разному объёму семантического содержания в разных языках. Более корректным был бы расчёт глобальной «масштабной константы» на основе эквивалентного семантического содержания — например, с использованием параллельных переводов для нормализации по размеру в байтах «Гарри Поттера» на английском, делённому на семантические единицы. В текущем виде межъязыковые сравнения следует интерпретировать с осторожностью; надёжнее сравнивать разные токенизаторы в рамках одного языка.

**Метрики словаря**: Для анализа самих словарей токенизаторов я случайным образом выбираю 10 000 токенов из словаря каждого токенизатора и анализирую их декодированные свойства.

**Определения языковых единиц**: Поскольку разные языки по‑разному структурируют информацию, я определяю «единицы» для метрик фертильности и разбиения следующим образом:

- **Языки с разделением пробелами**: токенов на слово (единицы, разделённые пробелами)
- **Символьно ориентированные языки** (например, китайский, японский, тайский): токенов на символ (без учёта пробелов)
- **Слоговые языки** (например, тибетский): токенов на слог (единицы, разделённые tsehg, с резервными методами)

## Метрики и результаты по языкам

Сравним GPT-2, Llama 3 и Kimi K2 на подмножестве популярных языков, чтобы показать, какие инсайты может дать tokka-bench. Я выбрал эти три модели, чтобы проиллюстрировать эволюцию подходов к токенизации со временем.

Контекст для каждой:

- У GPT-2 размер словаря ~50K, релиз — февраль 2019
- У Llama 3 размер словаря ~128K, релиз — апрель 2024
- У Kimi K2 размер словаря ~164K, релиз — июль 2025

### Эффективность (байты на токен)

**`bytes_per_token`**: Среднее число байтов UTF-8 на токен (total_bytes / total_tokens). Более высокие значения означают более эффективное «упаковивание» текста в токены.

![Graph of bytes-per-token in multiple languages](/blog-images/tokka-bench-efficiency.png)

Различия в эффективности отражают приоритеты обучения и состав данных. Языки с более высоким отношением байтов на токен сжимаются эффективнее, что может говорить о более удачном распределении словаря или большем объёме данных для обучения словарю.

**Важное ограничение**: Эта метрика не учитывает различия кодировки UTF-8 между письменностями. Например, хинди показывает искусственно высокую эффективность просто потому, что каждому символу требуется 3 байта — если выделить всего 50 токенов для представления каждого символа алфавита хинди, получится эффективность 3 байта/токен. Однако многие символы хинди образуются сочетаниями согласных с гласными знаками или кластерами согласных, поэтому добавление токенов для этих комбинаций (по 6–9 байт каждая) может завышать метрики эффективности, при этом давая слабое семантическое покрытие. Это не отражает реальной семантической эффективности. Метрика лучше всего подходит для сравнения разных токенизаторов на одном и том же языке, а не для сопоставления эффективности между разными письменностями.

### Покрытие (уникальные токены)

**`unique_tokens`**: Число различных ID токенов, использованных при кодировании образца текста на каждом языке. Более высокие значения указывают на лучшее покрытие письменности(ей) этого языка и на меньшее число байтовых откатов к отдельным символам.

![Graph of unique tokens in multiple languages](/blog-images/tokka-bench-coverage.png)

В целом я считаю, что показатель покрытия лучше всего отражает языковую структуру предобучающих данных. Посмотрите, насколько выше покрытие китайской письменности у Kimi K2 по сравнению с другими токенизаторами! Это именно то, что и следовало ожидать, поскольку это китайская LLM со словарём, специально оптимизированным под китайский текст.

Иерархия покрытия показывает явные приоритеты обучения:

- Китайский имеет выдающееся покрытие в Kimi K2
- Английский демонстрирует лучшее покрытие письменности среди всех моделей, за исключением того, что в Kimi K2 он лишь на втором месте
- Языки на латинской основе (особенно романские) показывают хорошие результаты
- За ними следуют другие языки на латинском алфавите
- Корейский, японский и русский имеют умеренное покрытие
- Хинди, персидский и кхмерский заметно отстают

**Примечание о межъязыковом сравнении**: Поскольку покрытие вычисляется на фиксированных 2 МБ текстовых выборках, различное число UTF-8 байтов, необходимое для представления эквивалентного семантического содержания в разных языках, делает прямое сравнение проблематичным. Более строгий подход определял бы покрытие как процент относительно нормализованной базы, но пока этот показатель надёжнее всего для сравнения разных токенизаторов в рамках одного и того же языка, а не для сравнения покрытия между различными письменностями.

### Частота разбиения слов

**`word_split_pct`**: Процент единиц, которые разбиваются на более чем один токен. Единицы зависят от языка (слова — для языков с разделением пробелами, иероглифы/символы — для иероглифических языков, слоги — для слоговых языков). Более низкие значения обычно означают лучшее совпадение с естественными границами единиц.

![График доли разбиений слов в разных языках](/blog-images/tokka-bench-word-splitting.png)

В китайском (мандаринском) у Kimi K2 самый низкий показатель продолжения слова: лишь 4% токенов являются продолжением.

*Предупреждение: помните, что для иероглифических языков, таких как китайский (мандаринский), эта метрика фактически считается по символам, а не по словам. Слова в китайском могут состоять из одного иероглифа или больше — большинство на самом деле двуслоговые/двухиероглифные, — но быстро определять это в бенчмарке вычислительно сложно.*

### Плодородность субслов

**`subword_fertility`**: Число токенов на единицу, где единицы определяются исходя из структуры языка (см. методологию выше). Меньше — лучше: значение, ближе к 1, означает меньше фрагментов на семантическую единицу.

![Graph of subword fertility in multiple languages](/blog-images/tokka-bench-subword-fertility.png)

В китайском (мандаринском) Kimi K2 имеет самую низкую «плодородность» субслов! Показатель ниже 1, то есть в среднем один токен охватывает более чем один иероглиф.

## Метрики словаря (агрегировано по всем языкам)

Рассчитано путём выборки токенов из словаря токенизатора с последующей декодировкой:

**`tokens_starting_with_space_pct`**: Доля токенов, которые декодируются с ведущим пробелом. Это отражает и дизайн токенизатора (какая часть словаря отведена под начала слов vs продолжения), и характеристики обучающих данных (в языках без пробелов между словами процент будет естественно ниже).

**`tokens_with_whitespace_in_middle_pct`**: Доля токенов, в чьём декодированном тексте есть пробельные символы не в начале. Указывает на многословные или насыщенные пробелами токены, пересекающие естественные границы.

**`tokens_with_script_overlap_pct`**: Доля токенов, содержащих символы из нескольких семейств юникод-скриптов. Высокие значения могут указывать на смешанные скрипты или байтовые токены, которые не соблюдают границы скриптов.

**`tokens_with_{script}_unicode_pct`**: Распределение по скриптам (например, Latin, Cyrillic, Chinese, Japanese, Korean, Arabic, Devanagari, Thai, Hebrew, Greek, numbers, punctuation, symbols). Показывает, какие системы письма токены токенизатора фактически покрывают на практике.

## Бонусный раздел: языки программирования

Наконец, давайте рассмотрим кое-что интересное, что я заметил в отношении языков программирования (здесь переключимся с GPT-2 на gpt-oss):

![График байт на токен для различных языков программирования](/blog-images/tokka-bench-coding-efficiency.png)

Разброс эффективности между языками программирования заметно меньше — Kimi K2, Llama 3 и GPT-OSS показывают почти идентичные значения байт на токен для каждого языка!

Я не до конца понимаю, почему происходит такая сходимость, но это меня очень заинтересовало. Это может указывать на общие датасеты, используемые всеми тремя моделями, или, возможно, на схожие пропорции разных языков программирования в GitHub и других распространённых источниках обучения.

## Заключение

Надеюсь, tokka-bench будет для вас таким же полезным и показательным, как для меня! Возможно, где-то прячутся ошибки — я многое протестировал, но инструменту очень нужны более тщательные проверки сообществом на разных языках и в разных сценариях.

Пожалуйста, помогите и внесите свой вклад! Будь то отчёты об ошибках, новые метрики, дополнительные токенайзеры или расширение языкового охвата — участие сообщества сделает этот инструмент гораздо ценнее.

Если вы работаете в AI‑лаборатории с закрытой моделью, но готовы поделиться метриками вашего токенайзера в информационных целях, свяжитесь со мной! Сообщество получит огромную пользу, узнав, как передовые системы справляются с многоязычной токенизацией.

## Благодарности и ссылки

- [Vin Howe](https://howe.vin/), [Sachin Raja](https://x.com/s4chinraja) и [Jacob Holloway](https://www.linkedin.com/in/jhollowayj/) прочитали мой пост и дали полезную обратную связь
- [Harsha Vardhan Khurdula](https://www.linkedin.com/in/harsha-vardhan-khurdula-99b400183/) помог мне собрать релевантные исследования и системно продумать метрики
- Насколько я могу судить, Judit Ács первой ввела понятия subword fertility и доли continuation word pieces как стандартные метрики токенизации в [этом блоге](https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html)
- Rust и соавт. развили эти идеи в [статье на ACL](https://aclanthology.org/2021.acl-long.243.pdf), которая оказалась чрезвычайно полезной

## Идеи для будущих исследований

**Связь с производительностью**: Что важнее для последующей многоязычной производительности: эффективность токенизации или охват словаря? Связь неочевидна и, вероятно, зависит от типа задачи.

**Компромиссы оптимизации**: Насколько можно увеличить охват, сохраняя эффективность? Существует ли граница Парето, которую можно описать математически?

**Прогностическая сила**: Можно ли предсказывать возможности многоязычных моделей только по метрикам токенизатора? Если да, это может дать быстрый способ оценить потенциал модели до дорогостоящих процедур оценки.