---
title: Accelerate vs. DeepSpeed vs. FSDP
description: Что выбрать для распределённого обучения?
date: "2023-08-29"
tags: [ml/ai]
---

## Введение

Существует множество библиотек и подходов к распределённому обучению. В этой статье мы рассмотрим три из самых популярных: [Accelerate](https://huggingface.co/docs/accelerate/index), [DeepSpeed](https://www.deepspeed.ai/) и [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). Поговорим о различиях между ними и о том, когда стоит выбирать каждую из них.

## Accelerate

[Accelerate](https://huggingface.co/docs/accelerate/index) — популярная библиотека, разработанная и поддерживаемая HuggingFace. О ней можно думать как об обёртке над `torch.distributed`. По сути, она позволяет просто запускать обучение или [инференс](./multi-gpu-inference-with-accelerate) на нескольких GPU или узлах.

В самом простом случае вы используете Accelerate, чтобы инициализировать модель PyTorch на каждом GPU. Внеся всего несколько изменений в цикл обучения, вы делегируете Accelerate обработку параллелизма по данным.

Если ваша модель слишком велика, чтобы уместиться на одном GPU, вы можете с помощью Accelerate разбить её по нескольким GPU, передав `device_map="auto"` в метод `from_pretrained` библиотеки transformers. Учтите: `device_map="auto"` можно использовать только при запуске с `num_processes=1`, поскольку инициализируется всего одна модель.

Если вам нужно более продвинутое шардирование модели (под «sharding» понимается разбиение модели по устройствам), вы можете использовать DeepSpeed или FSDP совместно с Accelerate.

## DeepSpeed

[DeepSpeed](https://www.deepspeed.ai/) предлагает оптимизатор Zero Redundancy Optimizer (ZeRO). Он называется «Zero Redundancy», потому что позволяет распределять модель по нескольким GPU без необходимости дублировать параметры модели на каждой GPU. Это огромное преимущество, поскольку оно позволяет обучать модели, которые превышают объём памяти любой отдельной GPU.

Существует три стадии ZeRO:

- **ZeRO Stage 1** распределяет состояния оптимизатора
- **ZeRO Stage 2** также распределяет градиенты
- **ZeRO Stage 3** также распределяет параметры

Если у вас всё ещё возникают проблемы с памятью, DeepSpeed позволяет выгружать состояния оптимизатора, градиенты и часть весов модели в память CPU или на NVMe-накопитель. Это называется «**ZeRO-Infinity**», и — хотя это заметно медленнее, чем обучение без выгрузки — позволяет обучать по-настоящему огромные модели.

## FSDP

[FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) расшифровывается как “Fully Sharded Data Parallel” («полностью сегментированный параллелизм по данным»). Изначально разработан Facebook AI Research и выпущен в библиотеке Fairscale, позже [родная поддержка была добавлена в PyTorch](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) в версии 1.11.

Он по сути делает то же, что и DeepSpeed ZeRO: управляет шардингом состояний оптимизатора, градиентов и параметров модели. Также поддерживается выгрузка на CPU. Одна из полезных особенностей — его можно использовать как полноценную замену DistributedDataParallel.

## Кратко

- Accelerate — это обёртка над `torch.distributed`, которая упрощает запуск обучения и инференса на нескольких GPU или узлах. Её также можно использовать для простой разбивки модели и она хорошо работает с DeepSpeed и FSDP для более продвинутых сценариев.
- DeepSpeed и FSDP — это две разные реализации одной идеи: распределения (шардинга) параметров модели, градиентов и состояний оптимизатора между несколькими GPU. Обе поддерживают выгрузку на CPU и могут использоваться совместно с Accelerate.