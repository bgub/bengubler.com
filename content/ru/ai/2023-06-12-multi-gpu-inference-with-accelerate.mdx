---
title: Мульти-GPU инференс с Accelerate
description: Ускорьте инференс, передавая промпты на несколько GPU параллельно.
date: "2023-06-12"
lastUpdated: "2024-06-24"
tags: [ml/ai]
archived: true
---

*ОБНОВЛЕНИЕ 2024: Информация в этом посте может быть устаревшей или неточной. Как отметил Alex Salinas в комментариях ниже, этому коду, вероятно, стоит использовать torchpippy вместо split&#95;between&#95;processes.*

Исторически больше внимания уделялось распределённому обучению, чем распределённому инференсу. Обучение действительно более ресурсоёмко. Однако более крупные и сложные большие языковые модели (LLMs) могут долго выполнять задачи дополнения текста. И в исследовательских целях, и в продакшене важно параллелить инференс, чтобы максимизировать производительность.

Важно понимать разницу между распределением весов одной модели по нескольким GPU и распределением промптов или входных данных по нескольким моделям. Первое относительно просто, а второе (на чём я и сосредоточусь) немного сложнее.

Неделю назад в версии 0.20.0 [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/index) добавил функцию, которая значительно упрощает мульти-GPU инференс: `Accelerator.split_between_processes()`. Она основана на `torch.distributed`, но заметно проще в использовании.

Давайте посмотрим, как использовать эту новую функцию с LLaMA. Код написан с расчётом на то, что вы сохранили веса LLaMA в [формате Hugging Face Transformers](https://huggingface.co/docs/transformers/main/model_doc/llama).

Сначала импортируйте необходимые модули и инициализируйте токенизатор и модель.

```python
from transformers import LlamaForCausalLM, LlamaTokenizer
from accelerate import Accelerator

accelerator = Accelerator()

MODEL_PATH = "path-to-llama-model"

tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)

model = LlamaForCausalLM.from_pretrained(MODEL_PATH, device_map="auto")
```

Обратите внимание, что мы передаём `device_map="auto"`. Это позволяет Accelerate равномерно распределять веса модели по доступным GPU.

При желании можно вызвать `model.to(accelerator.device)`. Это перенесёт модель на конкретный GPU. Значение `accelerator.device` будет различаться для каждого параллельно работающего процесса, так что одна копия модели может быть загружена на GPU 0, другая — на GPU 1 и т.д. Однако в данном случае мы останемся при `device_map="auto"`. Это позволит использовать модели, которые не поместились бы на одном GPU.

Теперь напишем код для выполнения инференса!

```python

data = [
    "собака",
    "кот",
    "полёвка",
    "летучая мышь",
    "птица",
    "рыба",
    "лошадь",
    "корова",
    "овца",
    "коза",
    "свинья",
    "курица",
]

# Accelerator автоматически разделит эти данные между всеми запущенными процессами.
# Массив выше содержит 12 элементов. Если у нас 4 процесса, каждому процессу
# будет назначено по 3 строки в качестве промптов.

with accelerator.split_between_processes(data,) as prompts:
    for prompt in prompts:

        # перемещаем тензор на GPU, так как он должен быть на CUDA
        inputs = tokenizer(prompt, return_tensors="pt").to(accelerator.device)

        # инференс
        generate_ids = model.generate(**inputs, max_length=30)

        # декодирование
        result = tokenizer.batch_decode(
            generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

        # выводим номер процесса и результат инференса
        print(
            f"Процесс {accelerator.process_index} - "
            + result.replace("\n", "\\n")
        )

```

Наконец, остается лишь запустить Accelerate с помощью Accelerate CLI:

```bash
accelerate launch --num_processes=4 script.py
```

Когда мы запускаем приведённый выше код, четыре копии модели распределяются по доступным GPU. Наши запросы равномерно делятся между четырьмя моделями, что существенно повышает производительность.

Вывод этого кода (после сообщений о загрузке моделей) должен выглядеть так:


```text
Процесс 1 - ванна с душевой лейкой ванна с душевой лейкой ванна с душевой лейкой и ручным душем
Процесс 0 - собачья жизнь, собачья жизнь, собачья жизнь, собачья жизнь, собачья жизнь
Процесс 1 - лучше синица в руках, чем журавль в небе.\nлучше синица в руках, чем журавль в небе.\nлучше синица в
Процесс 2 - коня, коня, полцарства за коня!\nконя, коня, полцарства за коня!\nконя,
Процесс 3 - коза, овца, корова, свинья, собака, кошка, лошадь, курица, ут
Процесс 0 - катастрофическое событие, которое навсегда изменит мир.\nМир охвачен глобальной пандемией.\n
Процесс 1 - рыбалка на Багамах.\nНе уверен, смогу ли я туда попасть.\n
Процесс 2 - мой коллега большой поклонник этого шоу и пытается уговорить меня его посмотреть. Я
Процесс 3 - копилка-свинка, копилка-свинка, копилка-свинка, копилка-свинка, копилка-свинка
Процесс 0 - полёвка, мышь, землеройка, хомяк, песчанка, морская свинка, кролик,
Процесс 2 - овца, коза, баран, бычок, ягнёнок, горлица, и
Процесс 3 - курица в каждом доме, машина в каждом гараже, дом у каждой семьи, работа для каждого, колледж
```

Надеюсь, это было полезно! Если интересно, больше об Accelerate и распределённом инференсе можно узнать в документации Accelerate [здесь](https://huggingface.co/docs/accelerate/usage_guides/distributed_inference).
