[
  {
    "content": "## Introduction\n\nThere are many different libraries and strategies for distributed training. In this article, we'll look at three of the most popular: [Accelerate](https://huggingface.co/docs/accelerate/index), [DeepSpeed](https://www.deepspeed.ai/), and [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). We'll discuss the differences between them, and when you might want to use one over the other.\n\n## Accelerate\n\n[Accelerate](https://huggingface.co/docs/accelerate/index) is a popular library developed and maintained by HuggingFace. You can think of it as a wrapper around `torch.distributed`. Essentially, it allows you to simply run training or [inference](./multi-gpu-inference-with-accelerate) across multiple GPUs or nodes.\n\nIn its most basic form, you use Accelerate to initialize a PyTorch model on each GPU. By simply making a few modifications to your training loop, Accelerate will handle data parallelism for you.\n\nIf your model is too large to fit on any one GPU, you can use Accelerate to split the model across multiple GPUs by passing `device_map=\"auto\"` into the transformers `from_pretrained` method. Be warned — you can only use `device_map=\"auto\"` if you're running with `num_processes=1`, because you're only initializing one model.\n\nIf you need more sophisticated model sharding (\"sharding\" refers to splitting a model across devices) you can use DeepSpeed or FSDP alongside Accelerate\n\n## DeepSpeed\n\n[DeepSpeed](https://www.deepspeed.ai/) offers the Zero Redundancy Optimizer (ZeRO). It's called \"Zero Redundancy\" because it allows you to partition a model across multiple GPUs without having to replicate the model's parameters across each GPU. This is a huge benefit, because it allows you to train models that are larger than the memory of any one GPU.\n\nThere are three stages of ZeRO:\n\n- **ZeRO Stage 1** partitions optimizer states\n- **ZeRO Stage 2** also partitions gradients\n- **ZeRO Stage 3** also partitions parameters\n\nIf you're still running into memory issues, DeepSpeed allows you to offload the optimizer state, gradients, and some model weights to CPU memory or NVMe storage. This is called \"**ZeRO-Infinity**,\" and — though significantly slower than training without offload — allows for training truly huge models.\n\n## FSDP\n\n[FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) stands for \"Fully Sharded Data Parallel.\" It was originally developed by Facebook AI Research and released in the Fairscale library, but upstream support was [added natively to PyTorch](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) in PyTorch version 1.11.\n\nIt does essentially the same thing as DeepSpeed ZeRO — manage sharding of optimizer states, gradients, and model parameters. It also supports CPU offload. One helpful feature is that it can serve as a drop-in replacement for DistributedDataParallel.\n\n## Summary\n\n- Accelerate is a wrapper around `torch.distributed` that allows you to easily run training or inference across multiple GPUs or nodes. It can also be used for simple model partitioning, and works well with both DeepSpeed and FSDP for more advanced use cases.\n- DeepSpeed and FSDP are two different implementations of the same idea: sharding model parameters, gradients, and optimizer states across multiple GPUs. They both support CPU offload and can be used in conjunction with Accelerate.\n\n_If you liked this article, don't forget to share it and follow me at [@nebrelbug](https://twitter.com/nebrelbug) on Twitter._",
    "title": "Accelerate vs. DeepSpeed vs. FSDP",
    "_meta": {
      "filePath": "accelerate-deepspeed-fsdp.mdx",
      "fileName": "accelerate-deepspeed-fsdp.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "accelerate-deepspeed-fsdp"
    },
    "mdx": "var Component=(()=>{var p=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var r in e)i(t,r,{get:e[r],enumerable:!0})},l=(t,e,r,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!g.call(t,a)&&a!==r&&i(t,a,{get:()=>e[a],enumerable:!(o=u(e,a))||o.enumerable});return t};var b=(t,e,r)=>(r=t!=null?p(f(t)):{},l(e||!t||!t.__esModule?i(r,\"default\",{value:t,enumerable:!0}):r,t)),P=t=>l(i({},\"__esModule\",{value:!0}),t);var d=y((I,s)=>{s.exports=_jsx_runtime});var D={};w(D,{default:()=>h});var n=b(d());function c(t){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",li:\"li\",p:\"p\",strong:\"strong\",ul:\"ul\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Introduction\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"There are many different libraries and strategies for distributed training. In this article, we'll look at three of the most popular: \",(0,n.jsx)(e.a,{href:\"https://huggingface.co/docs/accelerate/index\",children:\"Accelerate\"}),\", \",(0,n.jsx)(e.a,{href:\"https://www.deepspeed.ai/\",children:\"DeepSpeed\"}),\", and \",(0,n.jsx)(e.a,{href:\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\",children:\"FSDP\"}),\". We'll discuss the differences between them, and when you might want to use one over the other.\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Accelerate\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://huggingface.co/docs/accelerate/index\",children:\"Accelerate\"}),\" is a popular library developed and maintained by HuggingFace. You can think of it as a wrapper around \",(0,n.jsx)(e.code,{children:\"torch.distributed\"}),\". Essentially, it allows you to simply run training or \",(0,n.jsx)(e.a,{href:\"./multi-gpu-inference-with-accelerate\",children:\"inference\"}),\" across multiple GPUs or nodes.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In its most basic form, you use Accelerate to initialize a PyTorch model on each GPU. By simply making a few modifications to your training loop, Accelerate will handle data parallelism for you.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"If your model is too large to fit on any one GPU, you can use Accelerate to split the model across multiple GPUs by passing \",(0,n.jsx)(e.code,{children:'device_map=\"auto\"'}),\" into the transformers \",(0,n.jsx)(e.code,{children:\"from_pretrained\"}),\" method. Be warned \\u2014 you can only use \",(0,n.jsx)(e.code,{children:'device_map=\"auto\"'}),\" if you're running with \",(0,n.jsx)(e.code,{children:\"num_processes=1\"}),\", because you're only initializing one model.\"]}),`\n`,(0,n.jsx)(e.p,{children:'If you need more sophisticated model sharding (\"sharding\" refers to splitting a model across devices) you can use DeepSpeed or FSDP alongside Accelerate'}),`\n`,(0,n.jsx)(e.h2,{children:\"DeepSpeed\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://www.deepspeed.ai/\",children:\"DeepSpeed\"}),` offers the Zero Redundancy Optimizer (ZeRO). It's called \"Zero Redundancy\" because it allows you to partition a model across multiple GPUs without having to replicate the model's parameters across each GPU. This is a huge benefit, because it allows you to train models that are larger than the memory of any one GPU.`]}),`\n`,(0,n.jsx)(e.p,{children:\"There are three stages of ZeRO:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"ZeRO Stage 1\"}),\" partitions optimizer states\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"ZeRO Stage 2\"}),\" also partitions gradients\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"ZeRO Stage 3\"}),\" also partitions parameters\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[`If you're still running into memory issues, DeepSpeed allows you to offload the optimizer state, gradients, and some model weights to CPU memory or NVMe storage. This is called \"`,(0,n.jsx)(e.strong,{children:\"ZeRO-Infinity\"}),',\" and \\u2014 though significantly slower than training without offload \\u2014 allows for training truly huge models.']}),`\n`,(0,n.jsx)(e.h2,{children:\"FSDP\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\",children:\"FSDP\"}),' stands for \"Fully Sharded Data Parallel.\" It was originally developed by Facebook AI Research and released in the Fairscale library, but upstream support was ',(0,n.jsx)(e.a,{href:\"https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/\",children:\"added natively to PyTorch\"}),\" in PyTorch version 1.11.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"It does essentially the same thing as DeepSpeed ZeRO \\u2014 manage sharding of optimizer states, gradients, and model parameters. It also supports CPU offload. One helpful feature is that it can serve as a drop-in replacement for DistributedDataParallel.\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Summary\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Accelerate is a wrapper around \",(0,n.jsx)(e.code,{children:\"torch.distributed\"}),\" that allows you to easily run training or inference across multiple GPUs or nodes. It can also be used for simple model partitioning, and works well with both DeepSpeed and FSDP for more advanced use cases.\"]}),`\n`,(0,n.jsx)(e.li,{children:\"DeepSpeed and FSDP are two different implementations of the same idea: sharding model parameters, gradients, and optimizer states across multiple GPUs. They both support CPU offload and can be used in conjunction with Accelerate.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked this article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function h(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(c,{...t})}):c(t)}return P(D);})();\n;return Component;"
  },
  {
    "content": "_This is part 1 of a 2-part series. [Part 2](./enroot-on-slurm-for-distributed-ml-part-2) is available here._\n\nIn the lab where I work, we have access to a High Performance Computing (HPC) environment that uses the [Slurm Workload Manager](https://slurm.schedmd.com/documentation.html). Our HPC runs RHEL (Red Hat Enterprise Linux) 7, and individual users have significantly restricted permissions. We don't have `sudo` access and can't access the internet from the compute nodes.\n\nIn fact, the process of loading packages and updating drivers from inside a compute node is so difficult that it makes distributed training using modern software incredibly complicated. Luckily, there's a solution: containerization.\n\nWe'll use Docker to build an image on our local machine that contains all of the packages we need. Then we can transfer that image to the HPC, and use it to run our training script.\n\n## Step 1: Build a Docker Image Locally\n\nI already wrote about [the Docker setup I use for machine learning](./ultimate-ml-dockerfile), so I won't repeat myself here. The important thing is that you have a tagged Docker image with the packages you need to run your training script. Mine uses Ubuntu 20.04 and CUDA 11.8.\n\n## Step 2: Squash and Transfer the Image\n\nInstall [Enroot](https://github.com/NVIDIA/enroot), then run the following command to turn your Docker image into a squashfs file:\n\n```bash\nenroot import dockerd://<image-name>\n```\n\nThis will create a file called `<image-name>.sqsh` in your current directory. Transfer this file to the HPC using `scp`.\n\n## Step 3: Load the Image on the HPC\n\nEnter a compute node using `salloc --nodes=1 --gpus=8 --qos=<qos> --mem=2000G --time=72:00:00 --ntasks=1 --cpus-per-task=128`.\n\nFirst we need to load the Enroot module:\n\n```bash\nmodule load jq zstd pigz parallel libnvidia-container enroot\n```\n\nOn the HPC, create the image using the following command:\n\n```bash\nenroot create --name image-name /path/to/image-name.sqsh\n```\n\nThen run it:\n\n```bash\nenroot start --mount /local/file/path:/image/file/path \\\n             --rw image-name bash\n```\n\nThis will open up an interactive shell inside the container. Don't forget the `--rw` flag, which makes the root filesystem writable. You can add as many `--mount` flags as you need to mount files and directories from the host machine.\n\nIf you want to pass through environment variables, you can use the `--env` flag along with the name of the environment variable on the host machine. For example, `--env SLURM_NODEID` will pass through the `SLURM_NODEID` environment variable.\n\n_If you liked the article, don't forget to share it and follow me at [@nebrelbug](https://twitter.com/nebrelbug) on Twitter._",
    "title": "Enroot on Slurm for Distributed ML: Part 1",
    "_meta": {
      "filePath": "enroot-on-slurm-for-distributed-ml-part-1.mdx",
      "fileName": "enroot-on-slurm-for-distributed-ml-part-1.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "enroot-on-slurm-for-distributed-ml-part-1"
    },
    "mdx": "var Component=(()=>{var m=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var w=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),b=(t,e)=>{for(var a in e)i(t,a,{get:e[a],enumerable:!0})},h=(t,e,a,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!f.call(t,r)&&r!==a&&i(t,r,{get:()=>e[r],enumerable:!(o=u(e,r))||o.enumerable});return t};var y=(t,e,a)=>(a=t!=null?m(g(t)):{},h(e||!t||!t.__esModule?i(a,\"default\",{value:t,enumerable:!0}):a,t)),k=t=>h(i({},\"__esModule\",{value:!0}),t);var l=w((D,c)=>{c.exports=_jsx_runtime});var v={};b(v,{default:()=>d});var n=y(l());function s(t){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",p:\"p\",pre:\"pre\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"This is part 1 of a 2-part series. \",(0,n.jsx)(e.a,{href:\"./enroot-on-slurm-for-distributed-ml-part-2\",children:\"Part 2\"}),\" is available here.\"]})}),`\n`,(0,n.jsxs)(e.p,{children:[\"In the lab where I work, we have access to a High Performance Computing (HPC) environment that uses the \",(0,n.jsx)(e.a,{href:\"https://slurm.schedmd.com/documentation.html\",children:\"Slurm Workload Manager\"}),\". Our HPC runs RHEL (Red Hat Enterprise Linux) 7, and individual users have significantly restricted permissions. We don't have \",(0,n.jsx)(e.code,{children:\"sudo\"}),\" access and can't access the internet from the compute nodes.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In fact, the process of loading packages and updating drivers from inside a compute node is so difficult that it makes distributed training using modern software incredibly complicated. Luckily, there's a solution: containerization.\"}),`\n`,(0,n.jsx)(e.p,{children:\"We'll use Docker to build an image on our local machine that contains all of the packages we need. Then we can transfer that image to the HPC, and use it to run our training script.\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 1: Build a Docker Image Locally\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"I already wrote about \",(0,n.jsx)(e.a,{href:\"./ultimate-ml-dockerfile\",children:\"the Docker setup I use for machine learning\"}),\", so I won't repeat myself here. The important thing is that you have a tagged Docker image with the packages you need to run your training script. Mine uses Ubuntu 20.04 and CUDA 11.8.\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 2: Squash and Transfer the Image\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Install \",(0,n.jsx)(e.a,{href:\"https://github.com/NVIDIA/enroot\",children:\"Enroot\"}),\", then run the following command to turn your Docker image into a squashfs file:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`enroot import dockerd://<image-name>\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"This will create a file called \",(0,n.jsx)(e.code,{children:\"<image-name>.sqsh\"}),\" in your current directory. Transfer this file to the HPC using \",(0,n.jsx)(e.code,{children:\"scp\"}),\".\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 3: Load the Image on the HPC\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Enter a compute node using \",(0,n.jsx)(e.code,{children:\"salloc --nodes=1 --gpus=8 --qos=<qos> --mem=2000G --time=72:00:00 --ntasks=1 --cpus-per-task=128\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"First we need to load the Enroot module:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`module load jq zstd pigz parallel libnvidia-container enroot\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"On the HPC, create the image using the following command:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`enroot create --name image-name /path/to/image-name.sqsh\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Then run it:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`enroot start --mount /local/file/path:/image/file/path \\\\\n             --rw image-name bash\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"This will open up an interactive shell inside the container. Don't forget the \",(0,n.jsx)(e.code,{children:\"--rw\"}),\" flag, which makes the root filesystem writable. You can add as many \",(0,n.jsx)(e.code,{children:\"--mount\"}),\" flags as you need to mount files and directories from the host machine.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you want to pass through environment variables, you can use the \",(0,n.jsx)(e.code,{children:\"--env\"}),\" flag along with the name of the environment variable on the host machine. For example, \",(0,n.jsx)(e.code,{children:\"--env SLURM_NODEID\"}),\" will pass through the \",(0,n.jsx)(e.code,{children:\"SLURM_NODEID\"}),\" environment variable.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function d(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(s,{...t})}):s(t)}return k(v);})();\n;return Component;"
  },
  {
    "content": "_This is part 2 of a 2-part series. [Part 1](./enroot-on-slurm-for-distributed-ml-part-1) is available here._\n\nIn [part 1](./enroot-on-slurm-for-distributed-ml-part-1), we covered how to use Enroot on Slurm for containerized _single-node_ training using `salloc`. In this post, we'll cover how to use Enroot on Slurm for containerized _multi-node_ training, and transition to using `sbatch`.\n\n## Step 1: Slurm Launch Script\n\nWe'll end up creating several Bash files, all of which should be in the same directory as your training script. The first will be a Slurm launch file that we'll run with `sbatch`. This file will contain the same commands we ran with `salloc` in [part 1](../enroot-on-slurm-for-distributed-ml-part-1), but declared using `#SBATCH` processing directives.\n\n`launch.sh`\n\n```bash\n#!/bin/bash\n#SBATCH -J \"JOBNAME\"\n#SBATCH --nodes=2\n#SBATCH --gpus-per-node=8\n#SBATCH --cpus-per-task=128\n#SBATCH --mem=2000G\n#SBATCH --time=72:00:00\n#SBATCH --qos=<qos>\n\nexport CUR_DIR=$(pwd)\nsrun --nodes=2 stage1.sh\n```\n\nNote that we create a variable `CUR_DIR` to store the current working directory (the directory where the `sbatch` command was run). I use this variable to share the location of my training directory between scripts, so I don't have to hard-code paths. But it's not required.\n\nSlurm will automatically pass local environment variables through to the `srun` command, which will run the `stage1.sh` script on each node.\n\n## Step 2. Enroot Launch Script\n\nNext, we'll create a script that will be run on each node. This script will be responsible for launching the container and running the training script. We'll call this script `stage1.sh`.\n\n`stage1.sh`\n\n```bash\n#!/bin/bash\n\nmodule load jq zstd pigz parallel libnvidia-container enroot\n\nexport MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1) # get the IP address of the first node in the list\nexport MASTER_PORT=6000 # set the port to use for communication between nodes\n\nenroot create --name image-name /path/to/image-name.sqsh\n\nenroot start --env SLURM_NODEID \\\n             --env MASTER_ADDR \\\n             --env MASTER_PORT \\\n             --env SLURM_JOB_NAME \\\n             --env CUR_DIR \\\n             --mount /local/file/path:/image/file/path \\\n             --rw image-name \\\n             bash ${CUR_DIR}/stage2.sh\n```\n\nNote that we pass several important environment variables provided by Slurm, along with `CUR_DIR`, into the container. The `MASTER_ADDR` and `MASTER_PORT` variables are used by PyTorch's distributed training backend to coordinate communication between nodes.\n\nWe also mount a local file path into the container (make sure it contains your training script!).\n\n## Step 3. Training Script\n\nFinally, we'll create a training script that will be run inside the container. We'll call this script `stage2.sh`.\n\n`stage2.sh`\n\n```bash\n#!/bin/bash\n\nexport NCCL_DEBUG=INFO # if you want to see NCCL logs\nexport NODE_RANK=$SLURM_NODEID # set the node rank to the node ID (0, 1, 2, etc.)\necho NODE_RANK: $NODE_RANK # print the node rank for debugging purposes\n\n# Run training script\n# NOTE: modify as desired if you're not using accelerate\n\naccelerate launch --config_file ./accelerate_config.yaml --main_process_ip=$MASTER_ADDR --main_process_port=$MASTER_PORT --machine_rank $NODE_RANK ${CUR_DIR}/loop.py\n```\n\nHere I've used [accelerate](https://huggingface.co/docs/accelerate) as a launcher for my distributed training script, but you can use whatever launcher you want. Just make sure you pass relevant environment variables through!\n\nFor the sake of completeness, here's my `accelerate_config.yaml` file. It utilizes FSDP (Fully Sharded Data Parallel) to split model parameters and gradients across processes. This is a great way to train large models that won't fit on just one GPU.\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: FSDP\ndowncast_bf16: \"no\"\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: 1\n  fsdp_state_dict_type: FULL_STATE_DICT\n  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer\nmain_training_function: main\nmixed_precision: \"no\"\nnum_machines: 2\nnum_processes: 16 # 8 GPUs per node * 2 nodes = 16 processes\nuse_cpu: false\n```\n\n## Step 4. Submit the Job\n\nNow that we've created all the necessary scripts, we can submit the job to Slurm using `sbatch`! From the directory containing the scripts, run:\n\n```bash\nsbatch launch.sh\n```\n\nYour job will be submitted to Slurm and run as soon as resources are available. Output logs will be stored at `slurm-<jobid>.out` in the current directory.\n\n## Conclusion\n\nI hope this was helpful! There are many parts involved in getting distributed training working, but it's not too difficult once you get over the initial learning curve.\n\n_If you liked the article, don't forget to share it and follow me at [@nebrelbug](https://twitter.com/nebrelbug) on Twitter._",
    "title": "Enroot on Slurm for Distributed ML: Part 2",
    "_meta": {
      "filePath": "enroot-on-slurm-for-distributed-ml-part-2.mdx",
      "fileName": "enroot-on-slurm-for-distributed-ml-part-2.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "enroot-on-slurm-for-distributed-ml-part-2"
    },
    "mdx": "var Component=(()=>{var p=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,_=Object.prototype.hasOwnProperty;var f=(r,e)=>()=>(e||r((e={exports:{}}).exports,e),e.exports),b=(r,e)=>{for(var t in e)a(r,t,{get:e[t],enumerable:!0})},s=(r,e,t,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!_.call(r,i)&&i!==t&&a(r,i,{get:()=>e[i],enumerable:!(o=u(e,i))||o.enumerable});return r};var w=(r,e,t)=>(t=r!=null?p(g(r)):{},s(e||!r||!r.__esModule?a(t,\"default\",{value:r,enumerable:!0}):t,r)),R=r=>s(a({},\"__esModule\",{value:!0}),r);var l=f((T,c)=>{c.exports=_jsx_runtime});var S={};b(S,{default:()=>d});var n=w(l());function h(r){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",p:\"p\",pre:\"pre\",...r.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"This is part 2 of a 2-part series. \",(0,n.jsx)(e.a,{href:\"./enroot-on-slurm-for-distributed-ml-part-1\",children:\"Part 1\"}),\" is available here.\"]})}),`\n`,(0,n.jsxs)(e.p,{children:[\"In \",(0,n.jsx)(e.a,{href:\"./enroot-on-slurm-for-distributed-ml-part-1\",children:\"part 1\"}),\", we covered how to use Enroot on Slurm for containerized \",(0,n.jsx)(e.em,{children:\"single-node\"}),\" training using \",(0,n.jsx)(e.code,{children:\"salloc\"}),\". In this post, we'll cover how to use Enroot on Slurm for containerized \",(0,n.jsx)(e.em,{children:\"multi-node\"}),\" training, and transition to using \",(0,n.jsx)(e.code,{children:\"sbatch\"}),\".\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 1: Slurm Launch Script\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"We'll end up creating several Bash files, all of which should be in the same directory as your training script. The first will be a Slurm launch file that we'll run with \",(0,n.jsx)(e.code,{children:\"sbatch\"}),\". This file will contain the same commands we ran with \",(0,n.jsx)(e.code,{children:\"salloc\"}),\" in \",(0,n.jsx)(e.a,{href:\"../enroot-on-slurm-for-distributed-ml-part-1\",children:\"part 1\"}),\", but declared using \",(0,n.jsx)(e.code,{children:\"#SBATCH\"}),\" processing directives.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"launch.sh\"})}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`#!/bin/bash\n#SBATCH -J \"JOBNAME\"\n#SBATCH --nodes=2\n#SBATCH --gpus-per-node=8\n#SBATCH --cpus-per-task=128\n#SBATCH --mem=2000G\n#SBATCH --time=72:00:00\n#SBATCH --qos=<qos>\n\nexport CUR_DIR=$(pwd)\nsrun --nodes=2 stage1.sh\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Note that we create a variable \",(0,n.jsx)(e.code,{children:\"CUR_DIR\"}),\" to store the current working directory (the directory where the \",(0,n.jsx)(e.code,{children:\"sbatch\"}),\" command was run). I use this variable to share the location of my training directory between scripts, so I don't have to hard-code paths. But it's not required.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Slurm will automatically pass local environment variables through to the \",(0,n.jsx)(e.code,{children:\"srun\"}),\" command, which will run the \",(0,n.jsx)(e.code,{children:\"stage1.sh\"}),\" script on each node.\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 2. Enroot Launch Script\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Next, we'll create a script that will be run on each node. This script will be responsible for launching the container and running the training script. We'll call this script \",(0,n.jsx)(e.code,{children:\"stage1.sh\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"stage1.sh\"})}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`#!/bin/bash\n\nmodule load jq zstd pigz parallel libnvidia-container enroot\n\nexport MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1) # get the IP address of the first node in the list\nexport MASTER_PORT=6000 # set the port to use for communication between nodes\n\nenroot create --name image-name /path/to/image-name.sqsh\n\nenroot start --env SLURM_NODEID \\\\\n             --env MASTER_ADDR \\\\\n             --env MASTER_PORT \\\\\n             --env SLURM_JOB_NAME \\\\\n             --env CUR_DIR \\\\\n             --mount /local/file/path:/image/file/path \\\\\n             --rw image-name \\\\\n             bash \\${CUR_DIR}/stage2.sh\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Note that we pass several important environment variables provided by Slurm, along with \",(0,n.jsx)(e.code,{children:\"CUR_DIR\"}),\", into the container. The \",(0,n.jsx)(e.code,{children:\"MASTER_ADDR\"}),\" and \",(0,n.jsx)(e.code,{children:\"MASTER_PORT\"}),\" variables are used by PyTorch's distributed training backend to coordinate communication between nodes.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We also mount a local file path into the container (make sure it contains your training script!).\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 3. Training Script\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Finally, we'll create a training script that will be run inside the container. We'll call this script \",(0,n.jsx)(e.code,{children:\"stage2.sh\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"stage2.sh\"})}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`#!/bin/bash\n\nexport NCCL_DEBUG=INFO # if you want to see NCCL logs\nexport NODE_RANK=$SLURM_NODEID # set the node rank to the node ID (0, 1, 2, etc.)\necho NODE_RANK: $NODE_RANK # print the node rank for debugging purposes\n\n# Run training script\n# NOTE: modify as desired if you're not using accelerate\n\naccelerate launch --config_file ./accelerate_config.yaml --main_process_ip=$MASTER_ADDR --main_process_port=$MASTER_PORT --machine_rank $NODE_RANK \\${CUR_DIR}/loop.py\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Here I've used \",(0,n.jsx)(e.a,{href:\"https://huggingface.co/docs/accelerate\",children:\"accelerate\"}),\" as a launcher for my distributed training script, but you can use whatever launcher you want. Just make sure you pass relevant environment variables through!\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"For the sake of completeness, here's my \",(0,n.jsx)(e.code,{children:\"accelerate_config.yaml\"}),\" file. It utilizes FSDP (Fully Sharded Data Parallel) to split model parameters and gradients across processes. This is a great way to train large models that won't fit on just one GPU.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`compute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: FSDP\ndowncast_bf16: \"no\"\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: 1\n  fsdp_state_dict_type: FULL_STATE_DICT\n  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer\nmain_training_function: main\nmixed_precision: \"no\"\nnum_machines: 2\nnum_processes: 16 # 8 GPUs per node * 2 nodes = 16 processes\nuse_cpu: false\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 4. Submit the Job\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now that we've created all the necessary scripts, we can submit the job to Slurm using \",(0,n.jsx)(e.code,{children:\"sbatch\"}),\"! From the directory containing the scripts, run:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`sbatch launch.sh\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Your job will be submitted to Slurm and run as soon as resources are available. Output logs will be stored at \",(0,n.jsx)(e.code,{children:\"slurm-<jobid>.out\"}),\" in the current directory.\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Conclusion\"}),`\n`,(0,n.jsx)(e.p,{children:\"I hope this was helpful! There are many parts involved in getting distributed training working, but it's not too difficult once you get over the initial learning curve.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function d(r={}){let{wrapper:e}=r.components||{};return e?(0,n.jsx)(e,{...r,children:(0,n.jsx)(h,{...r})}):h(r)}return R(S);})();\n;return Component;"
  },
  {
    "content": "Modern web development involves working with multiple JS build tools and frameworks, each requiring their own configuration files. Managing these configuration files, such as `.eslintrc`, `next.config.js`, and `tailwind.config.js`, can become cumbersome and time-consuming. In this blog post, I'll explore the idea of combining these configuration files into a single file called `global.config.js`, centralizing project configuration and reducing distractions.\n\n## Config files everywhere\n\nAs I'm writing this blog post, my project root contains an `.eslintrc.json`, `next.config.js`, `postcss.config.js`, `tailwind.config.js`, and `tsconfig.json`. Although my configuration is pretty out-of-the-box and each file is less than 30 lines, those files take up valuable space in my VSCode sidebar and distract from what's important: my source code.\n\nMy case is far from exceptional. Some other projects use far more configuration files. Imagine how cluttered a project can get when you add a `.babelrc`, `prettier.config.js`, `jest.config.js`, `cypress.json`, etc.\n\n## The solution: `global.config.js`\n\nI'd like to propose a simple solution: consolidating configuration files in a file called `global.config.js`. The configuration for each tool would be stored in the exported object, under a key with the name of the npm package.\n\nProjects should still allow usage of individual configuration files for cases when configuration is large and complex (e.g. `tsconfig.json`), but should first scan to see if a `global.config.js` exists and configuration for their tool is present.\n\nHere's what a simple `global.config.js` might look like:\n\n```js\nmodule.exports = {\n  eslint: {\n    extends: [\"next/core-web-vitals\"]\n  },\n  postcss: {\n    plugins: {\n      tailwindcss: {},\n      autoprefixer: {}\n    }\n  },\n  tailwindcss: {\n    content: [\"./src/pages/**/*.{js,ts,jsx,tsx,mdx}\"],\n    theme: {\n      extend: {\n        backgroundImage: {\n          \"gradient-radial\": \"radial-gradient(var(--tw-gradient-stops))\"\n        }\n      }\n    },\n    plugins: [require(\"@tailwindcss/typography\")]\n  }\n}\n```\n\n## But what about types?\n\nType completion can be easily enabled for `global.config.js` by adding type definitions in comments, like Tailwind and NextJS already do in their config files.\n\n```js\nmodule.exports = {\n  // ...\n\n  /** @type {import('tailwindcss').Config} */\n  tailwindcss: {\n    content: [\"./src/pages/**/*.{js,ts,jsx,tsx,mdx}\"],\n    theme: {\n      extend: {\n        backgroundImage: {\n          \"gradient-radial\": \"radial-gradient(var(--tw-gradient-stops))\"\n        }\n      }\n    },\n    plugins: [require(\"@tailwindcss/typography\")]\n  }\n}\n```\n\n## Next steps\n\nIf you like this idea, submit a PR to your favorite build tool! If you don't, let me know why on Twitter. Or don't and just keep going about your life.\n\n_If you liked the article, don't forget to share it and follow me at [@nebrelbug](https://twitter.com/nebrelbug) on Twitter._",
    "title": "One Config File to Rule Them All",
    "_meta": {
      "filePath": "global-config-js.mdx",
      "fileName": "global-config-js.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "global-config-js"
    },
    "mdx": "var Component=(()=>{var g=Object.create;var r=Object.defineProperty;var f=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var j=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),b=(i,e)=>{for(var o in e)r(i,o,{get:e[o],enumerable:!0})},c=(i,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of p(e))!m.call(i,t)&&t!==o&&r(i,t,{get:()=>e[t],enumerable:!(s=f(e,t))||s.enumerable});return i};var w=(i,e,o)=>(o=i!=null?g(u(i)):{},c(e||!i||!i.__esModule?r(o,\"default\",{value:i,enumerable:!0}):o,i)),x=i=>c(r({},\"__esModule\",{value:!0}),i);var a=j((v,l)=>{l.exports=_jsx_runtime});var y={};b(y,{default:()=>h});var n=w(a());function d(i){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",p:\"p\",pre:\"pre\",...i.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Modern web development involves working with multiple JS build tools and frameworks, each requiring their own configuration files. Managing these configuration files, such as \",(0,n.jsx)(e.code,{children:\".eslintrc\"}),\", \",(0,n.jsx)(e.code,{children:\"next.config.js\"}),\", and \",(0,n.jsx)(e.code,{children:\"tailwind.config.js\"}),\", can become cumbersome and time-consuming. In this blog post, I'll explore the idea of combining these configuration files into a single file called \",(0,n.jsx)(e.code,{children:\"global.config.js\"}),\", centralizing project configuration and reducing distractions.\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Config files everywhere\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"As I'm writing this blog post, my project root contains an \",(0,n.jsx)(e.code,{children:\".eslintrc.json\"}),\", \",(0,n.jsx)(e.code,{children:\"next.config.js\"}),\", \",(0,n.jsx)(e.code,{children:\"postcss.config.js\"}),\", \",(0,n.jsx)(e.code,{children:\"tailwind.config.js\"}),\", and \",(0,n.jsx)(e.code,{children:\"tsconfig.json\"}),\". Although my configuration is pretty out-of-the-box and each file is less than 30 lines, those files take up valuable space in my VSCode sidebar and distract from what's important: my source code.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"My case is far from exceptional. Some other projects use far more configuration files. Imagine how cluttered a project can get when you add a \",(0,n.jsx)(e.code,{children:\".babelrc\"}),\", \",(0,n.jsx)(e.code,{children:\"prettier.config.js\"}),\", \",(0,n.jsx)(e.code,{children:\"jest.config.js\"}),\", \",(0,n.jsx)(e.code,{children:\"cypress.json\"}),\", etc.\"]}),`\n`,(0,n.jsxs)(e.h2,{children:[\"The solution: \",(0,n.jsx)(e.code,{children:\"global.config.js\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"I'd like to propose a simple solution: consolidating configuration files in a file called \",(0,n.jsx)(e.code,{children:\"global.config.js\"}),\". The configuration for each tool would be stored in the exported object, under a key with the name of the npm package.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Projects should still allow usage of individual configuration files for cases when configuration is large and complex (e.g. \",(0,n.jsx)(e.code,{children:\"tsconfig.json\"}),\"), but should first scan to see if a \",(0,n.jsx)(e.code,{children:\"global.config.js\"}),\" exists and configuration for their tool is present.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Here's what a simple \",(0,n.jsx)(e.code,{children:\"global.config.js\"}),\" might look like:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-js\",children:`module.exports = {\n  eslint: {\n    extends: [\"next/core-web-vitals\"]\n  },\n  postcss: {\n    plugins: {\n      tailwindcss: {},\n      autoprefixer: {}\n    }\n  },\n  tailwindcss: {\n    content: [\"./src/pages/**/*.{js,ts,jsx,tsx,mdx}\"],\n    theme: {\n      extend: {\n        backgroundImage: {\n          \"gradient-radial\": \"radial-gradient(var(--tw-gradient-stops))\"\n        }\n      }\n    },\n    plugins: [require(\"@tailwindcss/typography\")]\n  }\n}\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"But what about types?\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Type completion can be easily enabled for \",(0,n.jsx)(e.code,{children:\"global.config.js\"}),\" by adding type definitions in comments, like Tailwind and NextJS already do in their config files.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-js\",children:`module.exports = {\n  // ...\n\n  /** @type {import('tailwindcss').Config} */\n  tailwindcss: {\n    content: [\"./src/pages/**/*.{js,ts,jsx,tsx,mdx}\"],\n    theme: {\n      extend: {\n        backgroundImage: {\n          \"gradient-radial\": \"radial-gradient(var(--tw-gradient-stops))\"\n        }\n      }\n    },\n    plugins: [require(\"@tailwindcss/typography\")]\n  }\n}\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Next steps\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you like this idea, submit a PR to your favorite build tool! If you don't, let me know why on Twitter. Or don't and just keep going about your life.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function h(i={}){let{wrapper:e}=i.components||{};return e?(0,n.jsx)(e,{...i,children:(0,n.jsx)(d,{...i})}):d(i)}return x(y);})();\n;return Component;"
  },
  {
    "content": "## TL;DR\n\n`gom` stands for GPU Output Monitor. It's a pip package that provides a CLI for monitoring GPU usage. Think of it as `nvidia-smi`, but faster and more minimalist. And it has a bonus feature: **in environments where Docker containers are using GPUs, it will break down usage by container**! (Don't worry, it also works in environments without Docker and even inside Docker containers.)\n\n_I owe my colleague [Vin](https://howe.vin/) credit for inspiring this project. He used GPT-4 to create an initial prototype in Bash, but I had to rewrite from scratch due to bugs and performance issues._\n\n## Instructions\n\n1. Run `pip3 install gom`\n2. Depending on your CUDA version, install the correct version of `pynvml`\n3. Run `gom show` (to show usage once) or `gom watch` (to monitor usage, updated roughly every second)\n\n## Comparing `gom` and `nvidia-smi`\n\nI think the results speak for themselves :). This first screenshot is the result of running `gom watch`. You can see that four different Docker containers, `r0`, `r1`, `r2`, and `r3`, are each using a GPU quite heavily. There's also slight usage of all GPUs that's not coming from any container.\n\n![output of running `gom watch` command](/blog-images/gom-watch.png)\n\nThis second screenshot is the result of running `nvidia-smi`. It's complex and unnecessarily verbose. In more space than `gom`, it only manages to show information for 8 GPUs!\n\n![output of running `nvidia-smi` command](/blog-images/nvidia-smi.png)\n\n## Conclusion\n\nI created `gom` because I wanted to monitor GPU usage across different Docker containers. I use it frequently when doing ML tasks because it's fast and the output fits on a small terminal. Hopefully it's helpful for you. If you have suggestions, feel free to open an issue at the [GitHub repo](https://github.com/nebrelbug/gom).\n\n_If you liked the article, don't forget to share it and follow me at [@nebrelbug](https://twitter.com/nebrelbug) on Twitter._",
    "title": "An nvidia-smi Replacement for Monitoring GPU Usage across Containers",
    "_meta": {
      "filePath": "gom-gpu-monitor-nvidia-smi-replacement.mdx",
      "fileName": "gom-gpu-monitor-nvidia-smi-replacement.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "gom-gpu-monitor-nvidia-smi-replacement"
    },
    "mdx": "var Component=(()=>{var u=Object.create;var t=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var w=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),b=(o,e)=>{for(var i in e)t(o,i,{get:e[i],enumerable:!0})},c=(o,e,i,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of g(e))!p.call(o,r)&&r!==i&&t(o,r,{get:()=>e[r],enumerable:!(s=m(e,r))||s.enumerable});return o};var v=(o,e,i)=>(i=o!=null?u(f(o)):{},c(e||!o||!o.__esModule?t(i,\"default\",{value:o,enumerable:!0}):i,o)),y=o=>c(t({},\"__esModule\",{value:!0}),o);var d=w((D,a)=>{a.exports=_jsx_runtime});var k={};b(k,{default:()=>l});var n=v(d());function h(o){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",img:\"img\",li:\"li\",ol:\"ol\",p:\"p\",strong:\"strong\",...o.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"TL;DR\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.code,{children:\"gom\"}),\" stands for GPU Output Monitor. It's a pip package that provides a CLI for monitoring GPU usage. Think of it as \",(0,n.jsx)(e.code,{children:\"nvidia-smi\"}),\", but faster and more minimalist. And it has a bonus feature: \",(0,n.jsx)(e.strong,{children:\"in environments where Docker containers are using GPUs, it will break down usage by container\"}),\"! (Don't worry, it also works in environments without Docker and even inside Docker containers.)\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"I owe my colleague \",(0,n.jsx)(e.a,{href:\"https://howe.vin/\",children:\"Vin\"}),\" credit for inspiring this project. He used GPT-4 to create an initial prototype in Bash, but I had to rewrite from scratch due to bugs and performance issues.\"]})}),`\n`,(0,n.jsx)(e.h2,{children:\"Instructions\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Run \",(0,n.jsx)(e.code,{children:\"pip3 install gom\"})]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Depending on your CUDA version, install the correct version of \",(0,n.jsx)(e.code,{children:\"pynvml\"})]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Run \",(0,n.jsx)(e.code,{children:\"gom show\"}),\" (to show usage once) or \",(0,n.jsx)(e.code,{children:\"gom watch\"}),\" (to monitor usage, updated roughly every second)\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{children:[\"Comparing \",(0,n.jsx)(e.code,{children:\"gom\"}),\" and \",(0,n.jsx)(e.code,{children:\"nvidia-smi\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"I think the results speak for themselves :). This first screenshot is the result of running \",(0,n.jsx)(e.code,{children:\"gom watch\"}),\". You can see that four different Docker containers, \",(0,n.jsx)(e.code,{children:\"r0\"}),\", \",(0,n.jsx)(e.code,{children:\"r1\"}),\", \",(0,n.jsx)(e.code,{children:\"r2\"}),\", and \",(0,n.jsx)(e.code,{children:\"r3\"}),\", are each using a GPU quite heavily. There's also slight usage of all GPUs that's not coming from any container.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/gom-watch.png\",alt:\"output of running gom watch command\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"This second screenshot is the result of running \",(0,n.jsx)(e.code,{children:\"nvidia-smi\"}),\". It's complex and unnecessarily verbose. In more space than \",(0,n.jsx)(e.code,{children:\"gom\"}),\", it only manages to show information for 8 GPUs!\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/nvidia-smi.png\",alt:\"output of running nvidia-smi command\"})}),`\n`,(0,n.jsx)(e.h2,{children:\"Conclusion\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"I created \",(0,n.jsx)(e.code,{children:\"gom\"}),\" because I wanted to monitor GPU usage across different Docker containers. I use it frequently when doing ML tasks because it's fast and the output fits on a small terminal. Hopefully it's helpful for you. If you have suggestions, feel free to open an issue at the \",(0,n.jsx)(e.a,{href:\"https://github.com/nebrelbug/gom\",children:\"GitHub repo\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function l(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,{...o,children:(0,n.jsx)(h,{...o})}):h(o)}return y(k);})();\n;return Component;"
  },
  {
    "content": "## Background\n\nToday, Eta is my most popular and widely used open-source project. But when I first published it 3 years ago, it wasn't one of my primary focuses. In fact, I created Eta as a slimmed-down variation of [Squirrelly](https://squirrelly.js.org), a more complex template engine with features like helpers and filters.\n\nAs time passed, I realized that for most projects, an embedded template engine was actually a better fit than something more complex. Projects which needed complex or client-side HTML processing typically used a framework like React or Vue. Eta's performance and low bundle size, meanwhile, made it a great fit for projects which needed fast processing, low memory usage, or to handle non-XML languages.\n\nAt the same time, Eta had become increasingly popular thanks to its speed, Deno support, and syntactic advantages over EJS. Given those factors, I decided to make Eta my main focus. I spent time writing tutorials, fixing issues, and polishing documentation.\n\nAfter several years and some time spent away from programming as a missionary, I finally had time to work on Eta again. I decided to make some big changes to the project, including the build system, API, and documentation.\n\n## Build System Updates\n\nDespite Eta's advantages and features, it had some big problems. One such problem was the build system. Complex and unwieldy, it was difficult to maintain and update. I dealt with complex configuration files and the necessity of transpiling a version specifically for Deno.\n\nChanges in version 3:\n\n- Using [microbundle](https://github.com/developit/microbundle) to bundle the library helped me avoid the need for complex configuration files.\n- Using GitHub Actions to run tests and collect coverage allowed me to consolidate the services I used.\n- By setting `allowImportingTsExtensions: true` in `tsconfig.json`, I was able to avoid using [Denoify](https://github.com/garronej/denoify) for a separate Deno build.\n\n## API Changes\n\nAnother problem was the API. Simple methods like `eta.render()` had many function overloads, making types difficult to infer and usage unintuitive. A custom configuration object could be passed in when calling user-exposed functions like `render`, `parse`, and `compile`. In practice, that meant the user-provided configuration had to be merged with the default configuration every time any of those functions was called.\n\nChanges in version 3:\n\n- There's only one export, a named class called `Eta`. This class has a single constructor, which processes a configuration object and generates template caches at instantiation time.\n- The `render()` and `renderAsync()` functions now have a single function signature.\n  - In Eta v2, `render()` and `renderAsync()` could be used to render either named templates or template strings. Eta v3 introduces two new functions to render template strings: `renderString()` and `renderStringAsync()`.\n- The `readFile()` and `resolvePath()` functions, which Eta uses internally, can be overridden as class methods by the user.\n- Internal variables and methods inside each compiled template are stored in the `__eta` object, rather than across several variables including `__res`.\n- Rather than allowing users to specify one `root` directory and multiple `views` directories, users may just specify a single `views` directory. This directory is used as the root directory for all template resolution. All template files must be inside this directory or a subdirectory of it, improving template security and reducing expensive file-lookup operations.\n\n## Developer Experience Changes\n\nOne of the biggest changes in Eta v3 was the addition of detailed runtime errors (inspired by EJS). Consider a template like the following, which will throw because of an undefined variable:\n\n```eta\nTemplate header\n<%= undefinedVariable %>\nLorem Ipsum\n```\n\nEta v2 would throw an error with some generic info, but it wasn't incredibly helpful. In contrast, Eta v3 throws a detailed error with the template name, line number, and error message:\n\n```\nEtaError [ReferenceError]: .../my-dir/templates/runtime-error.eta:2\n    1| Template header\n >> 2| <%= undefinedVariable %>\n    3| Lorem Ipsum\n\nundefinedVariable is not defined\n```\n\n## Documentation Changes\n\nThe documentation for Eta v2 was extensive but very difficult to navigate. Information about the project was split over 40+ (!) documentation pages, found in multiple folders spread across 3 different website sections.\n\nThe documentation for Eta v3 takes up 9 pages, all found in the same part of the website ([eta.js.org](https://eta.js.org)). Topics like template syntax and API overview are covered in a single page, rather than being split across multiple pages.\n\n## The Future of Eta\n\nI'm proud of the changes included in Eta v3, and of the project as a whole. Much thanks to those who contributed to the project through PRs, issues, and suggestions. Additional thanks to projects like [ejs](https://github.com/mde/ejs), from which Eta continues to draw inspiration.\n\nI see Eta as mostly feature-complete at this point, though I'll continue to fix bugs and add some small features. I'd encourage current users of the library to upgrade to v3, and I hope new users will find Eta to be a great fit for their projects.\n\n## Links\n\n- [Eta on GitHub](https://github.com/eta-dev/eta)\n- [Eta on npm](https://www.npmjs.com/package/eta)\n- [Eta website and docs](https://eta.js.org)\n\n_If you liked the article, don't forget to share it and follow me at [@nebrelbug](https://twitter.com/nebrelbug) on Twitter._",
    "title": "Introducing Eta v3",
    "_meta": {
      "filePath": "introducing-eta-v3.mdx",
      "fileName": "introducing-eta-v3.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "introducing-eta-v3"
    },
    "mdx": "var Component=(()=>{var u=Object.create;var a=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var w=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),b=(t,e)=>{for(var i in e)a(t,i,{get:e[i],enumerable:!0})},s=(t,e,i,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of m(e))!g.call(t,r)&&r!==i&&a(t,r,{get:()=>e[r],enumerable:!(o=p(e,r))||o.enumerable});return t};var y=(t,e,i)=>(i=t!=null?u(f(t)):{},s(e||!t||!t.__esModule?a(i,\"default\",{value:t,enumerable:!0}):i,t)),v=t=>s(a({},\"__esModule\",{value:!0}),t);var l=w((j,d)=>{d.exports=_jsx_runtime});var E={};b(E,{default:()=>h});var n=y(l());function c(t){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",li:\"li\",p:\"p\",pre:\"pre\",ul:\"ul\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Background\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Today, Eta is my most popular and widely used open-source project. But when I first published it 3 years ago, it wasn't one of my primary focuses. In fact, I created Eta as a slimmed-down variation of \",(0,n.jsx)(e.a,{href:\"https://squirrelly.js.org\",children:\"Squirrelly\"}),\", a more complex template engine with features like helpers and filters.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As time passed, I realized that for most projects, an embedded template engine was actually a better fit than something more complex. Projects which needed complex or client-side HTML processing typically used a framework like React or Vue. Eta's performance and low bundle size, meanwhile, made it a great fit for projects which needed fast processing, low memory usage, or to handle non-XML languages.\"}),`\n`,(0,n.jsx)(e.p,{children:\"At the same time, Eta had become increasingly popular thanks to its speed, Deno support, and syntactic advantages over EJS. Given those factors, I decided to make Eta my main focus. I spent time writing tutorials, fixing issues, and polishing documentation.\"}),`\n`,(0,n.jsx)(e.p,{children:\"After several years and some time spent away from programming as a missionary, I finally had time to work on Eta again. I decided to make some big changes to the project, including the build system, API, and documentation.\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Build System Updates\"}),`\n`,(0,n.jsx)(e.p,{children:\"Despite Eta's advantages and features, it had some big problems. One such problem was the build system. Complex and unwieldy, it was difficult to maintain and update. I dealt with complex configuration files and the necessity of transpiling a version specifically for Deno.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Changes in version 3:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Using \",(0,n.jsx)(e.a,{href:\"https://github.com/developit/microbundle\",children:\"microbundle\"}),\" to bundle the library helped me avoid the need for complex configuration files.\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Using GitHub Actions to run tests and collect coverage allowed me to consolidate the services I used.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"By setting \",(0,n.jsx)(e.code,{children:\"allowImportingTsExtensions: true\"}),\" in \",(0,n.jsx)(e.code,{children:\"tsconfig.json\"}),\", I was able to avoid using \",(0,n.jsx)(e.a,{href:\"https://github.com/garronej/denoify\",children:\"Denoify\"}),\" for a separate Deno build.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"API Changes\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Another problem was the API. Simple methods like \",(0,n.jsx)(e.code,{children:\"eta.render()\"}),\" had many function overloads, making types difficult to infer and usage unintuitive. A custom configuration object could be passed in when calling user-exposed functions like \",(0,n.jsx)(e.code,{children:\"render\"}),\", \",(0,n.jsx)(e.code,{children:\"parse\"}),\", and \",(0,n.jsx)(e.code,{children:\"compile\"}),\". In practice, that meant the user-provided configuration had to be merged with the default configuration every time any of those functions was called.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Changes in version 3:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"There's only one export, a named class called \",(0,n.jsx)(e.code,{children:\"Eta\"}),\". This class has a single constructor, which processes a configuration object and generates template caches at instantiation time.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"The \",(0,n.jsx)(e.code,{children:\"render()\"}),\" and \",(0,n.jsx)(e.code,{children:\"renderAsync()\"}),\" functions now have a single function signature.\",`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"In Eta v2, \",(0,n.jsx)(e.code,{children:\"render()\"}),\" and \",(0,n.jsx)(e.code,{children:\"renderAsync()\"}),\" could be used to render either named templates or template strings. Eta v3 introduces two new functions to render template strings: \",(0,n.jsx)(e.code,{children:\"renderString()\"}),\" and \",(0,n.jsx)(e.code,{children:\"renderStringAsync()\"}),\".\"]}),`\n`]}),`\n`]}),`\n`,(0,n.jsxs)(e.li,{children:[\"The \",(0,n.jsx)(e.code,{children:\"readFile()\"}),\" and \",(0,n.jsx)(e.code,{children:\"resolvePath()\"}),\" functions, which Eta uses internally, can be overridden as class methods by the user.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Internal variables and methods inside each compiled template are stored in the \",(0,n.jsx)(e.code,{children:\"__eta\"}),\" object, rather than across several variables including \",(0,n.jsx)(e.code,{children:\"__res\"}),\".\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Rather than allowing users to specify one \",(0,n.jsx)(e.code,{children:\"root\"}),\" directory and multiple \",(0,n.jsx)(e.code,{children:\"views\"}),\" directories, users may just specify a single \",(0,n.jsx)(e.code,{children:\"views\"}),\" directory. This directory is used as the root directory for all template resolution. All template files must be inside this directory or a subdirectory of it, improving template security and reducing expensive file-lookup operations.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.h2,{children:\"Developer Experience Changes\"}),`\n`,(0,n.jsx)(e.p,{children:\"One of the biggest changes in Eta v3 was the addition of detailed runtime errors (inspired by EJS). Consider a template like the following, which will throw because of an undefined variable:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-eta\",children:`Template header\n<%= undefinedVariable %>\nLorem Ipsum\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Eta v2 would throw an error with some generic info, but it wasn't incredibly helpful. In contrast, Eta v3 throws a detailed error with the template name, line number, and error message:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{children:`EtaError [ReferenceError]: .../my-dir/templates/runtime-error.eta:2\n    1| Template header\n >> 2| <%= undefinedVariable %>\n    3| Lorem Ipsum\n\nundefinedVariable is not defined\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Documentation Changes\"}),`\n`,(0,n.jsx)(e.p,{children:\"The documentation for Eta v2 was extensive but very difficult to navigate. Information about the project was split over 40+ (!) documentation pages, found in multiple folders spread across 3 different website sections.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"The documentation for Eta v3 takes up 9 pages, all found in the same part of the website (\",(0,n.jsx)(e.a,{href:\"https://eta.js.org\",children:\"eta.js.org\"}),\"). Topics like template syntax and API overview are covered in a single page, rather than being split across multiple pages.\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"The Future of Eta\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"I'm proud of the changes included in Eta v3, and of the project as a whole. Much thanks to those who contributed to the project through PRs, issues, and suggestions. Additional thanks to projects like \",(0,n.jsx)(e.a,{href:\"https://github.com/mde/ejs\",children:\"ejs\"}),\", from which Eta continues to draw inspiration.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"I see Eta as mostly feature-complete at this point, though I'll continue to fix bugs and add some small features. I'd encourage current users of the library to upgrade to v3, and I hope new users will find Eta to be a great fit for their projects.\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Links\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://github.com/eta-dev/eta\",children:\"Eta on GitHub\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://www.npmjs.com/package/eta\",children:\"Eta on npm\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://eta.js.org\",children:\"Eta website and docs\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function h(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(c,{...t})}):c(t)}return v(E);})();\n;return Component;"
  },
  {
    "content": "## The Problem\n\nLLMs have tremendous potential in many areas, but most contemporary models have one inherent limitation: they're solely feed-forward in structure. This means that data flows linearly from input to output, with no recursion or backtracking. This enables incredibly fast and efficient training using gradient descent and back-propagation. Computations can be done in parallel using matrix multiplication.\n\nUnfortunately, their lack of recursion makes some types of mathematical operations impossible. Consider exponentiation. ChatGPT can handle simple exponent problems, but when asked what X^Y is for high values of X or Y, it becomes inaccurate.\n\nThough exponential operations can be broken down into a linear sequence, it's impossible for a finite, feed-forward neural net to handle any possible recursive operation (i.e., X^Y with any possible value for Y). The amount of recursion an LLM can \"simulate\" is limited by the number of its parameters and layers.\n\n## Summary\n\nLack of recursion is an inherent design limitation in current GPT-style LLMs which prevents them from being able to perform complicated math operations. The fact is, though, that doesn't matter in most use cases for LLMs! They're still powerful and helpful in a wide variety of circumstances.\n\n## Fun Stuff\n\nThere's still a lot of work to be done in understanding the behavior of trained large language models. Here's something fascinating I found while writing this article:\n\nWhen I asked ChatGPT what 7^15 equals, it gave the answer **170,859,375**. The correct answer is **4,747,561,509,943**.\n\nThough the answer is obviously incorrect, **170,859,375** has a unique property: it factors into **(3^7)\\*(5^7)**. The model seems to have converted **A^(B\\*C)** into **(B^A)\\*(C^A)** under the hood. I'd be interested to learn why this happens!\n\n_If you liked the article, don't forget to share it and follow me at [@nebrelbug](https://twitter.com/nebrelbug) on Twitter._",
    "title": "LLMs Will Never Be Able to Do Math",
    "_meta": {
      "filePath": "llms-limitation-recursion.mdx",
      "fileName": "llms-limitation-recursion.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "llms-limitation-recursion"
    },
    "mdx": "var Component=(()=>{var u=Object.create;var o=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var b=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var i in e)o(t,i,{get:e[i],enumerable:!0})},s=(t,e,i,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!g.call(t,r)&&r!==i&&o(t,r,{get:()=>e[r],enumerable:!(a=m(e,r))||a.enumerable});return t};var y=(t,e,i)=>(i=t!=null?u(f(t)):{},s(e||!t||!t.__esModule?o(i,\"default\",{value:t,enumerable:!0}):i,t)),T=t=>s(o({},\"__esModule\",{value:!0}),t);var l=b((k,h)=>{h.exports=_jsx_runtime});var v={};w(v,{default:()=>d});var n=y(l());function c(t){let e={a:\"a\",em:\"em\",h2:\"h2\",p:\"p\",strong:\"strong\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"The Problem\"}),`\n`,(0,n.jsx)(e.p,{children:\"LLMs have tremendous potential in many areas, but most contemporary models have one inherent limitation: they're solely feed-forward in structure. This means that data flows linearly from input to output, with no recursion or backtracking. This enables incredibly fast and efficient training using gradient descent and back-propagation. Computations can be done in parallel using matrix multiplication.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Unfortunately, their lack of recursion makes some types of mathematical operations impossible. Consider exponentiation. ChatGPT can handle simple exponent problems, but when asked what X^Y is for high values of X or Y, it becomes inaccurate.\"}),`\n`,(0,n.jsx)(e.p,{children:`Though exponential operations can be broken down into a linear sequence, it's impossible for a finite, feed-forward neural net to handle any possible recursive operation (i.e., X^Y with any possible value for Y). The amount of recursion an LLM can \"simulate\" is limited by the number of its parameters and layers.`}),`\n`,(0,n.jsx)(e.h2,{children:\"Summary\"}),`\n`,(0,n.jsx)(e.p,{children:\"Lack of recursion is an inherent design limitation in current GPT-style LLMs which prevents them from being able to perform complicated math operations. The fact is, though, that doesn't matter in most use cases for LLMs! They're still powerful and helpful in a wide variety of circumstances.\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Fun Stuff\"}),`\n`,(0,n.jsx)(e.p,{children:\"There's still a lot of work to be done in understanding the behavior of trained large language models. Here's something fascinating I found while writing this article:\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"When I asked ChatGPT what 7^15 equals, it gave the answer \",(0,n.jsx)(e.strong,{children:\"170,859,375\"}),\". The correct answer is \",(0,n.jsx)(e.strong,{children:\"4,747,561,509,943\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Though the answer is obviously incorrect, \",(0,n.jsx)(e.strong,{children:\"170,859,375\"}),\" has a unique property: it factors into \",(0,n.jsx)(e.strong,{children:\"(3^7)*(5^7)\"}),\". The model seems to have converted \",(0,n.jsx)(e.strong,{children:\"A^(B*C)\"}),\" into \",(0,n.jsx)(e.strong,{children:\"(B^A)*(C^A)\"}),\" under the hood. I'd be interested to learn why this happens!\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function d(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(c,{...t})}):c(t)}return T(v);})();\n;return Component;"
  },
  {
    "content": "Historically, more attention has been paid to distributed training than to distributed inference. After all, training is more computationally expensive. Larger and more complex LLMs, however, can take a long time to perform text completion tasks. Whether for research or in production, it's valuable to parallelize inference in order to maximize performance.\n\nIt's important to recognize that there's a difference between distributing the weights of one model across multiple GPUs and distributing model prompts or inputs across multiple models. The first is relatively simple, while the latter (which I'll be focusing on) is slightly more involved.\n\nA week ago, in version 0.20.0, [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/index) released a feature that significantly simplifies multi-GPU inference: `Accelerator.split_between_processes()`. It's based on `torch.distributed`, but is much simpler to use.\n\nLet's look at how we can use this new feature with LLaMA. The code will be written assuming that you've saved LLaMA weights in the [Hugging Face Transformers format](https://huggingface.co/docs/transformers/main/model_doc/llama).\n\nFirst, start out by importing the required modules and initializing the tokenizer and model.\n\n```python\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\n\nMODEL_PATH = \"path-to-llama-model\"\n\ntokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n\nmodel = LlamaForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\")\n```\n\nNotice how we pass in `device_map=\"auto\"`. This allows Accelerate to evenly spread out the model's weights across available GPUs.\n\nIf we wanted, we could run `model.to(accelerator.device)`. This would transfer the model to a specific GPU. `accelerator.device` will be different for each process running in parallel, so you could have a model loaded onto GPU 0, another loaded onto GPU 1, etc. In this case, though, we'll stick with `device_map=\"auto\"`. This allows us to use larger models than could fit on a single GPU.\n\nNext, we'll write the code to perform inference!\n\n```python\n\ndata = [\n    \"a dog\",\n    \"a cat\",\n    \"a vole\",\n    \"a bat\",\n    \"a bird\",\n    \"a fish\",\n    \"a horse\",\n    \"a cow\",\n    \"a sheep\",\n    \"a goat\",\n    \"a pig\",\n    \"a chicken\",\n]\n\n# Accelerator will automatically split this data up between each running process.\n# The above array has 12 items. So if we had 4 processes, each process\n# would be assigned 3 strings as prompts.\n\nwith accelerator.split_between_processes(data,) as prompts:\n    for prompt in prompts:\n\n        # move the tensor to a GPU, since it needs to be on CUDA\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(accelerator.device)\n\n        # inference\n        generate_ids = model.generate(**inputs, max_length=30)\n\n        # decoding\n        result = tokenizer.batch_decode(\n            generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )[0]\n\n        # print the process number and inference result\n        print(\n            f\"Process {accelerator.process_index} - \"\n            + result.replace(\"\\n\", \"\\\\n\")\n        )\n\n```\n\nFinally, all that remains is to launch Accelerate using the Accelerate CLI:\n\n```\naccelerate launch --num_processes=4 script.py\n```\n\nWhen we run the above code, 4 copies of the model are loaded across available GPUs. Our prompts are evenly split across the 4 models, which significantly improves performance.\n\nThe output of the above code (after logs from loading the models) should look like this:\n\n```\nProcess 1 - a bathtub with a shower head a bathtub with a shower head bathtub with shower head and handheld\nProcess 0 - a dog's life, a dog's life, a dog's life, a dog's life, a dog's life\nProcess 1 - a bird in the hand is worth two in the bush.\\na bird in the hand is worth two in the bush.\\na bird in\nProcess 2 - a horse, a horse, my kingdom for a horse!\\na horse, a horse, my kingdom for a horse!\\na horse,\nProcess 3 - a goat, a sheep, a cow, a pig, a dog, a cat, a horse, a chicken, a du\nProcess 0 - a catastrophic event that will change the world forever.\\nThe world is in the grip of a global pandemic.\\nThe\nProcess 1 - a fishing trip to the Bahamas.\\nI'm not sure if I'm going to be able to make it.\\n\nProcess 2 - a coworker of mine is a big fan of the show and he's been trying to get me to watch it. I've\nProcess 3 - a piggy bank, a piggy bank, a piggy bank, a piggy bank, a piggy bank\nProcess 0 - a vole, a mouse, a shrew, a hamster, a gerbil, a guinea pig, a rabbit,\nProcess 2 - a sheep, a goat, a ram, a bullock, a he-lamb, a turtle-dove, and\nProcess 3 - a chicken in every pot, a car in every garage, a house in every backyard, a job for every man, a college\n```\n\nI hope this was helpful! You can learn more about Accelerate and distributed inference in the Accelerate documentation [here](https://huggingface.co/docs/accelerate/usage_guides/distributed_inference) if you're interested.\n\nIf you liked the article, don't forget to share it and follow me at [@nebrelbug](https://twitter.com/nebrelbug) on Twitter.",
    "title": "Multi-GPU Inference with Accelerate",
    "_meta": {
      "filePath": "multi-gpu-inference-with-accelerate.mdx",
      "fileName": "multi-gpu-inference-with-accelerate.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "multi-gpu-inference-with-accelerate"
    },
    "mdx": "var Component=(()=>{var p=Object.create;var o=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),w=(n,e)=>{for(var t in e)o(n,t,{get:e[t],enumerable:!0})},s=(n,e,t,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of u(e))!f.call(n,r)&&r!==t&&o(n,r,{get:()=>e[r],enumerable:!(i=m(e,r))||i.enumerable});return n};var v=(n,e,t)=>(t=n!=null?p(g(n)):{},s(e||!n||!n.__esModule?o(t,\"default\",{value:n,enumerable:!0}):t,n)),_=n=>s(o({},\"__esModule\",{value:!0}),n);var l=b((P,c)=>{c.exports=_jsx_runtime});var y={};w(y,{default:()=>d});var a=v(l());function h(n){let e={a:\"a\",code:\"code\",p:\"p\",pre:\"pre\",...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.p,{children:\"Historically, more attention has been paid to distributed training than to distributed inference. After all, training is more computationally expensive. Larger and more complex LLMs, however, can take a long time to perform text completion tasks. Whether for research or in production, it's valuable to parallelize inference in order to maximize performance.\"}),`\n`,(0,a.jsx)(e.p,{children:\"It's important to recognize that there's a difference between distributing the weights of one model across multiple GPUs and distributing model prompts or inputs across multiple models. The first is relatively simple, while the latter (which I'll be focusing on) is slightly more involved.\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"A week ago, in version 0.20.0, \",(0,a.jsx)(e.a,{href:\"https://huggingface.co/docs/accelerate/index\",children:\"HuggingFace Accelerate\"}),\" released a feature that significantly simplifies multi-GPU inference: \",(0,a.jsx)(e.code,{children:\"Accelerator.split_between_processes()\"}),\". It's based on \",(0,a.jsx)(e.code,{children:\"torch.distributed\"}),\", but is much simpler to use.\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"Let's look at how we can use this new feature with LLaMA. The code will be written assuming that you've saved LLaMA weights in the \",(0,a.jsx)(e.a,{href:\"https://huggingface.co/docs/transformers/main/model_doc/llama\",children:\"Hugging Face Transformers format\"}),\".\"]}),`\n`,(0,a.jsx)(e.p,{children:\"First, start out by importing the required modules and initializing the tokenizer and model.\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:\"language-python\",children:`from transformers import LlamaForCausalLM, LlamaTokenizer\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\n\nMODEL_PATH = \"path-to-llama-model\"\n\ntokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n\nmodel = LlamaForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\")\n`})}),`\n`,(0,a.jsxs)(e.p,{children:[\"Notice how we pass in \",(0,a.jsx)(e.code,{children:'device_map=\"auto\"'}),\". This allows Accelerate to evenly spread out the model's weights across available GPUs.\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"If we wanted, we could run \",(0,a.jsx)(e.code,{children:\"model.to(accelerator.device)\"}),\". This would transfer the model to a specific GPU. \",(0,a.jsx)(e.code,{children:\"accelerator.device\"}),\" will be different for each process running in parallel, so you could have a model loaded onto GPU 0, another loaded onto GPU 1, etc. In this case, though, we'll stick with \",(0,a.jsx)(e.code,{children:'device_map=\"auto\"'}),\". This allows us to use larger models than could fit on a single GPU.\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Next, we'll write the code to perform inference!\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:\"language-python\",children:`\ndata = [\n    \"a dog\",\n    \"a cat\",\n    \"a vole\",\n    \"a bat\",\n    \"a bird\",\n    \"a fish\",\n    \"a horse\",\n    \"a cow\",\n    \"a sheep\",\n    \"a goat\",\n    \"a pig\",\n    \"a chicken\",\n]\n\n# Accelerator will automatically split this data up between each running process.\n# The above array has 12 items. So if we had 4 processes, each process\n# would be assigned 3 strings as prompts.\n\nwith accelerator.split_between_processes(data,) as prompts:\n    for prompt in prompts:\n\n        # move the tensor to a GPU, since it needs to be on CUDA\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(accelerator.device)\n\n        # inference\n        generate_ids = model.generate(**inputs, max_length=30)\n\n        # decoding\n        result = tokenizer.batch_decode(\n            generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )[0]\n\n        # print the process number and inference result\n        print(\n            f\"Process {accelerator.process_index} - \"\n            + result.replace(\"\\\\n\", \"\\\\\\\\n\")\n        )\n\n`})}),`\n`,(0,a.jsx)(e.p,{children:\"Finally, all that remains is to launch Accelerate using the Accelerate CLI:\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:`accelerate launch --num_processes=4 script.py\n`})}),`\n`,(0,a.jsx)(e.p,{children:\"When we run the above code, 4 copies of the model are loaded across available GPUs. Our prompts are evenly split across the 4 models, which significantly improves performance.\"}),`\n`,(0,a.jsx)(e.p,{children:\"The output of the above code (after logs from loading the models) should look like this:\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:`Process 1 - a bathtub with a shower head a bathtub with a shower head bathtub with shower head and handheld\nProcess 0 - a dog's life, a dog's life, a dog's life, a dog's life, a dog's life\nProcess 1 - a bird in the hand is worth two in the bush.\\\\na bird in the hand is worth two in the bush.\\\\na bird in\nProcess 2 - a horse, a horse, my kingdom for a horse!\\\\na horse, a horse, my kingdom for a horse!\\\\na horse,\nProcess 3 - a goat, a sheep, a cow, a pig, a dog, a cat, a horse, a chicken, a du\nProcess 0 - a catastrophic event that will change the world forever.\\\\nThe world is in the grip of a global pandemic.\\\\nThe\nProcess 1 - a fishing trip to the Bahamas.\\\\nI'm not sure if I'm going to be able to make it.\\\\n\nProcess 2 - a coworker of mine is a big fan of the show and he's been trying to get me to watch it. I've\nProcess 3 - a piggy bank, a piggy bank, a piggy bank, a piggy bank, a piggy bank\nProcess 0 - a vole, a mouse, a shrew, a hamster, a gerbil, a guinea pig, a rabbit,\nProcess 2 - a sheep, a goat, a ram, a bullock, a he-lamb, a turtle-dove, and\nProcess 3 - a chicken in every pot, a car in every garage, a house in every backyard, a job for every man, a college\n`})}),`\n`,(0,a.jsxs)(e.p,{children:[\"I hope this was helpful! You can learn more about Accelerate and distributed inference in the Accelerate documentation \",(0,a.jsx)(e.a,{href:\"https://huggingface.co/docs/accelerate/usage_guides/distributed_inference\",children:\"here\"}),\" if you're interested.\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,a.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})]})}function d(n={}){let{wrapper:e}=n.components||{};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(h,{...n})}):h(n)}return _(y);})();\n;return Component;"
  },
  {
    "content": "I'm excited to present the first version of my new personal website!\n\nI built this website using [Next.js 13](https://nextjs.org/), with the new App Router architecture. Other tools I used include [Tailwind CSS](https://tailwindcss.com/), [MDX](https://mdxjs.com/), and [Bright](https://github.com/code-hike/bright). I'm hosting it on [Vercel](https://vercel.com/).\n\nI hope to post more about the process of building this website soon. In the meantime, you can check out the [source code](https://github.com/nebrelbug/bengubler.com).\n\nI'm grateful for the official NextJS docs and [tutorial](https://nextjs.org/learn/basics/create-nextjs-app), which I used heavily; [this fantastic blog post by Max Leiter](https://maxleiter.com/blog/build-a-blog-with-nextjs-13#sitemap-support-sitemapjs), about using the new App Router architecture; and the [Precedent theme](https://precedent.dev/), from which I borrowed my header component.",
    "title": "Introducing My New Website",
    "_meta": {
      "filePath": "my-new-website.mdx",
      "fileName": "my-new-website.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "my-new-website"
    },
    "mdx": "var Component=(()=>{var p=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,b=Object.prototype.hasOwnProperty;var g=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),x=(t,e)=>{for(var r in e)i(t,r,{get:e[r],enumerable:!0})},s=(t,e,r,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let h of m(e))!b.call(t,h)&&h!==r&&i(t,h,{get:()=>e[h],enumerable:!(o=u(e,h))||o.enumerable});return t};var w=(t,e,r)=>(r=t!=null?p(f(t)):{},s(e||!t||!t.__esModule?i(r,\"default\",{value:t,enumerable:!0}):r,t)),j=t=>s(i({},\"__esModule\",{value:!0}),t);var a=g((y,c)=>{c.exports=_jsx_runtime});var I={};x(I,{default:()=>l});var n=w(a());function d(t){let e={a:\"a\",p:\"p\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"I'm excited to present the first version of my new personal website!\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"I built this website using \",(0,n.jsx)(e.a,{href:\"https://nextjs.org/\",children:\"Next.js 13\"}),\", with the new App Router architecture. Other tools I used include \",(0,n.jsx)(e.a,{href:\"https://tailwindcss.com/\",children:\"Tailwind CSS\"}),\", \",(0,n.jsx)(e.a,{href:\"https://mdxjs.com/\",children:\"MDX\"}),\", and \",(0,n.jsx)(e.a,{href:\"https://github.com/code-hike/bright\",children:\"Bright\"}),\". I'm hosting it on \",(0,n.jsx)(e.a,{href:\"https://vercel.com/\",children:\"Vercel\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"I hope to post more about the process of building this website soon. In the meantime, you can check out the \",(0,n.jsx)(e.a,{href:\"https://github.com/nebrelbug/bengubler.com\",children:\"source code\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"I'm grateful for the official NextJS docs and \",(0,n.jsx)(e.a,{href:\"https://nextjs.org/learn/basics/create-nextjs-app\",children:\"tutorial\"}),\", which I used heavily; \",(0,n.jsx)(e.a,{href:\"https://maxleiter.com/blog/build-a-blog-with-nextjs-13#sitemap-support-sitemapjs\",children:\"this fantastic blog post by Max Leiter\"}),\", about using the new App Router architecture; and the \",(0,n.jsx)(e.a,{href:\"https://precedent.dev/\",children:\"Precedent theme\"}),\", from which I borrowed my header component.\"]})]})}function l(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(d,{...t})}):d(t)}return j(I);})();\n;return Component;"
  },
  {
    "content": "In the lab I work in, we have access to a High Performance Computing (HPC) environment that uses the [Slurm Workload Manager](https://slurm.schedmd.com/documentation.html).\n\nI've been using it for a while now, and I've found a few commands that I use all the time. I thought I'd share them here in case they're useful to anyone else.\n\n## Checking job status\n\n```bash\n# View all jobs\nsqueue\n# Check the status of just your jobs\nsqueue -u <username>\n# Check the status of jobs with a specific QOS\nsqueue -q <QOS>\n```\n\n## Cancelling jobs\n\n```bash\n# Cancel a specific job\nscancel <job_id>\n# Cancel all your jobs\nscancel -u <username>\n```\n\n## Requesting a node interactively\n\n```bash\n# Requesting a single node (this will open it up interactively in your terminal)\n# When you exit the terminal, the node will be reallocated\nsalloc --nodes=1 --gpus=8 --qos=<QOS> --mem=2000G --time=72:00:00 --ntasks=1 --cpus-per-task=128\n```\n\n## Submitting a job\n\nWhat if all of your compute nodes are allocated, or you don't want your job to exit as soon as your terminal connection is closed? In that case, you can use `sbatch` to submit a job to the queue. It will automatically run as soon as it can allocate the resources.\n\nThis will take slightly more setup. Assume that the job we actually want to run is contained in `myjob.sh`. In order to submit that script as a job, we'll first create a Bash script that will be run by Slurm. Let's call it `run.sh`:\n\n```bash\n#!/bin/bash\n#SBATCH -J \"JOBNAME\"\n#SBATCH --nodes=1\n#SBATCH --gpus-per-node=8\n#SBATCH --cpus-per-task=128\n#SBATCH --mem=2000G\n#SBATCH --time=72:00:00\n#SBATCH --qos=<QOS>\n\nsrun --nodes=1 myjob.sh\n```\n\nNote that we're using the `#SBATCH` processing directive to pass in the parameters that we would have passed to `salloc` before. We're also using `srun` to run our actual job; it will handle running the script across multiple nodes, if we so desire.\n\nFinally, to launch our script, we'll run:\n\n```bash\nsbatch run.sh\n```\n\n## Conclusion\n\nThat's it! I hope this was helpful. If you have any questions, you can ask ChatGPT or Bard (they'll give either incredibly helpful or completely incorrect answers, but it's worth a shot!)\n\nYou can also look through the [Slurm documentation](https://slurm.schedmd.com/documentation.html) or the [Leo's notes](https://leo.leung.xyz/wiki/Slurm) page on Slurm for more information.\n\n_If you liked the article, don't forget to share it and follow me at [@nebrelbug](https://twitter.com/nebrelbug) on Twitter._",
    "title": "Quick & Helpful Slurm Commands",
    "_meta": {
      "filePath": "quick-helpful-slurm-commands.mdx",
      "fileName": "quick-helpful-slurm-commands.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "quick-helpful-slurm-commands"
    },
    "mdx": "var Component=(()=>{var d=Object.create;var s=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var b=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var o in e)s(t,o,{get:e[o],enumerable:!0})},l=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!g.call(t,a)&&a!==o&&s(t,a,{get:()=>e[a],enumerable:!(r=m(e,a))||r.enumerable});return t};var y=(t,e,o)=>(o=t!=null?d(b(t)):{},l(e||!t||!t.__esModule?s(o,\"default\",{value:t,enumerable:!0}):o,t)),j=t=>l(s({},\"__esModule\",{value:!0}),t);var c=f((k,i)=>{i.exports=_jsx_runtime});var C={};w(C,{default:()=>u});var n=y(c());function h(t){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",p:\"p\",pre:\"pre\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"In the lab I work in, we have access to a High Performance Computing (HPC) environment that uses the \",(0,n.jsx)(e.a,{href:\"https://slurm.schedmd.com/documentation.html\",children:\"Slurm Workload Manager\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"I've been using it for a while now, and I've found a few commands that I use all the time. I thought I'd share them here in case they're useful to anyone else.\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Checking job status\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`# View all jobs\nsqueue\n# Check the status of just your jobs\nsqueue -u <username>\n# Check the status of jobs with a specific QOS\nsqueue -q <QOS>\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Cancelling jobs\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`# Cancel a specific job\nscancel <job_id>\n# Cancel all your jobs\nscancel -u <username>\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Requesting a node interactively\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`# Requesting a single node (this will open it up interactively in your terminal)\n# When you exit the terminal, the node will be reallocated\nsalloc --nodes=1 --gpus=8 --qos=<QOS> --mem=2000G --time=72:00:00 --ntasks=1 --cpus-per-task=128\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Submitting a job\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"What if all of your compute nodes are allocated, or you don't want your job to exit as soon as your terminal connection is closed? In that case, you can use \",(0,n.jsx)(e.code,{children:\"sbatch\"}),\" to submit a job to the queue. It will automatically run as soon as it can allocate the resources.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"This will take slightly more setup. Assume that the job we actually want to run is contained in \",(0,n.jsx)(e.code,{children:\"myjob.sh\"}),\". In order to submit that script as a job, we'll first create a Bash script that will be run by Slurm. Let's call it \",(0,n.jsx)(e.code,{children:\"run.sh\"}),\":\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`#!/bin/bash\n#SBATCH -J \"JOBNAME\"\n#SBATCH --nodes=1\n#SBATCH --gpus-per-node=8\n#SBATCH --cpus-per-task=128\n#SBATCH --mem=2000G\n#SBATCH --time=72:00:00\n#SBATCH --qos=<QOS>\n\nsrun --nodes=1 myjob.sh\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Note that we're using the \",(0,n.jsx)(e.code,{children:\"#SBATCH\"}),\" processing directive to pass in the parameters that we would have passed to \",(0,n.jsx)(e.code,{children:\"salloc\"}),\" before. We're also using \",(0,n.jsx)(e.code,{children:\"srun\"}),\" to run our actual job; it will handle running the script across multiple nodes, if we so desire.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Finally, to launch our script, we'll run:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`sbatch run.sh\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Conclusion\"}),`\n`,(0,n.jsx)(e.p,{children:\"That's it! I hope this was helpful. If you have any questions, you can ask ChatGPT or Bard (they'll give either incredibly helpful or completely incorrect answers, but it's worth a shot!)\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"You can also look through the \",(0,n.jsx)(e.a,{href:\"https://slurm.schedmd.com/documentation.html\",children:\"Slurm documentation\"}),\" or the \",(0,n.jsx)(e.a,{href:\"https://leo.leung.xyz/wiki/Slurm\",children:\"Leo's notes\"}),\" page on Slurm for more information.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function u(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(h,{...t})}):h(t)}return j(C);})();\n;return Component;"
  },
  {
    "content": "## Table of Contents\n\n## Introduction\n\nIn March of this year (2023), a lab at Stanford released a small project that quickly became massively influential — [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html). The authors used `text-davinci-003` (an InstructGPT model from OpenAI) to generate a dataset with 52K examples of prompts and responses, then fine-tuned Llama-7B using those prompt and response pairs.\n\nThe result was surprisingly good — Alpaca was able to interact with users similarly to OpenAI's InstructGPT models, despite being inexpensive to train and not using a human-created training dataset. In this blog post, we'll write code to train our own model from scratch using the Alpaca dataset.\n\n_The code in this blog post is based on that in the [Alpaca repo](https://github.com/tatsu-lab/stanford_alpaca), though my hope is that it should be simpler and more intuitive. All credit should go to the original authors of the paper._\n\n## Setup\n\nYou'll need to install `torch`, `transformers`, `datasets`, and `accelerate`. `wandb` is great if you want to track training loss over time. And, of course, you'll need some good GPUs if you want your model to train quickly.\n\nStart out by creating one main folder, `alpaca-repro`, with two subfolders: one called `trainer`, where your training code will go, and one called `finetunes`, where we'll save your fine-tuned model.\n\n## Step 1: Loading and Processing the Data\n\nPut all of the code in this section into `trainer/get_data.py`.\n\nWe'll begin by loading the [Alpaca data](https://huggingface.co/datasets/tatsu-lab/alpaca) from the Hugging Face hub. Each question/prompt pair in the dataset needs to be converted into a single string that we can train the model on, but we actually generate one extra string: `source`, which we use further down to ignore labels so our model doesn't train on instructions.\n\n```python\nfrom datasets import load_dataset\n\noriginal_dataset = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n\ntemplate_no_context = \"\"\"Below is an instruction that describes a task. \\\nWrite a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n\"\"\"\n\ntemplate_context = \"\"\"Below is an instruction that describes a task. \\\nWrite a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n\"\"\"\n\ndef data_to_string(data):\n\n    instruction = data[\"instruction\"]\n    context = data[\"input\"]\n    response = data[\"output\"]\n\n    template = template_context if len(context) > 0 else template_no_context\n    source = template.format(instruction=instruction, input=context)\n\n    return {\n        \"source\": source,\n        \"text\": source + response,\n    }\n\n\ndataset = original_dataset.map(\n    data_to_string\n).remove_columns(['instruction', 'input', 'output'])\n```\n\nHere we split the data so we can use 10% for evaluation and tests later on.\n\n```python\nprocessed_dataset = dataset.train_test_split(test_size=0.1)\n\ntrain_dataset = processed_dataset[\"train\"]\neval_dataset = processed_dataset[\"test\"]\n```\n\nFinally, we define a data collator to be used by our training loop. Remember that each `text` string is just made up of the `source` plus the response. So we tokenize the `source` string to figure out how many labels in the `text` string to ignore.\n\n```python\nIGNORE_TOKEN = -100\n\ndef data_collator(features, tokenizer):\n    sources = [feature[\"source\"] for feature in features]\n    targets = [feature[\"text\"] for feature in features]\n\n    source_tokens = tokenizer(\n        sources,\n        return_tensors=\"pt\",\n        padding='longest',\n        max_length=None,\n    )\n\n    target_tokens = tokenizer(\n        targets,\n        return_tensors=\"pt\",\n        padding='longest',\n        max_length=None,\n    )\n\n    labels = target_tokens[\"input_ids\"].clone()\n\n    for i in range(len(labels)):\n        source_len = source_tokens[\"attention_mask\"][i].sum()\n\n        labels[i, :source_len] = IGNORE_TOKEN\n\n    res = {\n        \"input_ids\": target_tokens[\"input_ids\"],\n        \"attention_mask\": target_tokens[\"attention_mask\"],\n        \"labels\": labels,\n    }\n\n    return res\n```\n\n## Step 2: Writing our Training Loop\n\nPut all of the code in this section into `trainer/loop.py`.\n\nThis code is fairly self-explanatory, so I've just annotated it with comments.\n\n```python\nfrom transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\nfrom accelerate import Accelerator\nfrom get_data import train_dataset, eval_dataset, data_collator\n\naccelerator = Accelerator()\n\nMODEL_PATH = \"meta-llama/Llama-2-7b-hf\" # path to Llama on Hugging Face Hub\nOUTPUT_DIR = \"../finetunes/alpaca-7b\" # where to save the fine-tuned model\n\ntokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH, legacy=False)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # not set by default, strangely\n\nmodel = LlamaForCausalLM.from_pretrained(\n    MODEL_PATH, device_map=\"auto\"\n)\n\ntraining_args = TrainingArguments(\n    output_dir='checkpoints', # where Trainer will save model checkpoints\n    num_train_epochs=1, # start with a low number of epochs for testing\n    learning_rate=2e-5,\n    logging_steps=10,\n    per_device_train_batch_size=8,\n    remove_unused_columns=False,\n    save_steps=1000,\n    save_total_limit=1,\n    report_to=\"wandb\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=lambda x: data_collator(x, tokenizer),\n)\n\ntrainer.train()\ntrainer.evaluate()\n\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n```\n\n## Step 3: Running our Training Loop\n\nCreate `trainer/accelerate_config.yaml`, and paste in the following configuration:\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: \"NO\"\ndowncast_bf16: \"no\"\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: \"no\"\nnum_machines: 1\nnum_processes: 1\nuse_cpu: false\n```\n\nThen `cd` into `./trainer` and run:\n\n```bash\naccelerate launch --config_file accelerate_config.yaml loop.py\n```\n\nSaving the model and weights might take a while, so be patient!\n\n## Step 4: Testing our Fine-Tuned Model!\n\nI wrote a simple script to load up our fine-tuned model and interact with it! It doesn't support conversations with context, but it's a great way to see how the model is working.\n\nCreate a new file called `alpaca-repro/model_test.py`, then run `python3 model_test.py`.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\ntemplate = \"\"\"Below is an instruction that describes a task. \\\nWrite a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n\"\"\"\n\nmodel_path = \"./finetunes/alpaca-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, legacy=False)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, device_map=\"auto\", local_files_only=True\n)\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=False,\n    do_sample=True,\n    temperature=0.9,\n    max_new_tokens=200,\n)\n\ndef prompt_model():\n    prompt = input(\"Enter your question: \")\n    prompt = template.format(instruction=prompt)\n    answer = pipe(prompt)\n    print(answer[0][\"generated_text\"])\n\nwhile True:\n    prompt_model()\n```\n\n## Conclusion\n\nI hope this article was helpful and informative! My plan is to follow it up in a few days with an explanation of how to use FSDP with the Hugging Face Trainer.\n\nIf you got mixed up along the way, here's a Gist with the final project code: https://gist.github.com/nebrelbug/1da2c0064d53decf197a304267799708\n\n_If you liked this article, don't forget to share it and follow me at [@nebrelbug](https://twitter.com/nebrelbug) on Twitter._",
    "title": "Rebuilding Alpaca with the Hugging Face Trainer Class",
    "_meta": {
      "filePath": "rebuilding-alpaca-huggingface-trainer.mdx",
      "fileName": "rebuilding-alpaca-huggingface-trainer.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "rebuilding-alpaca-huggingface-trainer"
    },
    "mdx": "var Component=(()=>{var h=Object.create;var o=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var _=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var a in e)o(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of m(e))!g.call(t,r)&&r!==a&&o(t,r,{get:()=>e[r],enumerable:!(i=u(e,r))||i.enumerable});return t};var b=(t,e,a)=>(a=t!=null?h(_(t)):{},s(e||!t||!t.__esModule?o(a,\"default\",{value:t,enumerable:!0}):a,t)),y=t=>s(o({},\"__esModule\",{value:!0}),t);var d=f((x,l)=>{l.exports=_jsx_runtime});var k={};w(k,{default:()=>p});var n=b(d());function c(t){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",p:\"p\",pre:\"pre\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Table of Contents\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Introduction\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"In March of this year (2023), a lab at Stanford released a small project that quickly became massively influential \\u2014 \",(0,n.jsx)(e.a,{href:\"https://crfm.stanford.edu/2023/03/13/alpaca.html\",children:\"Alpaca\"}),\". The authors used \",(0,n.jsx)(e.code,{children:\"text-davinci-003\"}),\" (an InstructGPT model from OpenAI) to generate a dataset with 52K examples of prompts and responses, then fine-tuned Llama-7B using those prompt and response pairs.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The result was surprisingly good \\u2014 Alpaca was able to interact with users similarly to OpenAI's InstructGPT models, despite being inexpensive to train and not using a human-created training dataset. In this blog post, we'll write code to train our own model from scratch using the Alpaca dataset.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"The code in this blog post is based on that in the \",(0,n.jsx)(e.a,{href:\"https://github.com/tatsu-lab/stanford_alpaca\",children:\"Alpaca repo\"}),\", though my hope is that it should be simpler and more intuitive. All credit should go to the original authors of the paper.\"]})}),`\n`,(0,n.jsx)(e.h2,{children:\"Setup\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"You'll need to install \",(0,n.jsx)(e.code,{children:\"torch\"}),\", \",(0,n.jsx)(e.code,{children:\"transformers\"}),\", \",(0,n.jsx)(e.code,{children:\"datasets\"}),\", and \",(0,n.jsx)(e.code,{children:\"accelerate\"}),\". \",(0,n.jsx)(e.code,{children:\"wandb\"}),\" is great if you want to track training loss over time. And, of course, you'll need some good GPUs if you want your model to train quickly.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Start out by creating one main folder, \",(0,n.jsx)(e.code,{children:\"alpaca-repro\"}),\", with two subfolders: one called \",(0,n.jsx)(e.code,{children:\"trainer\"}),\", where your training code will go, and one called \",(0,n.jsx)(e.code,{children:\"finetunes\"}),\", where we'll save your fine-tuned model.\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 1: Loading and Processing the Data\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Put all of the code in this section into \",(0,n.jsx)(e.code,{children:\"trainer/get_data.py\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We'll begin by loading the \",(0,n.jsx)(e.a,{href:\"https://huggingface.co/datasets/tatsu-lab/alpaca\",children:\"Alpaca data\"}),\" from the Hugging Face hub. Each question/prompt pair in the dataset needs to be converted into a single string that we can train the model on, but we actually generate one extra string: \",(0,n.jsx)(e.code,{children:\"source\"}),\", which we use further down to ignore labels so our model doesn't train on instructions.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-python\",children:`from datasets import load_dataset\n\noriginal_dataset = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n\ntemplate_no_context = \"\"\"Below is an instruction that describes a task. \\\\\nWrite a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n\"\"\"\n\ntemplate_context = \"\"\"Below is an instruction that describes a task. \\\\\nWrite a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n\"\"\"\n\ndef data_to_string(data):\n\n    instruction = data[\"instruction\"]\n    context = data[\"input\"]\n    response = data[\"output\"]\n\n    template = template_context if len(context) > 0 else template_no_context\n    source = template.format(instruction=instruction, input=context)\n\n    return {\n        \"source\": source,\n        \"text\": source + response,\n    }\n\n\ndataset = original_dataset.map(\n    data_to_string\n).remove_columns(['instruction', 'input', 'output'])\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Here we split the data so we can use 10% for evaluation and tests later on.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-python\",children:`processed_dataset = dataset.train_test_split(test_size=0.1)\n\ntrain_dataset = processed_dataset[\"train\"]\neval_dataset = processed_dataset[\"test\"]\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Finally, we define a data collator to be used by our training loop. Remember that each \",(0,n.jsx)(e.code,{children:\"text\"}),\" string is just made up of the \",(0,n.jsx)(e.code,{children:\"source\"}),\" plus the response. So we tokenize the \",(0,n.jsx)(e.code,{children:\"source\"}),\" string to figure out how many labels in the \",(0,n.jsx)(e.code,{children:\"text\"}),\" string to ignore.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-python\",children:`IGNORE_TOKEN = -100\n\ndef data_collator(features, tokenizer):\n    sources = [feature[\"source\"] for feature in features]\n    targets = [feature[\"text\"] for feature in features]\n\n    source_tokens = tokenizer(\n        sources,\n        return_tensors=\"pt\",\n        padding='longest',\n        max_length=None,\n    )\n\n    target_tokens = tokenizer(\n        targets,\n        return_tensors=\"pt\",\n        padding='longest',\n        max_length=None,\n    )\n\n    labels = target_tokens[\"input_ids\"].clone()\n\n    for i in range(len(labels)):\n        source_len = source_tokens[\"attention_mask\"][i].sum()\n\n        labels[i, :source_len] = IGNORE_TOKEN\n\n    res = {\n        \"input_ids\": target_tokens[\"input_ids\"],\n        \"attention_mask\": target_tokens[\"attention_mask\"],\n        \"labels\": labels,\n    }\n\n    return res\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 2: Writing our Training Loop\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Put all of the code in this section into \",(0,n.jsx)(e.code,{children:\"trainer/loop.py\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This code is fairly self-explanatory, so I've just annotated it with comments.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-python\",children:`from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\nfrom accelerate import Accelerator\nfrom get_data import train_dataset, eval_dataset, data_collator\n\naccelerator = Accelerator()\n\nMODEL_PATH = \"meta-llama/Llama-2-7b-hf\" # path to Llama on Hugging Face Hub\nOUTPUT_DIR = \"../finetunes/alpaca-7b\" # where to save the fine-tuned model\n\ntokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH, legacy=False)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # not set by default, strangely\n\nmodel = LlamaForCausalLM.from_pretrained(\n    MODEL_PATH, device_map=\"auto\"\n)\n\ntraining_args = TrainingArguments(\n    output_dir='checkpoints', # where Trainer will save model checkpoints\n    num_train_epochs=1, # start with a low number of epochs for testing\n    learning_rate=2e-5,\n    logging_steps=10,\n    per_device_train_batch_size=8,\n    remove_unused_columns=False,\n    save_steps=1000,\n    save_total_limit=1,\n    report_to=\"wandb\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=lambda x: data_collator(x, tokenizer),\n)\n\ntrainer.train()\ntrainer.evaluate()\n\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 3: Running our Training Loop\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Create \",(0,n.jsx)(e.code,{children:\"trainer/accelerate_config.yaml\"}),\", and paste in the following configuration:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`compute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: \"NO\"\ndowncast_bf16: \"no\"\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: \"no\"\nnum_machines: 1\nnum_processes: 1\nuse_cpu: false\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Then \",(0,n.jsx)(e.code,{children:\"cd\"}),\" into \",(0,n.jsx)(e.code,{children:\"./trainer\"}),\" and run:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`accelerate launch --config_file accelerate_config.yaml loop.py\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Saving the model and weights might take a while, so be patient!\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 4: Testing our Fine-Tuned Model!\"}),`\n`,(0,n.jsx)(e.p,{children:\"I wrote a simple script to load up our fine-tuned model and interact with it! It doesn't support conversations with context, but it's a great way to see how the model is working.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Create a new file called \",(0,n.jsx)(e.code,{children:\"alpaca-repro/model_test.py\"}),\", then run \",(0,n.jsx)(e.code,{children:\"python3 model_test.py\"}),\".\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-python\",children:`from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\ntemplate = \"\"\"Below is an instruction that describes a task. \\\\\nWrite a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n\"\"\"\n\nmodel_path = \"./finetunes/alpaca-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, legacy=False)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, device_map=\"auto\", local_files_only=True\n)\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=False,\n    do_sample=True,\n    temperature=0.9,\n    max_new_tokens=200,\n)\n\ndef prompt_model():\n    prompt = input(\"Enter your question: \")\n    prompt = template.format(instruction=prompt)\n    answer = pipe(prompt)\n    print(answer[0][\"generated_text\"])\n\nwhile True:\n    prompt_model()\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Conclusion\"}),`\n`,(0,n.jsx)(e.p,{children:\"I hope this article was helpful and informative! My plan is to follow it up in a few days with an explanation of how to use FSDP with the Hugging Face Trainer.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you got mixed up along the way, here's a Gist with the final project code: https://gist.github.com/nebrelbug/1da2c0064d53decf197a304267799708\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked this article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function p(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(c,{...t})}):c(t)}return y(k);})();\n;return Component;"
  },
  {
    "content": "This post is mainly meant for coworkers, but it might be useful for others as well. I'll be sharing the Dockerfile that I use to set up my machine learning environment. It's based on the `huggingface/transformers-pytorch-gpu` image, but I've added a few things (Infiniband support, upgraded Pip packages, a utility to show GPU status, etc.) to make it a bit more useful for me.\n\n## Setup\n\nPlace all of the below files in a new directory. Feel free to add or remove anything you want — I'll likely update this post as I make changes to my setup.\n\nAfter you've created the files, you can build the image with:\n\n```bash\ncd /path/to/directory\ndocker build -t <image-name> .\n```\n\n## Files\n\n### `Dockerfile`\n\n```dockerfile\nFROM huggingface/transformers-pytorch-gpu\n\n# Install Infiniband drivers\nRUN apt-get install -y libibverbs-dev librdmacm-dev libibumad-dev ibutils\n\n#####################\n# GH CLI & STARSHIP #\n#####################\n\n# Prep apt for GitHub CLI\nRUN apt-get update && apt-get install -y --no-install-recommends apt-utils\nRUN apt-get install -y curl apt-utils\n\n# GitHub CLI\nRUN curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg \\\n    && chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg \\\n    && echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null \\\n    && apt-get update \\\n    && apt-get install gh -y\n\nRUN apt-get upgrade -y\n\n# Starship Prompt\nRUN curl -sS https://starship.rs/install.sh -o starship-install.sh\nRUN sh starship-install.sh --yes\nRUN echo 'eval \"$(starship init bash)\"' >> ~/.bashrc\n\n# Starship Config\nCOPY starship.toml /root/.config/starship.toml\n\n#####################\n# PYTHON PACKAGES   #\n#####################\n\n# Disable the \"running pip as the 'root' user can...\" warning\nENV PIP_ROOT_USER_ACTION=ignore\n\n# Upgrade pip\nRUN pip3 install --upgrade pip\n\n# Upgrade important packages\nRUN pip3 install --upgrade torch torchvision torchaudio\nRUN pip3 install --upgrade transformers accelerate xformers deepspeed\n\n# Other useful machine learning packages\nRUN pip3 install --upgrade fire tqdm openai numpy rouge_score wandb ipython emoji tokenizers evaluate matplotlib seaborn lm-eval jupyter nltk tiktoken aiolimiter swifter pytorch-lightning lightning sentencepiece jsonargparse[signatures] bitsandbytes datasets zstandard rich\n\n#####################\n# ALIASES           #\n#####################\n\n# show-gpus, watch-gpus\nCOPY show_gpus.py /root/show_gpus.py\nRUN echo 'alias show-gpus=\"python3 /root/show_gpus.py\"' >> ~/.bashrc\nRUN echo 'alias watch-gpus=\"watch -c python3 /root/show_gpus.py\"' >> ~/.bashrc\n```\n\n### `starship.toml`\n\n```toml\n[character]\nsuccess_symbol = '[λ](bold green) '\nerror_symbol = '[λ](bold red) '\n\n[aws]\ndisabled = true\n```\n\n### `show_gpus.py`\n\n```python\nimport subprocess\nfrom tabulate import tabulate\n\n\ndef get_gpu_info():\n    gpu_info = []\n    command = \"nvidia-smi --query-gpu=index,uuid,memory.total,memory.used --format=csv,noheader\"\n    output = subprocess.check_output(command, shell=True).decode().strip()\n    lines = output.split(\"\\n\")\n    for line in lines:\n        values = line.split(\", \")\n        index = int(values[0])\n        uuid = values[1]\n        total_memory = values[2]\n        used_memory = values[3]\n        gpu_info.append(\n            {\n                \"index\": index,\n                \"uuid\": uuid,\n                \"total_memory\": total_memory,\n                \"used_memory\": used_memory,\n            }\n        )\n\n    return gpu_info\n\nclass bcolors:\n    GREEN = \"\\033[32m\"\n    ORANGE = \"\\033[33m\"\n    RED = \"\\033[31m\"\n    ENDC = \"\\033[0m\"\n\n\ndef colorize(num):\n    res = \"{0:.0%}\".format(num)\n    if num == 0:\n        return res\n    elif num < 0.25:\n        return bcolors.GREEN + res + bcolors.ENDC\n    elif num < 0.5:\n        return bcolors.ORANGE + res + bcolors.ENDC\n    else:\n        return bcolors.RED + res + bcolors.ENDC\n\n\ndef truncate_string(string, length):\n    if len(string) <= length:\n        return string\n    else:\n        truncated = string[: length - 3] + \"...\"\n        return truncated\n\n\ndef main():\n    gpu_info = get_gpu_info()\n\n    headers = [\"GPUs\"]\n\n    max_gpu_index_len = len(str(max([gpu[\"index\"] for gpu in gpu_info])))\n\n    table = [\n        [\n            \"gpu \"\n            + str(gpu[\"index\"])\n            + \" \" * (max_gpu_index_len - len(str(gpu[\"index\"])))\n            + \" - \"\n            + colorize(\n                float(gpu[\"used_memory\"].split()[0])\n                / float(gpu[\"total_memory\"].split()[0])\n            )\n        ]\n        for gpu in gpu_info\n    ]\n\n    print(\n        tabulate(\n            table,\n            headers,\n            tablefmt=\"mixed_outline\",\n            stralign=\"center\",\n            colalign=(\"left\",),\n        )\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n_If you liked the article, don't forget to share it and follow me at [@nebrelbug](https://twitter.com/nebrelbug) on Twitter._",
    "title": "Setting Up Docker for Machine Learning",
    "_meta": {
      "filePath": "ultimate-ml-dockerfile.mdx",
      "fileName": "ultimate-ml-dockerfile.mdx",
      "directory": ".",
      "extension": "mdx",
      "path": "ultimate-ml-dockerfile"
    },
    "mdx": "var Component=(()=>{var h=Object.create;var s=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,b=Object.prototype.hasOwnProperty;var f=(t,n)=>()=>(n||t((n={exports:{}}).exports,n),n.exports),y=(t,n)=>{for(var r in n)s(t,r,{get:n[r],enumerable:!0})},o=(t,n,r,a)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let i of g(n))!b.call(t,i)&&i!==r&&s(t,i,{get:()=>n[i],enumerable:!(a=d(n,i))||a.enumerable});return t};var _=(t,n,r)=>(r=t!=null?h(m(t)):{},o(n||!t||!t.__esModule?s(r,\"default\",{value:t,enumerable:!0}):r,t)),N=t=>o(s({},\"__esModule\",{value:!0}),t);var u=f((v,l)=>{l.exports=_jsx_runtime});var k={};y(k,{default:()=>c});var e=_(u());function p(t){let n={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",h3:\"h3\",p:\"p\",pre:\"pre\",...t.components};return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsxs)(n.p,{children:[\"This post is mainly meant for coworkers, but it might be useful for others as well. I'll be sharing the Dockerfile that I use to set up my machine learning environment. It's based on the \",(0,e.jsx)(n.code,{children:\"huggingface/transformers-pytorch-gpu\"}),\" image, but I've added a few things (Infiniband support, upgraded Pip packages, a utility to show GPU status, etc.) to make it a bit more useful for me.\"]}),`\n`,(0,e.jsx)(n.h2,{children:\"Setup\"}),`\n`,(0,e.jsx)(n.p,{children:\"Place all of the below files in a new directory. Feel free to add or remove anything you want \\u2014 I'll likely update this post as I make changes to my setup.\"}),`\n`,(0,e.jsx)(n.p,{children:\"After you've created the files, you can build the image with:\"}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-bash\",children:`cd /path/to/directory\ndocker build -t <image-name> .\n`})}),`\n`,(0,e.jsx)(n.h2,{children:\"Files\"}),`\n`,(0,e.jsx)(n.h3,{children:(0,e.jsx)(n.code,{children:\"Dockerfile\"})}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-dockerfile\",children:`FROM huggingface/transformers-pytorch-gpu\n\n# Install Infiniband drivers\nRUN apt-get install -y libibverbs-dev librdmacm-dev libibumad-dev ibutils\n\n#####################\n# GH CLI & STARSHIP #\n#####################\n\n# Prep apt for GitHub CLI\nRUN apt-get update && apt-get install -y --no-install-recommends apt-utils\nRUN apt-get install -y curl apt-utils\n\n# GitHub CLI\nRUN curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg \\\\\n    && chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg \\\\\n    && echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null \\\\\n    && apt-get update \\\\\n    && apt-get install gh -y\n\nRUN apt-get upgrade -y\n\n# Starship Prompt\nRUN curl -sS https://starship.rs/install.sh -o starship-install.sh\nRUN sh starship-install.sh --yes\nRUN echo 'eval \"$(starship init bash)\"' >> ~/.bashrc\n\n# Starship Config\nCOPY starship.toml /root/.config/starship.toml\n\n#####################\n# PYTHON PACKAGES   #\n#####################\n\n# Disable the \"running pip as the 'root' user can...\" warning\nENV PIP_ROOT_USER_ACTION=ignore\n\n# Upgrade pip\nRUN pip3 install --upgrade pip\n\n# Upgrade important packages\nRUN pip3 install --upgrade torch torchvision torchaudio\nRUN pip3 install --upgrade transformers accelerate xformers deepspeed\n\n# Other useful machine learning packages\nRUN pip3 install --upgrade fire tqdm openai numpy rouge_score wandb ipython emoji tokenizers evaluate matplotlib seaborn lm-eval jupyter nltk tiktoken aiolimiter swifter pytorch-lightning lightning sentencepiece jsonargparse[signatures] bitsandbytes datasets zstandard rich\n\n#####################\n# ALIASES           #\n#####################\n\n# show-gpus, watch-gpus\nCOPY show_gpus.py /root/show_gpus.py\nRUN echo 'alias show-gpus=\"python3 /root/show_gpus.py\"' >> ~/.bashrc\nRUN echo 'alias watch-gpus=\"watch -c python3 /root/show_gpus.py\"' >> ~/.bashrc\n`})}),`\n`,(0,e.jsx)(n.h3,{children:(0,e.jsx)(n.code,{children:\"starship.toml\"})}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-toml\",children:`[character]\nsuccess_symbol = '[\\u03BB](bold green) '\nerror_symbol = '[\\u03BB](bold red) '\n\n[aws]\ndisabled = true\n`})}),`\n`,(0,e.jsx)(n.h3,{children:(0,e.jsx)(n.code,{children:\"show_gpus.py\"})}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-python\",children:`import subprocess\nfrom tabulate import tabulate\n\n\ndef get_gpu_info():\n    gpu_info = []\n    command = \"nvidia-smi --query-gpu=index,uuid,memory.total,memory.used --format=csv,noheader\"\n    output = subprocess.check_output(command, shell=True).decode().strip()\n    lines = output.split(\"\\\\n\")\n    for line in lines:\n        values = line.split(\", \")\n        index = int(values[0])\n        uuid = values[1]\n        total_memory = values[2]\n        used_memory = values[3]\n        gpu_info.append(\n            {\n                \"index\": index,\n                \"uuid\": uuid,\n                \"total_memory\": total_memory,\n                \"used_memory\": used_memory,\n            }\n        )\n\n    return gpu_info\n\nclass bcolors:\n    GREEN = \"\\\\033[32m\"\n    ORANGE = \"\\\\033[33m\"\n    RED = \"\\\\033[31m\"\n    ENDC = \"\\\\033[0m\"\n\n\ndef colorize(num):\n    res = \"{0:.0%}\".format(num)\n    if num == 0:\n        return res\n    elif num < 0.25:\n        return bcolors.GREEN + res + bcolors.ENDC\n    elif num < 0.5:\n        return bcolors.ORANGE + res + bcolors.ENDC\n    else:\n        return bcolors.RED + res + bcolors.ENDC\n\n\ndef truncate_string(string, length):\n    if len(string) <= length:\n        return string\n    else:\n        truncated = string[: length - 3] + \"...\"\n        return truncated\n\n\ndef main():\n    gpu_info = get_gpu_info()\n\n    headers = [\"GPUs\"]\n\n    max_gpu_index_len = len(str(max([gpu[\"index\"] for gpu in gpu_info])))\n\n    table = [\n        [\n            \"gpu \"\n            + str(gpu[\"index\"])\n            + \" \" * (max_gpu_index_len - len(str(gpu[\"index\"])))\n            + \" - \"\n            + colorize(\n                float(gpu[\"used_memory\"].split()[0])\n                / float(gpu[\"total_memory\"].split()[0])\n            )\n        ]\n        for gpu in gpu_info\n    ]\n\n    print(\n        tabulate(\n            table,\n            headers,\n            tablefmt=\"mixed_outline\",\n            stralign=\"center\",\n            colalign=(\"left\",),\n        )\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n`})}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsxs)(n.em,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,e.jsx)(n.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function c(t={}){let{wrapper:n}=t.components||{};return n?(0,e.jsx)(n,{...t,children:(0,e.jsx)(p,{...t})}):p(t)}return N(k);})();\n;return Component;"
  }
]