"var Component=(()=>{var p=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var r in e)i(t,r,{get:e[r],enumerable:!0})},l=(t,e,r,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!g.call(t,a)&&a!==r&&i(t,a,{get:()=>e[a],enumerable:!(o=u(e,a))||o.enumerable});return t};var b=(t,e,r)=>(r=t!=null?p(f(t)):{},l(e||!t||!t.__esModule?i(r,\"default\",{value:t,enumerable:!0}):r,t)),P=t=>l(i({},\"__esModule\",{value:!0}),t);var d=y((I,s)=>{s.exports=_jsx_runtime});var D={};w(D,{default:()=>h});var n=b(d());function c(t){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",li:\"li\",p:\"p\",strong:\"strong\",ul:\"ul\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Introduction\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"There are many different libraries and strategies for distributed training. In this article, we'll look at three of the most popular: \",(0,n.jsx)(e.a,{href:\"https://huggingface.co/docs/accelerate/index\",children:\"Accelerate\"}),\", \",(0,n.jsx)(e.a,{href:\"https://www.deepspeed.ai/\",children:\"DeepSpeed\"}),\", and \",(0,n.jsx)(e.a,{href:\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\",children:\"FSDP\"}),\". We'll discuss the differences between them, and when you might want to use one over the other.\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Accelerate\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://huggingface.co/docs/accelerate/index\",children:\"Accelerate\"}),\" is a popular library developed and maintained by HuggingFace. You can think of it as a wrapper around \",(0,n.jsx)(e.code,{children:\"torch.distributed\"}),\". Essentially, it allows you to simply run training or \",(0,n.jsx)(e.a,{href:\"./multi-gpu-inference-with-accelerate\",children:\"inference\"}),\" across multiple GPUs or nodes.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In its most basic form, you use Accelerate to initialize a PyTorch model on each GPU. By simply making a few modifications to your training loop, Accelerate will handle data parallelism for you.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"If your model is too large to fit on any one GPU, you can use Accelerate to split the model across multiple GPUs by passing \",(0,n.jsx)(e.code,{children:'device_map=\"auto\"'}),\" into the transformers \",(0,n.jsx)(e.code,{children:\"from_pretrained\"}),\" method. Be warned \\u2014 you can only use \",(0,n.jsx)(e.code,{children:'device_map=\"auto\"'}),\" if you're running with \",(0,n.jsx)(e.code,{children:\"num_processes=1\"}),\", because you're only initializing one model.\"]}),`\n`,(0,n.jsx)(e.p,{children:'If you need more sophisticated model sharding (\"sharding\" refers to splitting a model across devices) you can use DeepSpeed or FSDP alongside Accelerate'}),`\n`,(0,n.jsx)(e.h2,{children:\"DeepSpeed\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://www.deepspeed.ai/\",children:\"DeepSpeed\"}),` offers the Zero Redundancy Optimizer (ZeRO). It's called \"Zero Redundancy\" because it allows you to partition a model across multiple GPUs without having to replicate the model's parameters across each GPU. This is a huge benefit, because it allows you to train models that are larger than the memory of any one GPU.`]}),`\n`,(0,n.jsx)(e.p,{children:\"There are three stages of ZeRO:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"ZeRO Stage 1\"}),\" partitions optimizer states\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"ZeRO Stage 2\"}),\" also partitions gradients\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"ZeRO Stage 3\"}),\" also partitions parameters\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[`If you're still running into memory issues, DeepSpeed allows you to offload the optimizer state, gradients, and some model weights to CPU memory or NVMe storage. This is called \"`,(0,n.jsx)(e.strong,{children:\"ZeRO-Infinity\"}),',\" and \\u2014 though significantly slower than training without offload \\u2014 allows for training truly huge models.']}),`\n`,(0,n.jsx)(e.h2,{children:\"FSDP\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\",children:\"FSDP\"}),' stands for \"Fully Sharded Data Parallel.\" It was originally developed by Facebook AI Research and released in the Fairscale library, but upstream support was ',(0,n.jsx)(e.a,{href:\"https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/\",children:\"added natively to PyTorch\"}),\" in PyTorch version 1.11.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"It does essentially the same thing as DeepSpeed ZeRO \\u2014 manage sharding of optimizer states, gradients, and model parameters. It also supports CPU offload. One helpful feature is that it can serve as a drop-in replacement for DistributedDataParallel.\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Summary\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Accelerate is a wrapper around \",(0,n.jsx)(e.code,{children:\"torch.distributed\"}),\" that allows you to easily run training or inference across multiple GPUs or nodes. It can also be used for simple model partitioning, and works well with both DeepSpeed and FSDP for more advanced use cases.\"]}),`\n`,(0,n.jsx)(e.li,{children:\"DeepSpeed and FSDP are two different implementations of the same idea: sharding model parameters, gradients, and optimizer states across multiple GPUs. They both support CPU offload and can be used in conjunction with Accelerate.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked this article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function h(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(c,{...t})}):c(t)}return P(D);})();\n;return Component;"