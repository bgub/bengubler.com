"var Component=(()=>{var p=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,_=Object.prototype.hasOwnProperty;var f=(r,e)=>()=>(e||r((e={exports:{}}).exports,e),e.exports),b=(r,e)=>{for(var t in e)a(r,t,{get:e[t],enumerable:!0})},s=(r,e,t,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!_.call(r,i)&&i!==t&&a(r,i,{get:()=>e[i],enumerable:!(o=u(e,i))||o.enumerable});return r};var w=(r,e,t)=>(t=r!=null?p(g(r)):{},s(e||!r||!r.__esModule?a(t,\"default\",{value:r,enumerable:!0}):t,r)),R=r=>s(a({},\"__esModule\",{value:!0}),r);var l=f((T,c)=>{c.exports=_jsx_runtime});var S={};b(S,{default:()=>d});var n=w(l());function h(r){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",p:\"p\",pre:\"pre\",...r.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"This is part 2 of a 2-part series. \",(0,n.jsx)(e.a,{href:\"./enroot-on-slurm-for-distributed-ml-part-1\",children:\"Part 1\"}),\" is available here.\"]})}),`\n`,(0,n.jsxs)(e.p,{children:[\"In \",(0,n.jsx)(e.a,{href:\"./enroot-on-slurm-for-distributed-ml-part-1\",children:\"part 1\"}),\", we covered how to use Enroot on Slurm for containerized \",(0,n.jsx)(e.em,{children:\"single-node\"}),\" training using \",(0,n.jsx)(e.code,{children:\"salloc\"}),\". In this post, we'll cover how to use Enroot on Slurm for containerized \",(0,n.jsx)(e.em,{children:\"multi-node\"}),\" training, and transition to using \",(0,n.jsx)(e.code,{children:\"sbatch\"}),\".\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 1: Slurm Launch Script\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"We'll end up creating several Bash files, all of which should be in the same directory as your training script. The first will be a Slurm launch file that we'll run with \",(0,n.jsx)(e.code,{children:\"sbatch\"}),\". This file will contain the same commands we ran with \",(0,n.jsx)(e.code,{children:\"salloc\"}),\" in \",(0,n.jsx)(e.a,{href:\"../enroot-on-slurm-for-distributed-ml-part-1\",children:\"part 1\"}),\", but declared using \",(0,n.jsx)(e.code,{children:\"#SBATCH\"}),\" processing directives.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"launch.sh\"})}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`#!/bin/bash\n#SBATCH -J \"JOBNAME\"\n#SBATCH --nodes=2\n#SBATCH --gpus-per-node=8\n#SBATCH --cpus-per-task=128\n#SBATCH --mem=2000G\n#SBATCH --time=72:00:00\n#SBATCH --qos=<qos>\n\nexport CUR_DIR=$(pwd)\nsrun --nodes=2 stage1.sh\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Note that we create a variable \",(0,n.jsx)(e.code,{children:\"CUR_DIR\"}),\" to store the current working directory (the directory where the \",(0,n.jsx)(e.code,{children:\"sbatch\"}),\" command was run). I use this variable to share the location of my training directory between scripts, so I don't have to hard-code paths. But it's not required.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Slurm will automatically pass local environment variables through to the \",(0,n.jsx)(e.code,{children:\"srun\"}),\" command, which will run the \",(0,n.jsx)(e.code,{children:\"stage1.sh\"}),\" script on each node.\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 2. Enroot Launch Script\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Next, we'll create a script that will be run on each node. This script will be responsible for launching the container and running the training script. We'll call this script \",(0,n.jsx)(e.code,{children:\"stage1.sh\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"stage1.sh\"})}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`#!/bin/bash\n\nmodule load jq zstd pigz parallel libnvidia-container enroot\n\nexport MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1) # get the IP address of the first node in the list\nexport MASTER_PORT=6000 # set the port to use for communication between nodes\n\nenroot create --name image-name /path/to/image-name.sqsh\n\nenroot start --env SLURM_NODEID \\\\\n             --env MASTER_ADDR \\\\\n             --env MASTER_PORT \\\\\n             --env SLURM_JOB_NAME \\\\\n             --env CUR_DIR \\\\\n             --mount /local/file/path:/image/file/path \\\\\n             --rw image-name \\\\\n             bash \\${CUR_DIR}/stage2.sh\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Note that we pass several important environment variables provided by Slurm, along with \",(0,n.jsx)(e.code,{children:\"CUR_DIR\"}),\", into the container. The \",(0,n.jsx)(e.code,{children:\"MASTER_ADDR\"}),\" and \",(0,n.jsx)(e.code,{children:\"MASTER_PORT\"}),\" variables are used by PyTorch's distributed training backend to coordinate communication between nodes.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We also mount a local file path into the container (make sure it contains your training script!).\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 3. Training Script\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Finally, we'll create a training script that will be run inside the container. We'll call this script \",(0,n.jsx)(e.code,{children:\"stage2.sh\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"stage2.sh\"})}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`#!/bin/bash\n\nexport NCCL_DEBUG=INFO # if you want to see NCCL logs\nexport NODE_RANK=$SLURM_NODEID # set the node rank to the node ID (0, 1, 2, etc.)\necho NODE_RANK: $NODE_RANK # print the node rank for debugging purposes\n\n# Run training script\n# NOTE: modify as desired if you're not using accelerate\n\naccelerate launch --config_file ./accelerate_config.yaml --main_process_ip=$MASTER_ADDR --main_process_port=$MASTER_PORT --machine_rank $NODE_RANK \\${CUR_DIR}/loop.py\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Here I've used \",(0,n.jsx)(e.a,{href:\"https://huggingface.co/docs/accelerate\",children:\"accelerate\"}),\" as a launcher for my distributed training script, but you can use whatever launcher you want. Just make sure you pass relevant environment variables through!\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"For the sake of completeness, here's my \",(0,n.jsx)(e.code,{children:\"accelerate_config.yaml\"}),\" file. It utilizes FSDP (Fully Sharded Data Parallel) to split model parameters and gradients across processes. This is a great way to train large models that won't fit on just one GPU.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`compute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: FSDP\ndowncast_bf16: \"no\"\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: 1\n  fsdp_state_dict_type: FULL_STATE_DICT\n  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer\nmain_training_function: main\nmixed_precision: \"no\"\nnum_machines: 2\nnum_processes: 16 # 8 GPUs per node * 2 nodes = 16 processes\nuse_cpu: false\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 4. Submit the Job\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now that we've created all the necessary scripts, we can submit the job to Slurm using \",(0,n.jsx)(e.code,{children:\"sbatch\"}),\"! From the directory containing the scripts, run:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`sbatch launch.sh\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Your job will be submitted to Slurm and run as soon as resources are available. Output logs will be stored at \",(0,n.jsx)(e.code,{children:\"slurm-<jobid>.out\"}),\" in the current directory.\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Conclusion\"}),`\n`,(0,n.jsx)(e.p,{children:\"I hope this was helpful! There are many parts involved in getting distributed training working, but it's not too difficult once you get over the initial learning curve.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function d(r={}){let{wrapper:e}=r.components||{};return e?(0,n.jsx)(e,{...r,children:(0,n.jsx)(h,{...r})}):h(r)}return R(S);})();\n;return Component;"