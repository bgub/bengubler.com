"var Component=(()=>{var h=Object.create;var s=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,b=Object.prototype.hasOwnProperty;var f=(t,n)=>()=>(n||t((n={exports:{}}).exports,n),n.exports),y=(t,n)=>{for(var r in n)s(t,r,{get:n[r],enumerable:!0})},o=(t,n,r,a)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let i of g(n))!b.call(t,i)&&i!==r&&s(t,i,{get:()=>n[i],enumerable:!(a=d(n,i))||a.enumerable});return t};var _=(t,n,r)=>(r=t!=null?h(m(t)):{},o(n||!t||!t.__esModule?s(r,\"default\",{value:t,enumerable:!0}):r,t)),N=t=>o(s({},\"__esModule\",{value:!0}),t);var u=f((v,l)=>{l.exports=_jsx_runtime});var k={};y(k,{default:()=>c});var e=_(u());function p(t){let n={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",h3:\"h3\",p:\"p\",pre:\"pre\",...t.components};return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsxs)(n.p,{children:[\"This post is mainly meant for coworkers, but it might be useful for others as well. I'll be sharing the Dockerfile that I use to set up my machine learning environment. It's based on the \",(0,e.jsx)(n.code,{children:\"huggingface/transformers-pytorch-gpu\"}),\" image, but I've added a few things (Infiniband support, upgraded Pip packages, a utility to show GPU status, etc.) to make it a bit more useful for me.\"]}),`\n`,(0,e.jsx)(n.h2,{children:\"Setup\"}),`\n`,(0,e.jsx)(n.p,{children:\"Place all of the below files in a new directory. Feel free to add or remove anything you want \\u2014 I'll likely update this post as I make changes to my setup.\"}),`\n`,(0,e.jsx)(n.p,{children:\"After you've created the files, you can build the image with:\"}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-bash\",children:`cd /path/to/directory\ndocker build -t <image-name> .\n`})}),`\n`,(0,e.jsx)(n.h2,{children:\"Files\"}),`\n`,(0,e.jsx)(n.h3,{children:(0,e.jsx)(n.code,{children:\"Dockerfile\"})}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-dockerfile\",children:`FROM huggingface/transformers-pytorch-gpu\n\n# Install Infiniband drivers\nRUN apt-get install -y libibverbs-dev librdmacm-dev libibumad-dev ibutils\n\n#####################\n# GH CLI & STARSHIP #\n#####################\n\n# Prep apt for GitHub CLI\nRUN apt-get update && apt-get install -y --no-install-recommends apt-utils\nRUN apt-get install -y curl apt-utils\n\n# GitHub CLI\nRUN curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg \\\\\n    && chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg \\\\\n    && echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null \\\\\n    && apt-get update \\\\\n    && apt-get install gh -y\n\nRUN apt-get upgrade -y\n\n# Starship Prompt\nRUN curl -sS https://starship.rs/install.sh -o starship-install.sh\nRUN sh starship-install.sh --yes\nRUN echo 'eval \"$(starship init bash)\"' >> ~/.bashrc\n\n# Starship Config\nCOPY starship.toml /root/.config/starship.toml\n\n#####################\n# PYTHON PACKAGES   #\n#####################\n\n# Disable the \"running pip as the 'root' user can...\" warning\nENV PIP_ROOT_USER_ACTION=ignore\n\n# Upgrade pip\nRUN pip3 install --upgrade pip\n\n# Upgrade important packages\nRUN pip3 install --upgrade torch torchvision torchaudio\nRUN pip3 install --upgrade transformers accelerate xformers deepspeed\n\n# Other useful machine learning packages\nRUN pip3 install --upgrade fire tqdm openai numpy rouge_score wandb ipython emoji tokenizers evaluate matplotlib seaborn lm-eval jupyter nltk tiktoken aiolimiter swifter pytorch-lightning lightning sentencepiece jsonargparse[signatures] bitsandbytes datasets zstandard rich\n\n#####################\n# ALIASES           #\n#####################\n\n# show-gpus, watch-gpus\nCOPY show_gpus.py /root/show_gpus.py\nRUN echo 'alias show-gpus=\"python3 /root/show_gpus.py\"' >> ~/.bashrc\nRUN echo 'alias watch-gpus=\"watch -c python3 /root/show_gpus.py\"' >> ~/.bashrc\n`})}),`\n`,(0,e.jsx)(n.h3,{children:(0,e.jsx)(n.code,{children:\"starship.toml\"})}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-toml\",children:`[character]\nsuccess_symbol = '[\\u03BB](bold green) '\nerror_symbol = '[\\u03BB](bold red) '\n\n[aws]\ndisabled = true\n`})}),`\n`,(0,e.jsx)(n.h3,{children:(0,e.jsx)(n.code,{children:\"show_gpus.py\"})}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-python\",children:`import subprocess\nfrom tabulate import tabulate\n\n\ndef get_gpu_info():\n    gpu_info = []\n    command = \"nvidia-smi --query-gpu=index,uuid,memory.total,memory.used --format=csv,noheader\"\n    output = subprocess.check_output(command, shell=True).decode().strip()\n    lines = output.split(\"\\\\n\")\n    for line in lines:\n        values = line.split(\", \")\n        index = int(values[0])\n        uuid = values[1]\n        total_memory = values[2]\n        used_memory = values[3]\n        gpu_info.append(\n            {\n                \"index\": index,\n                \"uuid\": uuid,\n                \"total_memory\": total_memory,\n                \"used_memory\": used_memory,\n            }\n        )\n\n    return gpu_info\n\nclass bcolors:\n    GREEN = \"\\\\033[32m\"\n    ORANGE = \"\\\\033[33m\"\n    RED = \"\\\\033[31m\"\n    ENDC = \"\\\\033[0m\"\n\n\ndef colorize(num):\n    res = \"{0:.0%}\".format(num)\n    if num == 0:\n        return res\n    elif num < 0.25:\n        return bcolors.GREEN + res + bcolors.ENDC\n    elif num < 0.5:\n        return bcolors.ORANGE + res + bcolors.ENDC\n    else:\n        return bcolors.RED + res + bcolors.ENDC\n\n\ndef truncate_string(string, length):\n    if len(string) <= length:\n        return string\n    else:\n        truncated = string[: length - 3] + \"...\"\n        return truncated\n\n\ndef main():\n    gpu_info = get_gpu_info()\n\n    headers = [\"GPUs\"]\n\n    max_gpu_index_len = len(str(max([gpu[\"index\"] for gpu in gpu_info])))\n\n    table = [\n        [\n            \"gpu \"\n            + str(gpu[\"index\"])\n            + \" \" * (max_gpu_index_len - len(str(gpu[\"index\"])))\n            + \" - \"\n            + colorize(\n                float(gpu[\"used_memory\"].split()[0])\n                / float(gpu[\"total_memory\"].split()[0])\n            )\n        ]\n        for gpu in gpu_info\n    ]\n\n    print(\n        tabulate(\n            table,\n            headers,\n            tablefmt=\"mixed_outline\",\n            stralign=\"center\",\n            colalign=(\"left\",),\n        )\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n`})}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsxs)(n.em,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,e.jsx)(n.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function c(t={}){let{wrapper:n}=t.components||{};return n?(0,e.jsx)(n,{...t,children:(0,e.jsx)(p,{...t})}):p(t)}return N(k);})();\n;return Component;"