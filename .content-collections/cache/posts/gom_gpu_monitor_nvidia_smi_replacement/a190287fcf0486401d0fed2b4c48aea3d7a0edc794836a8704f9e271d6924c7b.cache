"var Component=(()=>{var u=Object.create;var t=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var w=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),b=(o,e)=>{for(var i in e)t(o,i,{get:e[i],enumerable:!0})},c=(o,e,i,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of g(e))!p.call(o,r)&&r!==i&&t(o,r,{get:()=>e[r],enumerable:!(s=m(e,r))||s.enumerable});return o};var v=(o,e,i)=>(i=o!=null?u(f(o)):{},c(e||!o||!o.__esModule?t(i,\"default\",{value:o,enumerable:!0}):i,o)),y=o=>c(t({},\"__esModule\",{value:!0}),o);var d=w((D,a)=>{a.exports=_jsx_runtime});var k={};b(k,{default:()=>l});var n=v(d());function h(o){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",img:\"img\",li:\"li\",ol:\"ol\",p:\"p\",strong:\"strong\",...o.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"TL;DR\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.code,{children:\"gom\"}),\" stands for GPU Output Monitor. It's a pip package that provides a CLI for monitoring GPU usage. Think of it as \",(0,n.jsx)(e.code,{children:\"nvidia-smi\"}),\", but faster and more minimalist. And it has a bonus feature: \",(0,n.jsx)(e.strong,{children:\"in environments where Docker containers are using GPUs, it will break down usage by container\"}),\"! (Don't worry, it also works in environments without Docker and even inside Docker containers.)\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"I owe my colleague \",(0,n.jsx)(e.a,{href:\"https://howe.vin/\",children:\"Vin\"}),\" credit for inspiring this project. He used GPT-4 to create an initial prototype in Bash, but I had to rewrite from scratch due to bugs and performance issues.\"]})}),`\n`,(0,n.jsx)(e.h2,{children:\"Instructions\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Run \",(0,n.jsx)(e.code,{children:\"pip3 install gom\"})]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Depending on your CUDA version, install the correct version of \",(0,n.jsx)(e.code,{children:\"pynvml\"})]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Run \",(0,n.jsx)(e.code,{children:\"gom show\"}),\" (to show usage once) or \",(0,n.jsx)(e.code,{children:\"gom watch\"}),\" (to monitor usage, updated roughly every second)\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{children:[\"Comparing \",(0,n.jsx)(e.code,{children:\"gom\"}),\" and \",(0,n.jsx)(e.code,{children:\"nvidia-smi\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"I think the results speak for themselves :). This first screenshot is the result of running \",(0,n.jsx)(e.code,{children:\"gom watch\"}),\". You can see that four different Docker containers, \",(0,n.jsx)(e.code,{children:\"r0\"}),\", \",(0,n.jsx)(e.code,{children:\"r1\"}),\", \",(0,n.jsx)(e.code,{children:\"r2\"}),\", and \",(0,n.jsx)(e.code,{children:\"r3\"}),\", are each using a GPU quite heavily. There's also slight usage of all GPUs that's not coming from any container.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/gom-watch.png\",alt:\"output of running gom watch command\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"This second screenshot is the result of running \",(0,n.jsx)(e.code,{children:\"nvidia-smi\"}),\". It's complex and unnecessarily verbose. In more space than \",(0,n.jsx)(e.code,{children:\"gom\"}),\", it only manages to show information for 8 GPUs!\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/nvidia-smi.png\",alt:\"output of running nvidia-smi command\"})}),`\n`,(0,n.jsx)(e.h2,{children:\"Conclusion\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"I created \",(0,n.jsx)(e.code,{children:\"gom\"}),\" because I wanted to monitor GPU usage across different Docker containers. I use it frequently when doing ML tasks because it's fast and the output fits on a small terminal. Hopefully it's helpful for you. If you have suggestions, feel free to open an issue at the \",(0,n.jsx)(e.a,{href:\"https://github.com/nebrelbug/gom\",children:\"GitHub repo\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function l(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,{...o,children:(0,n.jsx)(h,{...o})}):h(o)}return y(k);})();\n;return Component;"