"var Component=(()=>{var h=Object.create;var o=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var _=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var a in e)o(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of m(e))!g.call(t,r)&&r!==a&&o(t,r,{get:()=>e[r],enumerable:!(i=u(e,r))||i.enumerable});return t};var b=(t,e,a)=>(a=t!=null?h(_(t)):{},s(e||!t||!t.__esModule?o(a,\"default\",{value:t,enumerable:!0}):a,t)),y=t=>s(o({},\"__esModule\",{value:!0}),t);var d=f((x,l)=>{l.exports=_jsx_runtime});var k={};w(k,{default:()=>p});var n=b(d());function c(t){let e={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",p:\"p\",pre:\"pre\",...t.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h2,{children:\"Table of Contents\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Introduction\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"In March of this year (2023), a lab at Stanford released a small project that quickly became massively influential \\u2014 \",(0,n.jsx)(e.a,{href:\"https://crfm.stanford.edu/2023/03/13/alpaca.html\",children:\"Alpaca\"}),\". The authors used \",(0,n.jsx)(e.code,{children:\"text-davinci-003\"}),\" (an InstructGPT model from OpenAI) to generate a dataset with 52K examples of prompts and responses, then fine-tuned Llama-7B using those prompt and response pairs.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The result was surprisingly good \\u2014 Alpaca was able to interact with users similarly to OpenAI's InstructGPT models, despite being inexpensive to train and not using a human-created training dataset. In this blog post, we'll write code to train our own model from scratch using the Alpaca dataset.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"The code in this blog post is based on that in the \",(0,n.jsx)(e.a,{href:\"https://github.com/tatsu-lab/stanford_alpaca\",children:\"Alpaca repo\"}),\", though my hope is that it should be simpler and more intuitive. All credit should go to the original authors of the paper.\"]})}),`\n`,(0,n.jsx)(e.h2,{children:\"Setup\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"You'll need to install \",(0,n.jsx)(e.code,{children:\"torch\"}),\", \",(0,n.jsx)(e.code,{children:\"transformers\"}),\", \",(0,n.jsx)(e.code,{children:\"datasets\"}),\", and \",(0,n.jsx)(e.code,{children:\"accelerate\"}),\". \",(0,n.jsx)(e.code,{children:\"wandb\"}),\" is great if you want to track training loss over time. And, of course, you'll need some good GPUs if you want your model to train quickly.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Start out by creating one main folder, \",(0,n.jsx)(e.code,{children:\"alpaca-repro\"}),\", with two subfolders: one called \",(0,n.jsx)(e.code,{children:\"trainer\"}),\", where your training code will go, and one called \",(0,n.jsx)(e.code,{children:\"finetunes\"}),\", where we'll save your fine-tuned model.\"]}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 1: Loading and Processing the Data\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Put all of the code in this section into \",(0,n.jsx)(e.code,{children:\"trainer/get_data.py\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We'll begin by loading the \",(0,n.jsx)(e.a,{href:\"https://huggingface.co/datasets/tatsu-lab/alpaca\",children:\"Alpaca data\"}),\" from the Hugging Face hub. Each question/prompt pair in the dataset needs to be converted into a single string that we can train the model on, but we actually generate one extra string: \",(0,n.jsx)(e.code,{children:\"source\"}),\", which we use further down to ignore labels so our model doesn't train on instructions.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-python\",children:`from datasets import load_dataset\n\noriginal_dataset = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n\ntemplate_no_context = \"\"\"Below is an instruction that describes a task. \\\\\nWrite a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n\"\"\"\n\ntemplate_context = \"\"\"Below is an instruction that describes a task. \\\\\nWrite a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### Response:\n\"\"\"\n\ndef data_to_string(data):\n\n    instruction = data[\"instruction\"]\n    context = data[\"input\"]\n    response = data[\"output\"]\n\n    template = template_context if len(context) > 0 else template_no_context\n    source = template.format(instruction=instruction, input=context)\n\n    return {\n        \"source\": source,\n        \"text\": source + response,\n    }\n\n\ndataset = original_dataset.map(\n    data_to_string\n).remove_columns(['instruction', 'input', 'output'])\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Here we split the data so we can use 10% for evaluation and tests later on.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-python\",children:`processed_dataset = dataset.train_test_split(test_size=0.1)\n\ntrain_dataset = processed_dataset[\"train\"]\neval_dataset = processed_dataset[\"test\"]\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Finally, we define a data collator to be used by our training loop. Remember that each \",(0,n.jsx)(e.code,{children:\"text\"}),\" string is just made up of the \",(0,n.jsx)(e.code,{children:\"source\"}),\" plus the response. So we tokenize the \",(0,n.jsx)(e.code,{children:\"source\"}),\" string to figure out how many labels in the \",(0,n.jsx)(e.code,{children:\"text\"}),\" string to ignore.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-python\",children:`IGNORE_TOKEN = -100\n\ndef data_collator(features, tokenizer):\n    sources = [feature[\"source\"] for feature in features]\n    targets = [feature[\"text\"] for feature in features]\n\n    source_tokens = tokenizer(\n        sources,\n        return_tensors=\"pt\",\n        padding='longest',\n        max_length=None,\n    )\n\n    target_tokens = tokenizer(\n        targets,\n        return_tensors=\"pt\",\n        padding='longest',\n        max_length=None,\n    )\n\n    labels = target_tokens[\"input_ids\"].clone()\n\n    for i in range(len(labels)):\n        source_len = source_tokens[\"attention_mask\"][i].sum()\n\n        labels[i, :source_len] = IGNORE_TOKEN\n\n    res = {\n        \"input_ids\": target_tokens[\"input_ids\"],\n        \"attention_mask\": target_tokens[\"attention_mask\"],\n        \"labels\": labels,\n    }\n\n    return res\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 2: Writing our Training Loop\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Put all of the code in this section into \",(0,n.jsx)(e.code,{children:\"trainer/loop.py\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This code is fairly self-explanatory, so I've just annotated it with comments.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-python\",children:`from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments\nfrom accelerate import Accelerator\nfrom get_data import train_dataset, eval_dataset, data_collator\n\naccelerator = Accelerator()\n\nMODEL_PATH = \"meta-llama/Llama-2-7b-hf\" # path to Llama on Hugging Face Hub\nOUTPUT_DIR = \"../finetunes/alpaca-7b\" # where to save the fine-tuned model\n\ntokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH, legacy=False)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # not set by default, strangely\n\nmodel = LlamaForCausalLM.from_pretrained(\n    MODEL_PATH, device_map=\"auto\"\n)\n\ntraining_args = TrainingArguments(\n    output_dir='checkpoints', # where Trainer will save model checkpoints\n    num_train_epochs=1, # start with a low number of epochs for testing\n    learning_rate=2e-5,\n    logging_steps=10,\n    per_device_train_batch_size=8,\n    remove_unused_columns=False,\n    save_steps=1000,\n    save_total_limit=1,\n    report_to=\"wandb\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=lambda x: data_collator(x, tokenizer),\n)\n\ntrainer.train()\ntrainer.evaluate()\n\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 3: Running our Training Loop\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Create \",(0,n.jsx)(e.code,{children:\"trainer/accelerate_config.yaml\"}),\", and paste in the following configuration:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`compute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: \"NO\"\ndowncast_bf16: \"no\"\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: \"no\"\nnum_machines: 1\nnum_processes: 1\nuse_cpu: false\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Then \",(0,n.jsx)(e.code,{children:\"cd\"}),\" into \",(0,n.jsx)(e.code,{children:\"./trainer\"}),\" and run:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`accelerate launch --config_file accelerate_config.yaml loop.py\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Saving the model and weights might take a while, so be patient!\"}),`\n`,(0,n.jsx)(e.h2,{children:\"Step 4: Testing our Fine-Tuned Model!\"}),`\n`,(0,n.jsx)(e.p,{children:\"I wrote a simple script to load up our fine-tuned model and interact with it! It doesn't support conversations with context, but it's a great way to see how the model is working.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Create a new file called \",(0,n.jsx)(e.code,{children:\"alpaca-repro/model_test.py\"}),\", then run \",(0,n.jsx)(e.code,{children:\"python3 model_test.py\"}),\".\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-python\",children:`from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\ntemplate = \"\"\"Below is an instruction that describes a task. \\\\\nWrite a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n\"\"\"\n\nmodel_path = \"./finetunes/alpaca-7b\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, legacy=False)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, device_map=\"auto\", local_files_only=True\n)\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=False,\n    do_sample=True,\n    temperature=0.9,\n    max_new_tokens=200,\n)\n\ndef prompt_model():\n    prompt = input(\"Enter your question: \")\n    prompt = template.format(instruction=prompt)\n    answer = pipe(prompt)\n    print(answer[0][\"generated_text\"])\n\nwhile True:\n    prompt_model()\n`})}),`\n`,(0,n.jsx)(e.h2,{children:\"Conclusion\"}),`\n`,(0,n.jsx)(e.p,{children:\"I hope this article was helpful and informative! My plan is to follow it up in a few days with an explanation of how to use FSDP with the Hugging Face Trainer.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you got mixed up along the way, here's a Gist with the final project code: https://gist.github.com/nebrelbug/1da2c0064d53decf197a304267799708\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsxs)(e.em,{children:[\"If you liked this article, don't forget to share it and follow me at \",(0,n.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})})]})}function p(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,{...t,children:(0,n.jsx)(c,{...t})}):c(t)}return y(k);})();\n;return Component;"