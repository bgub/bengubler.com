"var Component=(()=>{var p=Object.create;var o=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),w=(n,e)=>{for(var t in e)o(n,t,{get:e[t],enumerable:!0})},s=(n,e,t,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of u(e))!f.call(n,r)&&r!==t&&o(n,r,{get:()=>e[r],enumerable:!(i=m(e,r))||i.enumerable});return n};var v=(n,e,t)=>(t=n!=null?p(g(n)):{},s(e||!n||!n.__esModule?o(t,\"default\",{value:n,enumerable:!0}):t,n)),_=n=>s(o({},\"__esModule\",{value:!0}),n);var l=b((P,c)=>{c.exports=_jsx_runtime});var y={};w(y,{default:()=>d});var a=v(l());function h(n){let e={a:\"a\",code:\"code\",p:\"p\",pre:\"pre\",...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.p,{children:\"Historically, more attention has been paid to distributed training than to distributed inference. After all, training is more computationally expensive. Larger and more complex LLMs, however, can take a long time to perform text completion tasks. Whether for research or in production, it's valuable to parallelize inference in order to maximize performance.\"}),`\n`,(0,a.jsx)(e.p,{children:\"It's important to recognize that there's a difference between distributing the weights of one model across multiple GPUs and distributing model prompts or inputs across multiple models. The first is relatively simple, while the latter (which I'll be focusing on) is slightly more involved.\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"A week ago, in version 0.20.0, \",(0,a.jsx)(e.a,{href:\"https://huggingface.co/docs/accelerate/index\",children:\"HuggingFace Accelerate\"}),\" released a feature that significantly simplifies multi-GPU inference: \",(0,a.jsx)(e.code,{children:\"Accelerator.split_between_processes()\"}),\". It's based on \",(0,a.jsx)(e.code,{children:\"torch.distributed\"}),\", but is much simpler to use.\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"Let's look at how we can use this new feature with LLaMA. The code will be written assuming that you've saved LLaMA weights in the \",(0,a.jsx)(e.a,{href:\"https://huggingface.co/docs/transformers/main/model_doc/llama\",children:\"Hugging Face Transformers format\"}),\".\"]}),`\n`,(0,a.jsx)(e.p,{children:\"First, start out by importing the required modules and initializing the tokenizer and model.\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:\"language-python\",children:`from transformers import LlamaForCausalLM, LlamaTokenizer\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\n\nMODEL_PATH = \"path-to-llama-model\"\n\ntokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n\nmodel = LlamaForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\")\n`})}),`\n`,(0,a.jsxs)(e.p,{children:[\"Notice how we pass in \",(0,a.jsx)(e.code,{children:'device_map=\"auto\"'}),\". This allows Accelerate to evenly spread out the model's weights across available GPUs.\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"If we wanted, we could run \",(0,a.jsx)(e.code,{children:\"model.to(accelerator.device)\"}),\". This would transfer the model to a specific GPU. \",(0,a.jsx)(e.code,{children:\"accelerator.device\"}),\" will be different for each process running in parallel, so you could have a model loaded onto GPU 0, another loaded onto GPU 1, etc. In this case, though, we'll stick with \",(0,a.jsx)(e.code,{children:'device_map=\"auto\"'}),\". This allows us to use larger models than could fit on a single GPU.\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Next, we'll write the code to perform inference!\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:\"language-python\",children:`\ndata = [\n    \"a dog\",\n    \"a cat\",\n    \"a vole\",\n    \"a bat\",\n    \"a bird\",\n    \"a fish\",\n    \"a horse\",\n    \"a cow\",\n    \"a sheep\",\n    \"a goat\",\n    \"a pig\",\n    \"a chicken\",\n]\n\n# Accelerator will automatically split this data up between each running process.\n# The above array has 12 items. So if we had 4 processes, each process\n# would be assigned 3 strings as prompts.\n\nwith accelerator.split_between_processes(data,) as prompts:\n    for prompt in prompts:\n\n        # move the tensor to a GPU, since it needs to be on CUDA\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(accelerator.device)\n\n        # inference\n        generate_ids = model.generate(**inputs, max_length=30)\n\n        # decoding\n        result = tokenizer.batch_decode(\n            generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )[0]\n\n        # print the process number and inference result\n        print(\n            f\"Process {accelerator.process_index} - \"\n            + result.replace(\"\\\\n\", \"\\\\\\\\n\")\n        )\n\n`})}),`\n`,(0,a.jsx)(e.p,{children:\"Finally, all that remains is to launch Accelerate using the Accelerate CLI:\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:`accelerate launch --num_processes=4 script.py\n`})}),`\n`,(0,a.jsx)(e.p,{children:\"When we run the above code, 4 copies of the model are loaded across available GPUs. Our prompts are evenly split across the 4 models, which significantly improves performance.\"}),`\n`,(0,a.jsx)(e.p,{children:\"The output of the above code (after logs from loading the models) should look like this:\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:`Process 1 - a bathtub with a shower head a bathtub with a shower head bathtub with shower head and handheld\nProcess 0 - a dog's life, a dog's life, a dog's life, a dog's life, a dog's life\nProcess 1 - a bird in the hand is worth two in the bush.\\\\na bird in the hand is worth two in the bush.\\\\na bird in\nProcess 2 - a horse, a horse, my kingdom for a horse!\\\\na horse, a horse, my kingdom for a horse!\\\\na horse,\nProcess 3 - a goat, a sheep, a cow, a pig, a dog, a cat, a horse, a chicken, a du\nProcess 0 - a catastrophic event that will change the world forever.\\\\nThe world is in the grip of a global pandemic.\\\\nThe\nProcess 1 - a fishing trip to the Bahamas.\\\\nI'm not sure if I'm going to be able to make it.\\\\n\nProcess 2 - a coworker of mine is a big fan of the show and he's been trying to get me to watch it. I've\nProcess 3 - a piggy bank, a piggy bank, a piggy bank, a piggy bank, a piggy bank\nProcess 0 - a vole, a mouse, a shrew, a hamster, a gerbil, a guinea pig, a rabbit,\nProcess 2 - a sheep, a goat, a ram, a bullock, a he-lamb, a turtle-dove, and\nProcess 3 - a chicken in every pot, a car in every garage, a house in every backyard, a job for every man, a college\n`})}),`\n`,(0,a.jsxs)(e.p,{children:[\"I hope this was helpful! You can learn more about Accelerate and distributed inference in the Accelerate documentation \",(0,a.jsx)(e.a,{href:\"https://huggingface.co/docs/accelerate/usage_guides/distributed_inference\",children:\"here\"}),\" if you're interested.\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"If you liked the article, don't forget to share it and follow me at \",(0,a.jsx)(e.a,{href:\"https://twitter.com/nebrelbug\",children:\"@nebrelbug\"}),\" on Twitter.\"]})]})}function d(n={}){let{wrapper:e}=n.components||{};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(h,{...n})}):h(n)}return _(y);})();\n;return Component;"